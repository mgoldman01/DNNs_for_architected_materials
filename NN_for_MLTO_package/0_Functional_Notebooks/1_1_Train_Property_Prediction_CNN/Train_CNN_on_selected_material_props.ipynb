{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a convolutional neural network regressor to predict selected material properties"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview: This notebook provides for training a convolutional neural network to predict selected homogenized material properties. \n",
    "#### <u> Discussion</u>: The neural network architecture is modular, allowing the user to select which material properties a model instance will be trained to predict. It also has varying number of property prediction modules, which are fully connected linear neural networks that learn to predict material properties based on the feature vector that the convolutional layers produce. The feature vector is weighted using a linear self-attention layer. \n",
    "![Diagram](./media/CNN3D_diagram.png)\n",
    "\n",
    "#### __Configuration:__ The modules can be set to predict one or more material properties in custom order or grouping. The trained model provided in this package and made available for inference in folder 1_2 predicts all material properties using eight modules that each predict a set of grouped properties.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sections are as follows:\n",
    "1. Preliminaries - set filepaths and parameters, import libraries\n",
    "2. Setup database - training/validation/testing splits\n",
    "3. Train Model\n",
    "4. Generate predictions and plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For setting the directory references for the entire package\n",
    "from ML_workflow_utils_v3.PackageDirectories import PackageDirectories as PD   \n",
    "\n",
    "# This code automatically sets the rootpath as the directory the entire package is contained in, which is then called to initialize the PackageDirectories class below\n",
    "import os\n",
    "# check current path if desired\n",
    "# currentpath = os.getcwd()\n",
    "# print(currentpath)\n",
    "\n",
    "os.chdir('../../../')\n",
    "rootpath = os.getcwd()\n",
    "# print(rootpath)\n",
    "\n",
    "# Alternately, rootpath can be set manually\n",
    "# rootpath = 'filepath/containing/entire/ML_package/'\n",
    "\n",
    "directory = PD(rootpath = rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the entire ML package\n",
    "pkgpath = directory.pkgpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration - material properties\n",
    "\n",
    "#### <u>Discussion</u>: The material properties that the model will be trained to predict are set below using two variables, `matprops` (list) and `matprops_by_module` (list of lists)\n",
    "\n",
    "\n",
    "For example, predicting volume fraction, CH_11, CH_22, vH_12, vH_13, kappaH_11 in four separate modules would be configured as follows:\n",
    "\n",
    "`matprops = ['volFrac', 'CH_11 scaled', 'CH_22 scaled', 'vH_12 scaled', 'vH_13 scaled', 'kappaH_11 scaled']`\n",
    "\n",
    "`matprops_by_module = [['volFrac'], ['CH_11 scaled', 'CH_22 scaled',], ['vH_12 scaled', 'vH_13 scaled'], ['kappaH_11 scaled']]`\n",
    "\n",
    "The order of the material properties is maintained, and `matprops_by_module` is an input to the call to instantiate the model. Its number of elements defines the number of modules and the length of each sub-list defines the output dimension of the module.\n",
    "\n",
    "__Note:__ Due to the model's ability to separate material properties, using *scaled* values is not necessary. However, if desiring to use unscaled properties, we recommend grouping similar properties together to avoid confounding due to scale mismatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Material properties #####\n",
    "\n",
    "matprops = ['volFrac', 'CH_11 scaled', 'CH_22 scaled', 'vH_12 scaled', 'vH_13 scaled', 'kappaH_11 scaled']\n",
    "\n",
    "matprops_by_module = [['volFrac'], ['CH_11 scaled', 'CH_22 scaled',], ['vH_12 scaled', 'vH_13 scaled'], ['kappaH_11 scaled']]\n",
    "\n",
    "\n",
    "num_props = len(matprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set    file_suffix    to indicate the material properties being predicted, differentiating between different models\n",
    "we recommend setting it to a readable abbreviation\n",
    "\n",
    "\"\"\"\n",
    "# for the properties selected above\n",
    "file_suffix = 'vfC12v1213kap11'\n",
    "\n",
    "# fname_base = \"filename base\" - This is used as the base for model checkpoints and training history files\n",
    "fname_base = f'CNN3D_{file_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for saving outputs of the notebook\n",
    "# nbpath is the directory of this code notebook\n",
    "nbpath = directory.nb_1_1_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Will automatically select GPU acceleration if hardware is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# For plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle\n",
    "from plotly.colors import sequential, qualitative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom classes and functions\n",
    "from ML_workflow_utils_v3.Dataset_Preprocessor import Dataset_Preprocessor as DataP\n",
    "from ML_workflow_utils_v3.VoxelDataset import VoxelDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data\n",
    "#### Our updated dataset handling class, `Dataset_Preprocessor`, does not require pre-splitting the data into training/validation/test splits. It operates by loading the prepared material property database and splitting it, storing them as attributes of the class. We have indexed our voxel topology arrays using a part numbering scheme, which enables us to maintain every file (in compressed numpy format, .npz) in the same directory. During training, we use a custom dataset based on the PyTorch Dataset class that loads and formats the voxel topologies to then assemble them into batches. This approach is more memory-efficient as well - where previously the dataset splits were small enough to fit in memory, they were still quite large. Now, only small batches of arrays are loaded into memory.\n",
    "\n",
    "#### `Dataset_Preprocessor` maintains our approach of removing entire sets of topologies from the training set, so the model's performance is evaluated on data that it was not trained on. The set of topologies can be set randomly using a random seed, or using a custom-defined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For displaying the number of voxel meshes in each topology family\n",
    "# for topfam in datasplits.matpropcsv['topology_family'].unique():\n",
    "#     subdf = datasplits.matpropcsv[datasplits.matpropcsv.topology_family == topfam]\n",
    "#     print(len(subdf.cell_type.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the following variables from the directory instance to feed to the DatasetPreprocessor instance:\n",
    "data_source_directory = directory.source_data_path\n",
    "meshdir = directory.voxeltopo_path\n",
    "\n",
    "# set seed for test set so group is replicable\n",
    "testset_seed = seed = 17\n",
    "volfrac_range = (0.01, 0.98)\n",
    "\n",
    "csv_fn = 'topology_multiphysics_database_by_partno.csv'\n",
    "\n",
    "datasplits = DataP(csv_fn = csv_fn, data_source_directory = data_source_directory, meshdir = meshdir, volfrac_range=volfrac_range)\n",
    "\n",
    "\"\"\"\n",
    "The *length* of    test_set_topo_counts    must match the number of topologies in the test split.\n",
    "If the test set is chosen randomly, each number in test_set_topo_counts corresponds to the number of topologies from the respective topology family that will be in the test set.\n",
    "\n",
    "By default, every topology family is sampled from to construct the test set. If fewer are desired, create a custom list for topfam_sampling_set\n",
    "and construct test_set_topo_counts to reflect the number of topology families and number of topologies per family. For example\n",
    "\n",
    "topo_families = ['lattice', 'tpms', 'topopt']\n",
    "test_set_topo_counts = [3, 1, 2]\n",
    "\n",
    "\n",
    "Further configuration options for Dataset_Preprocessor are available in the .py file\n",
    "\"\"\"\n",
    "\n",
    "# Define list of topology families for sampling the training split\n",
    "topo_families = datasplits.matpropcsv['topology_family'].unique()\n",
    "# returns list of topology families - ['interp', 'lattice', 'plttube', 'synth', 'topopt', 'tpms']\n",
    "test_set_topo_counts = [1,2,2,2,2,2] # six topology families, two topologies from each family except for the interpolated ('interp') family\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset with the call to this function\n",
    "datasplits.TrainTestSplit(topfam_sampling_set=topo_families, test_set_topo_counts=test_set_topo_counts, translate=False, testset_seed=testset_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect list of topologies placed in testing split\n",
    "print(datasplits.testsubset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The splits are accessible as follows\n",
    "`idx` signifies that these are the indices of the data points\n",
    "\n",
    "`idxTr`  - Training\n",
    "\n",
    "`idxVal` - Validation\n",
    "\n",
    "`idxTe`  - Test\n",
    "\n",
    "e.g. `datasplits.idxTe` is the Testing subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect size of Training, Validation, and Testing splits\n",
    "\n",
    "print(datasplits.idxTr.shape, datasplits.idxVal.shape, datasplits.idxTe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "trdat = VoxelDataset(datasplits.idxTr, directory.voxeltopo_path, matprops)\n",
    "trloader = DataLoader(trdat, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "valdat = VoxelDataset(datasplits.idxVal, directory.voxeltopo_path, matprops)\n",
    "valloader = DataLoader(valdat, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "tedat = VoxelDataset(datasplits.idxTe, directory.voxeltopo_path, matprops)\n",
    "teloader = DataLoader(tedat, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_workflow_utils_v3.Training_Utils import train, validate\n",
    "from ML_workflow_utils_v3.CNN_Property_Predictor_Multimodule import MatProp_CNN3D_varmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This call instantiates the neural network class object contained in ParamNet.py\n",
    "num_outputs = num_params >>> the number of parameters the model is trained to predict\n",
    "    determines the length of its output vector\n",
    "\"\"\"\n",
    "cnn = MatProp_CNN3D_varmod(matprops_by_module).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "lossfunc = torch.nn.L1Loss() # Mean Absolute Error\n",
    "lossfunc_name = 'MAE'\n",
    "\n",
    "# lossfunc = torch.nn.MSELoss() # Mean Squared Error can be used as well - comment out the lines above (add # at start or press ctrl/cmd + /) and uncomment this line and line below (delete #)\n",
    "# lossfunc_name = 'MSE'\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Filepath of model checkpoint - specifies path where the model's weights are saved\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "# cp = 'checkpoint', which is the term for the model's saved state (trainable weights) for an epoch that has improved validation error\n",
    "cp_name= f'{fname_base}_model_weights.pth'\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These parameters establish the behavior of the training loop. The loop trains the model until \n",
    "validation loss does decrease greater than |earlystop_min_delta| for |patience| number of epochs.\n",
    "I.e., if validation loss does not improve by more than 0.00075 for more than 150 training iterations,\n",
    "training will stop.\n",
    "\"\"\"\n",
    "\n",
    "# Number of epochs to train a model\n",
    "EPOCHS = 150\n",
    "\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Number of training iterations after which to terminate training if performance does not improve by more than *earlystop_min_delta*\n",
    "patience = 75\n",
    "earlystop_min_delta = 0.00075 # change to 0.00005 or smaller if using MSE loss\n",
    "\n",
    "# Initializes records of losses and best epoch for tracking model training\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs_completed = 0\n",
    "\n",
    "# Path for training history JSONs\n",
    "\n",
    "histdir = os.path.join(nbpath, 'training_history_JSONs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:           \n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train the model\n",
    "        train_loss = train(cnn, trloader, lossfunc, optimizer)\n",
    "\n",
    "        # Calculate validation loss \n",
    "        val_loss = validate(cnn, valloader, lossfunc)\n",
    "\n",
    "        \n",
    "        # Save the model's weights if validation loss is improved\n",
    "        improvement_delta = best_val_loss - val_loss\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(cnn.state_dict(), best_weights_path)  # Save model weights to file\n",
    "\n",
    "            best_epoch = epoch\n",
    "\n",
    "            # Save training history at each epoch where validation loss improves\n",
    "            hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "            histdict_name = f'{fname_base}_traininghist_placeholder_{epoch}.json'\n",
    "            histpath = os.path.join(histdir, histdict_name)\n",
    "            with open(f'{histpath}', 'w') as f:\n",
    "                json.dump(hist_dict, f)\n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "            \n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "            \n",
    "\n",
    "        # Collect model training history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "\n",
    "            if device == torch.device('cpu'):\n",
    "                model_weights = torch.load(f\"{best_weights_path}\", map_location=torch.device('cpu'))\n",
    "            else:\n",
    "                model_weights = torch.load(f\"{best_weights_path}\")\n",
    "            cnn.load_state_dict(model_weights)\n",
    "            # cnn.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss:.5f}\\tValidation Loss: {val_loss:.5f}')\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    if device == torch.device('cpu'):\n",
    "        model_weights = torch.load(f\"{best_weights_path}\", map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        model_weights = torch.load(f\"{best_weights_path}\")\n",
    "    cnn.load_state_dict(model_weights)\n",
    "    # cnn.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "# Saves reloads model's weights from last saved checkpoint if \n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "    if device == torch.device('cpu'):\n",
    "        model_weights = torch.load(f\"{best_weights_path}\", map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        model_weights = torch.load(f\"{best_weights_path}\")\n",
    "    cnn.load_state_dict(model_weights)\n",
    "    # cnn.load_state_dict(torch.load(best_weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model training history\n",
    "hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "\n",
    "histdict_name = f'{fname_base}_model_hist_{EPOCHS}epochs_{best_epoch}_best.json'\n",
    "histpath = os.path.join(histdir, histdict_name)\n",
    "\n",
    "with open(f'{histpath}', 'w') as f:\n",
    "    json.dump(hist_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up columns for dataframe of test set parameters, predictions, residuals, and percent error. \n",
    "\n",
    "# idxcols is the list of column names to pull from the Training split dataframe.\n",
    "idxcols = ['topology_family', 'cell_type', 'dim_idx', 'volFrac',]\n",
    "idxcols.extend(matprops)\n",
    "# remove any duplicates - mainly volFrac - with OrderedDict\n",
    "from collections import OrderedDict\n",
    "idxcols = list(OrderedDict.fromkeys(idxcols))\n",
    "\n",
    "\n",
    "predcols = [f'Predicted {prop}' for prop in matprops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe from the split dataframes contained in the DatasetCreator class instance\n",
    "test_split_dataframe = datasplits.idxTe[idxcols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference on the test set, calculate errors by unit cell topology and material property and save the dataframe as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs each Voxel array in the Testing dataset through the trained CNN, produces a dataframe\n",
    "of the predictions under the columns of \"pred[icted] [parameter name]\"\n",
    "\"\"\"\n",
    "predsarray_batched = []\n",
    "\n",
    "for batch in teloader:\n",
    "    arrays, param_vecs = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = cnn(arrays.to(device))\n",
    "\n",
    "    predsarray_batched.append(outputs.cpu().numpy())\n",
    "\n",
    "predsarray = np.concatenate(predsarray_batched, axis=0)\n",
    "\n",
    "predictions_dataframe = pd.DataFrame(predsarray, columns=predcols)\n",
    "\n",
    "test_split_dataframe = test_split_dataframe.join(predictions_dataframe)\n",
    "\n",
    "\n",
    "for col in predcols:\n",
    "    par = col[10:]\n",
    "    rescol = f'Residual {par}'\n",
    "    test_split_dataframe[rescol] = test_split_dataframe[par] - test_split_dataframe[col]\n",
    "\n",
    "    pctcol = f'Pct error {par[:-7]}'\n",
    "\n",
    "    test_split_dataframe[pctcol] = (test_split_dataframe[col] - test_split_dataframe[par]) / test_split_dataframe[par] *100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true if reporting Mean Absolute Error\n",
    "mae_loss = True\n",
    "\n",
    "totalerrors = []\n",
    "for col in predcols:\n",
    "    par = col[10:]\n",
    "    test_error = 0\n",
    "    if mae_loss: # Calculates Mean Absolute Error\n",
    "        test_error +=  test_split_dataframe[f'Residual {par}'].abs().sum() # take sum of absolute error\n",
    "\n",
    "    else: # calculates Root Mean Squared Error\n",
    "        test_error += (test_split_dataframe[f'Residual {par}']**2).sum()**0.5\n",
    "\n",
    "    test_error /= len(test_split_dataframe) # Take mean\n",
    "\n",
    "    if par == 'volFrac':\n",
    "        print(f'Error of {par}: \\t{test_error:.4f}')\n",
    "    else:\n",
    "        print(f'Error of {par[:-7]}: \\t{test_error:.4f}')\n",
    "    totalerrors.append(test_error)\n",
    "\n",
    "totalerrors.insert(0, 'mean')\n",
    "\n",
    "\n",
    "errorcols = ['cell_type']\n",
    "errorcols.extend([i[10:] for i in predcols])\n",
    "errordf = pd.DataFrame(columns = errorcols)\n",
    "errordf['cell_type'] = test_split_dataframe.cell_type.unique()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in predcols:\n",
    "\n",
    "    par = col[10:]\n",
    "\n",
    "    errorseries = []\n",
    "\n",
    "    for celltype in test_split_dataframe.cell_type.unique():\n",
    "\n",
    "        celltypedf = test_split_dataframe[test_split_dataframe.cell_type == celltype]\n",
    "        test_error = 0\n",
    "        if mae_loss: # Calculates Mean Absolute Error\n",
    "            test_error +=  celltypedf[f'Residual {par}'].abs().sum() # take sum of absolute error\n",
    "\n",
    "        else: # calculates Root Mean Squared Error\n",
    "            test_error += (celltypedf[f'Residual {par}']**2).sum()**0.5\n",
    "\n",
    "        test_error /= len(celltypedf) # Take mean\n",
    "\n",
    "        errorseries.append(test_error)\n",
    "\n",
    "    errordf[par] = np.asarray(errorseries)\n",
    "\n",
    "errordf.loc[len(errordf.index)] = totalerrors\n",
    "\n",
    "cols = errordf.columns[2:]\n",
    "means = []\n",
    "for i in errordf.index:\n",
    "    celltype_mean = errordf.loc[i][cols].mean()\n",
    "    means.append(celltype_mean)\n",
    "errordf['cell_type mean error'] = means\n",
    "\n",
    "csvpath = os.path.join(nbpath, 'predictions_csvs_and_plots', f'{fname_base}_test_set_errors.csv')\n",
    "errordf.to_csv(csvpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions vs. actual and Volume Fraction vs. Residual or % Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell produces plots of each Predicted parameter versus the actual value, as well as either residuals or percentage error.\n",
    "Plots are:\n",
    "(1) Predicted vs. actual parameter\n",
    "(2) volume fraction vs residual or percentage error\n",
    "\"\"\"\n",
    "\n",
    "###################This section contains parameters for defining particulars of the plots ##################\n",
    "\n",
    "# As shipped, *second_plot* can be of either residuals or percentage error \n",
    "second_plot = 'Residual'# or 'Pct error' \n",
    "\n",
    "width = 1400  # width of plot\n",
    "height_incr = 450 # height of each subplot\n",
    "\n",
    "# color plot markers by individual cell type or topology family\n",
    "category =  'cell_type'# or 'topology_family'\n",
    "\n",
    "# Name of plot to save\n",
    "img_name = f'{fname_base}_predictions_plot_{second_plot}'\n",
    "img_path = os.path.join(nbpath, 'predictions_csvs_and_plots', img_name)\n",
    "\n",
    "# if desired to save the plot as a PNG, set to True\n",
    "save_img = True\n",
    "\n",
    "##########################\n",
    "\n",
    "colors = cycle(qualitative.G10)\n",
    "\n",
    "\n",
    "predsdf = test_split_dataframe\n",
    "\n",
    "# Makes subplots equivalent to the number of parameters Predicted\n",
    "fig = make_subplots(rows=len(matprops), cols=2, row_heights=[20 for i in range(len(matprops))], column_widths=[15,15],\n",
    "                    column_titles = ['Predicted vs Actual', f'{second_plot}'.capitalize()],\n",
    "                    row_titles = matprops)\n",
    "\n",
    "# Iterates over rows\n",
    "row=1\n",
    "for param in matprops:\n",
    "    # colors and colors2 definitions ensure that the predicted vs. actual and residuals plot the same colors with the same chosen category\n",
    "    colors = cycle(qualitative.G10)\n",
    "    colors2 = cycle(qualitative.G10)\n",
    "\n",
    "    # Plots predictions by category as selected above\n",
    "    for uc in predsdf[f'{category}'].unique():\n",
    "\n",
    "        df = predsdf[predsdf[f'{category}']==uc]\n",
    "\n",
    "        fig.append_trace(go.Scatter(x=df[f'Predicted {param}'], \n",
    "                                    y=df[f'{param}'], \n",
    "                                    mode='markers', marker=go.scatter.Marker(color=next(colors)), legendgroup='1',\n",
    "                                    showlegend=True, name=uc), row=row, col=1, )\n",
    "\n",
    "        \n",
    "    # Plots second_plot by category, both as selected above\n",
    "    for uc in predsdf[f'{category}'].unique():\n",
    "\n",
    "        df = predsdf[predsdf[f'{category}']==uc]\n",
    "\n",
    "        # df = predsdf[predsdf.cell_type==uc]\n",
    "        fig.append_trace(go.Scatter(x=df['volFrac'], \n",
    "                                    y=df[f'{second_plot} {param}'],  \n",
    "                                    #y=df[df[f'{second_plot} {param}'].between(-100,100)], # This can filter out extremely large percentage errors if desired\n",
    "                                    mode='markers', marker=go.scatter.Marker(color=next(colors2)), legendgroup='2',\n",
    "                                    showlegend=True, name=uc), row=row, col=2, )\n",
    "\n",
    "    # Plots a line of X=Y on all Predicted-vs-actual plots\n",
    "    x = np.linspace(predsdf['Predicted {param}'.format(param=param)].min(), predsdf['Predicted {param}'.format(param=param)].max(),100)\n",
    "    y = x\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, name='Predicted = actual', legendgroup='1',marker=go.scatter.Marker(color=next(colors))), row=row, col=1 )\n",
    "\n",
    "    # Updates figure with labels\n",
    "    fig.update_xaxes(title_text=\"Predicted {param}\".format(param=param), row=row, col=1)\n",
    "    fig.update_yaxes(title_text=\"Actual {param}\".format(param=param), row=row, col=1)\n",
    "    fig.update_xaxes(title_text=\"Volume Fraction\", row=row, col=2)\n",
    "    fig.update_yaxes(title_text=f\"{second_plot.capitalize()}\", row=row, col=2)\n",
    "    fig.update_layout(title_text = f'Predictions & {second_plot.capitalize()}</br>')\n",
    "\n",
    "    # fig = go.Figure(data=go.Scatter(x=x, y=x**2))\n",
    "\n",
    "    fig.update_xaxes(griddash='solid', minor_griddash=\"solid\")\n",
    "    fig.update_yaxes(griddash='solid', minor_griddash=\"solid\")\n",
    "    row+=1\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f'Plots of test data - multi-parameter prediction', #<br>{runnum} </br>\n",
    "        'xref':\"paper\",\n",
    "        'xanchor':'center',\n",
    "        'x':0.5},\n",
    "        height=height_incr*len(matprops), width=width,\n",
    "        legend_tracegroupgap = 50*len(matprops))\n",
    "fig.show()\n",
    "\n",
    "if save_img:\n",
    "    pngpath = os.path.join(nbpath, 'predictions_csvs_and_plots', f'{img_name}.png')\n",
    "    fig.write_image(pngpath)\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21d719bb0196cef14b3a1b5c98d44dc5c6eb472f8a40fea5f6e94c35d0d21b95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
