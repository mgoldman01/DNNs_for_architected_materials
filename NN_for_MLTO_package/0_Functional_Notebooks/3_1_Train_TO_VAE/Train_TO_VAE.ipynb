{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Topology-Optimizing Variational Autoencoder\n",
    "## <u> Overview</u>: This model uses the same architecture as the TopoGen-VAE and adds to it fully connected multilayer percptron (MLP), AKA fully connected neural networks, linear neural networks or feedforward neural networks, to predict material properties from the latent vector. Once trained, the material property prediction (MPP) modules are used to perform gradient descent optimization on the latent vectors to achieve targeted properties. The optimized latent vectors are then decoded into topologies that have improved material properties.\n",
    "\n",
    "### <u> Model configuration</u>: This model contains eight (8) MPP modules for predicting all nonzero material properties - mechanical, thermal conductivity, and fluid permeability. In the following notebook, \"3_2_TO_with_TO_VAE\", selected properties can be isolated for optimizing topologies with a smaller number of targeted properties and constraints. Having all properties embedded in the trained model adds robustness to the latent space correlations and organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For setting the directory references for the entire package\n",
    "from ML_workflow_utils_v3.PackageDirectories import PackageDirectories as PD   \n",
    "\n",
    "# This code automatically sets the rootpath as the directory the entire package is contained in, which is then called to initialize the PackageDirectories class below\n",
    "import os\n",
    "# check current path if desired\n",
    "# currentpath = os.getcwd()\n",
    "# print(currentpath)\n",
    "\n",
    "os.chdir('../../../')\n",
    "rootpath = os.getcwd()\n",
    "# print(rootpath)\n",
    "\n",
    "# Alternately, rootpath can be set manually\n",
    "# rootpath = 'filepath/containing/entire/ML_package/'\n",
    "\n",
    "directory = PD(rootpath = rootpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration - material properties\n",
    "\n",
    "#### <u>Discussion</u>: The material properties that the model will be trained to predict are set below using two variables, `matprops` (list) and `matprops_by_module` (list of lists)\n",
    "\n",
    "\n",
    "For example, predicting volume fraction, CH_11, CH_22, vH_12, vH_13, kappaH_11 in four separate modules would be configured as follows:\n",
    "\n",
    "`matprops = ['volFrac', 'CH_11 scaled', 'CH_22 scaled', 'vH_12 scaled', 'vH_13 scaled', 'kappaH_11 scaled']`\n",
    "\n",
    "`matprops_by_module = [['volFrac'], ['CH_11 scaled', 'CH_22 scaled',], ['vH_12 scaled', 'vH_13 scaled'], ['kappaH_11 scaled']]`\n",
    "\n",
    "The order of the material properties is maintained, and `matprops_by_module` is an input to the call to instantiate the model. Its number of elements defines the number of modules and the length of each sub-list defines the output dimension of the module.\n",
    "\n",
    "__Note:__ Due to the model's ability to separate material properties, using *scaled* values is not necessary. However, if desiring to use unscaled properties, we recommend grouping similar properties together to avoid confounding due to scale mismatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Material Properties #####\n",
    "matprops = ['volFrac', \n",
    "         'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "         'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "         'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "         'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "         'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "         'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "         'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']\n",
    "\n",
    "matprops_by_module = [['volFrac',], \n",
    "                      ['CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',],\n",
    "                      ['CH_12 scaled', 'CH_13 scaled','CH_23 scaled',],\n",
    "                      ['EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',],\n",
    "                      ['GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled',],\n",
    "                      ['vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',],\n",
    "                      ['KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled',],\n",
    "                      ['kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']]\n",
    "\n",
    "num_props = len(matprops) # for determining the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpath = directory.nb_3_1_path\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "\"\"\" \n",
    "fname_base = \"filename base\" - This is used as the base for model checkpoints and training history files\n",
    "set date to True if you want to include the date in the filename base\n",
    "\"\"\"\n",
    "date = False\n",
    "\n",
    "\"\"\"\n",
    "set    matprops_abbrev    to indicate the material properties being predicted, differentiating between different models\n",
    "we recommend setting it to a readable abbreviation\n",
    "\n",
    "examples: for all properties - matprops_abbrev = allprops\n",
    "          for all properties *unscaled* - matprops_abbrev = allprops_unscaled\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "matprops_abbrev = 'allprops'\n",
    "\n",
    "if date:\n",
    "    date = '24SEP24'\n",
    "\n",
    "    fname_base = f'TopOpt_VAE_{matprops_abbrev}_{date}'\n",
    "\n",
    "else:\n",
    "    fname_base = f'TopOpt_VAE_{matprops_abbrev}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from ML_workflow_utils_v3.Dataset_Preprocessor import Dataset_Preprocessor as DataP\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section sets up the dataset using the DatasetCreator class.\n",
    "\"\"\"\n",
    "\n",
    "# Define the following variables from the directory instance to feed to the DatasetPreprocessor instance:\n",
    "data_source_directory = directory.source_data_path\n",
    "meshdir = directory.voxeltopo_path\n",
    "\n",
    "# Random seed used for splitting the data, or at least the dataframes, into train/validation/test splits.\n",
    "testset_seed = seed = 17\n",
    "\n",
    "\"\"\"\n",
    "Setting range of volume fractions removes very low and very high volume fractions (densities) from the data, \n",
    "which are not practical for application considerations (additive manufacturing) and can confound model training.\n",
    "\"\"\"\n",
    "volfrac_range = (0.01, 0.98)\n",
    "\n",
    "\n",
    "# Sets filepaths for reference\n",
    "\n",
    "source_data_path = directory.source_data_path\n",
    "voxel_dir = directory.voxeltopo_path\n",
    "\n",
    "# Name of the CSV file that contains the homogenized material properties\n",
    "csv_fn = 'topology_multiphysics_database_by_partno.csv'\n",
    "\n",
    "# Instantiate the dataset splitting object\n",
    "datasplits = DataP(csv_fn = csv_fn, data_source_directory = data_source_directory, meshdir = meshdir, volfrac_range=volfrac_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The *length* of    test_set_topo_counts    must match the number of topologies in the test split.\n",
    "If the test set is chosen randomly, each number in test_set_topo_counts corresponds to the number of topologies from the respective topology family that will be in the test set.\n",
    "\n",
    "By default, every topology family is sampled from to construct the test set. If fewer are desired, create a custom list for topfam_sampling_set\n",
    "and construct test_set_topo_counts to reflect the number of topology families and number of topologies per family. For example\n",
    "\n",
    "topo_families = ['lattice', 'tpms', 'topopt']\n",
    "test_set_topo_counts = [3, 1, 2]\n",
    "\n",
    "\n",
    "Further configuration options for Dataset_Preprocessor are available in the .py file\n",
    "\"\"\"\n",
    "\n",
    "# Define list of topology families for sampling the training split\n",
    "topo_families = datasplits.matpropcsv['topology_family'].unique()\n",
    "\n",
    "test_set_topo_counts = [0,2,2,2,2,2] # five topology families, two topologies from each family - interpolated topologies not included\n",
    "\n",
    "# Split the dataset with the call to this function\n",
    "datasplits.TrainTestSplit(topfam_sampling_set=topo_families, test_set_topo_counts=test_set_topo_counts, translate=False, testset_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset creator class modified for Regression VAE\n",
    "\n",
    "from ML_workflow_utils_v3.TO_VAE_training_utils import tovae_train, tovae_validate, TOVAE_loss\n",
    "from ML_workflow_utils_v3.RVAE_training_utils import RVAEDataset\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "trdat = RVAEDataset(datasplits.idxTr, directory.voxeltopo_path, matprops)\n",
    "trloader = DataLoader(trdat, batch_size=batch_size, shuffle=True) #, shuffle=True\n",
    "\n",
    "valdat = RVAEDataset(datasplits.idxVal, directory.voxeltopo_path, matprops)\n",
    "valloader = DataLoader(valdat, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tedat = RVAEDataset(datasplits.idxTe, directory.voxeltopo_path, matprops)\n",
    "teloader = DataLoader(tedat, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is very sensitive to learning rate. Above 0.0005, loss diverges to very large numbers, 0.0001 results in stable training\n",
    "lrate = 0.0001\n",
    "\n",
    "lratestr = f'{lrate:.0e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dimension of latent vector\n",
    "latent_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_workflow_utils_v3.TO_VAE import TOVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE model\n",
    "\n",
    "# If multiple GPUs are available, set to True\n",
    "gpu_parallel = False\n",
    "\n",
    "if gpu_parallel:\n",
    "    tovae = torch.nn.DataParallel(TOVAE(latent_dim, matprops, matprops_by_module)).to(device)\n",
    "else:\n",
    "    tovae = TOVAE(latent_dim, matprops, matprops_by_module).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optim.Adam(tovae.parameters(), lr=lrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath of model checkpoint - specifies path where the model's weights are saved\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "# cp = 'checkpoint', which is the term for the model's saved state (trainable weights) for an epoch that has improved validation error\n",
    "cp_name= f'{fname_base}_model_weights.pth'\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)\n",
    "\n",
    "# Path for training history JSONs\n",
    "histdir = os.path.join(nbpath, 'training_history_JSONs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with model weight saving\n",
    "EPOCHS= 500\n",
    "patience = 100\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "earlystop_min_delta = 0.001\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "lossfunc_name = 'TOVAE_loss'\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Collecting the full loss value as well as its components. This is important data for monitoring training performance \n",
    "and ensuring that unacceptable loss behavior in one part of the function is not obscured by good performance of another component.\n",
    "\n",
    "tr = training, val = validation\n",
    "\n",
    "_losses = total loss value (sum)\n",
    "_recon_loss = reconstruction loss, the inaccuracy between the reconstructed output and the input\n",
    "_kld_loss = KL Divergence loss, measuring the inferred distribution\n",
    "_regr_loss = regression loss, or the difference between the material properties that the MPPs predict and the actual values (from the training dataset)\n",
    "_vf_loss = loss for the prediction of volume fraction, which is produced and trained through the variational sampling strocture\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tr_losses = []\n",
    "tr_recon_loss = []\n",
    "tr_kld_loss = []\n",
    "tr_regr_loss = []\n",
    "tr_vf_loss = []\n",
    "\n",
    "val_losses = []\n",
    "val_recon_loss = []\n",
    "val_kld_loss = []\n",
    "val_regr_loss = []\n",
    "val_vf_loss = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_completed=0\n",
    "\n",
    "# Setting the beta value for weighting KLD Loss in the loss function - for this value of beta, reconstruction loss converges and KLD Loss is not unacceptably large\n",
    "beta = 0.01\n",
    "\n",
    "# Alpha is for weighting the regression loss more strongly - we found that performance was acceptable with alpha = 1.0\n",
    "alpha = 1.0\n",
    "\n",
    "\n",
    "try:                \n",
    "    for epoch in range(EPOCHS):\n",
    "        # loss_criterion = vae_loss_mod\n",
    "        # Train the model\n",
    "        train_loss_output = tovae_train(tovae, trloader, optimizer, beta=beta, alpha=alpha) #beta_schedule[epoch]\n",
    "        train_loss = train_loss_output[0]\n",
    "        reconst_loss = train_loss_output[1]\n",
    "        kl_loss = train_loss_output[2]\n",
    "        reg_loss = train_loss_output[3]\n",
    "        vf_loss = train_loss_output[4]\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss_output = tovae_validate(tovae, valloader, beta=beta, alpha=alpha) # beta_schedule[epoch]\n",
    "        val_loss = val_loss_output[0]\n",
    "        vreconst_loss = val_loss_output[1]\n",
    "        vkl_loss = val_loss_output[2]\n",
    "        vreg_loss = val_loss_output[3]\n",
    "        vvf_loss = val_loss_output[4]\n",
    "\n",
    "        print(f'train reconst loss: {reconst_loss:.5f}, KL loss: {kl_loss:.5f}, regr loss: {reg_loss:.5f}, volfrac loss: {vf_loss:.5f}')\n",
    "        print(f'  val reconst loss: {vreconst_loss:.5f}, KL loss: {vkl_loss:.5f}, regr loss: {vreg_loss:.5f}, volfrac loss: {vvf_loss:.5f}')\n",
    "\n",
    "\n",
    "        # Collect model training history\n",
    "        tr_losses.append(train_loss)\n",
    "        tr_recon_loss.append(reconst_loss)\n",
    "        tr_kld_loss.append(kl_loss)\n",
    "        tr_regr_loss.append(reg_loss)\n",
    "        tr_vf_loss.append(vf_loss)\n",
    "\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_recon_loss.append(vreconst_loss)\n",
    "        val_kld_loss.append(vkl_loss)\n",
    "        val_regr_loss.append(vreg_loss)\n",
    "        val_vf_loss.append(vvf_loss)\n",
    "        \n",
    "        improvement_delta = best_val_loss - val_loss\n",
    "\n",
    "\n",
    "        # Save model weights and stop training if three components of loss - reconstruction, regression, and volFrac prediction - are at an acceptable level - set based on development experience\n",
    "        if vreconst_loss < 0.08 and vreg_loss <= 0.009 and vvf_loss < 0.005:\n",
    "            # SAVE\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(tovae.state_dict(), best_weights_path)  # Save model weights to file\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            hist_dict = {f'train_loss {lossfunc_name}': tr_losses, 'train_reconstruction_loss': tr_recon_loss, 'train_kld_loss': tr_kld_loss, 'train_regression_loss': tr_regr_loss, 'train_volfrac_loss': tr_vf_loss,\n",
    "                 f'val_loss {lossfunc_name}': val_losses, 'val_reconstruction_loss': val_recon_loss, 'val_kld_loss': val_kld_loss, 'val_regression_loss': val_regr_loss, 'val_volfrac_loss': val_vf_loss}\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss:.5f}\\tValidation Loss: {val_loss:.5f}')            \n",
    "            print('Acceptable loss criteria met, stopping training...')\n",
    "            \n",
    "            break\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Save the model's weights if validation loss is improved\n",
    "        \n",
    "        elif val_loss < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(tovae.state_dict(), best_weights_path)  # Save model weights to file\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            \n",
    "            hist_dict = {f'train_loss {lossfunc_name}': tr_losses, 'train_reconstruction_loss': tr_recon_loss, 'train_kld_loss': tr_kld_loss, 'train_regression_loss': tr_regr_loss, 'train_volfrac_loss': tr_vf_loss,\n",
    "                 f'val_loss {lossfunc_name}': val_losses, 'val_reconstruction_loss': val_recon_loss, 'val_kld_loss': val_kld_loss, 'val_regression_loss': val_regr_loss, 'val_volfrac_loss': val_vf_loss}\n",
    "            \n",
    "            \n",
    "            for key in hist_dict.keys():\n",
    "\n",
    "                for i in range(len(hist_dict[key])):\n",
    "                    item = hist_dict[key][i]\n",
    "                    if isinstance(item, torch.Tensor):\n",
    "                        # hist_dict[key][i] = item.detach().cpu().numpy().item()\n",
    "                        hist_dict[key][i] = item.item()\n",
    "\n",
    "            \n",
    "            histdict_name = f'{fname_base}_traininghist_placeholder_{epoch}.json'\n",
    "            histpath = os.path.join(histdir, histdict_name)\n",
    "            with open(f'{histpath}', 'w') as f:\n",
    "                json.dump(hist_dict, f)\n",
    "                    \n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "\n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "\n",
    "\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "            tovae.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss:.5f}\\tValidation Loss: {val_loss:.5f}')\n",
    "\n",
    "        \n",
    "        epochs_completed +=1\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    # tovae.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': tr_losses, 'train_reconstruction_loss': tr_recon_loss, 'train_kld_loss': tr_kld_loss, 'train_regression_loss': tr_regr_loss, 'train_volfrac_loss': tr_vf_loss,\n",
    "                 f'val_loss {lossfunc_name}': val_losses, 'val_reconstruction_loss': val_recon_loss, 'val_kld_loss': val_kld_loss, 'val_regression_loss': val_regr_loss, 'val_volfrac_loss': val_vf_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With training complete, define training history dictionary and save\n",
    "    \n",
    "hist_dict = {f'train_loss {lossfunc_name}': tr_losses, 'train_reconstruction_loss': tr_recon_loss, 'train_kld_loss': tr_kld_loss, 'train_regression_loss': tr_regr_loss, 'train_volfrac_loss': tr_vf_loss,\n",
    "             f'val_loss {lossfunc_name}': val_losses, 'val_reconstruction_loss': val_recon_loss, 'val_kld_loss': val_kld_loss, 'val_regression_loss': val_regr_loss, 'val_volfrac_loss': val_vf_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numbers from loss values if any are in torch.Tensor format\n",
    "\n",
    "for key in hist_dict.keys():\n",
    "\n",
    "    for i in range(len(hist_dict[key])):\n",
    "        item = hist_dict[key][i]\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            # hist_dict[key][i] = item.detach().cpu().numpy().item()\n",
    "            hist_dict[key][i] = item.item()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full model training history\n",
    "histdict_name = f'{fname_base}_model_hist_{EPOCHS}epochs_{best_epoch}_best.json'\n",
    "\n",
    "histpath = os.path.join(histdir, histdict_name)\n",
    "with open(f'{histpath}', 'w') as f:\n",
    "    json.dump(hist_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __End of notebook__ - use notebook \"3_2_TO_with_TO_VAE.ipynb\" for utilizing trained model to generate topologies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
