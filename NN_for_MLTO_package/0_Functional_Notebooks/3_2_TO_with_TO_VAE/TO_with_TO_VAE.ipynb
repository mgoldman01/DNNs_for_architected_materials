{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Perform Topology Optimization with TOVAE__\n",
    "#### <u>Overview</u>: This notebook performs inverse design using the trained TO-VAE. The Material Property Predictor(s) perform inference on the Latent Vector (without the volume fraction component). The prediction can be fed to a customizable loss function that implements an optimization problem. The loss function contains the objective function and the constraints, but the values are produced by the MPPs performing inference on the Latent Vector. Therefore, performing gradient descent optimization on the gradient of the latent vector with respect to the loss function reults in a latent vector that the MPPs calculate to have the targeted material properties. The final latent vector is then decoded into a topology using the decoder portion of the TOVAE, and notionally the topology exhibits the targeted material properties. Validation must be performed using FEA homogenization.\n",
    "\n",
    "__Optimization production loop__:\n",
    "\n",
    "1. Randomly select a topology from the dataset\n",
    "2. Generate its latent vector using the TO-VAE Encoder\n",
    "3. Produce an initial predicted MPV using the MPP portion of the TO-VAE\n",
    "4. Calculate loss\n",
    "5. Calculate gradient of loss with respect to the Latent Vector\n",
    "6. Adjust the LV using a gradient-descent optimizer\n",
    "7. Repeat 4-6 for set number of iterations\n",
    "8. Upon completion, decode the final latent vector into an array\n",
    "9. Apply FEA to determine material properties\n",
    "10. Repeat 1-9 for desired number of runs\n",
    "\n",
    "![TO-VAE diagram](./media/to_vae_w_512.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For setting the directory references for the entire package\n",
    "from ML_workflow_utils_v3.PackageDirectories import PackageDirectories as PD   \n",
    "\n",
    "# This code automatically sets the rootpath as the directory the entire package is contained in, which is then called to initialize the PackageDirectories class below\n",
    "import os\n",
    "# check current path if desired\n",
    "# currentpath = os.getcwd()\n",
    "# print(currentpath)\n",
    "\n",
    "os.chdir('../../../')\n",
    "rootpath = os.getcwd()\n",
    "# print(rootpath)\n",
    "\n",
    "# Alternately, rootpath can be set manually\n",
    "# rootpath = 'filepath/containing/entire/ML_package/'\n",
    "\n",
    "directory = PD(rootpath = rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from ML_workflow_utils_v3.Dataset_Preprocessor import Dataset_Preprocessor as DataP\n",
    "source_data_path = directory.source_data_path\n",
    "voxel_dir = directory.voxeltopo_path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from ML_workflow_utils_v3.Voxel_Mesh_Utils import target_binarray_threshold, Plot_Array \n",
    "from ML_workflow_utils_v3.Model_Weights_Util import convert_state_dict\n",
    "from ML_workflow_utils_v3.Misc_Utils import scale, unscale, save_dict_to_pickle, load_dict_from_pickle\n",
    "from collections import OrderedDict\n",
    "from scipy.io import savemat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook path (nbpath) for the notebook - all future directory/filepath calls\n",
    "nbpath = directory.nb_3_2_path\n",
    "\n",
    "# Set paths for database CSVs and filepath of voxel meshes\n",
    "source_data_path = directory.source_data_path\n",
    "voxel_dir = directory.voxeltopo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Material Properties #####\n",
    "matprops = ['volFrac', \n",
    "         'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "         'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "         'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "         'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "         'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "         'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "         'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']\n",
    "\n",
    "matprops_by_module = [['volFrac',], \n",
    "                      ['CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',],\n",
    "                      ['CH_12 scaled', 'CH_13 scaled','CH_23 scaled',],\n",
    "                      ['EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',],\n",
    "                      ['GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled',],\n",
    "                      ['vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',],\n",
    "                      ['KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled',],\n",
    "                      ['kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']]\n",
    "\n",
    "num_props = len(matprops) # for determining the output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vec(rvae, arraypath):\n",
    "    \"\"\"\n",
    "    Produces latent vector, given an input array in .npz file format (numpy compressed format)\n",
    "\n",
    "    Args:\n",
    "        rvae: loaded instance of trained TO-VAE\n",
    "        arraypath (str): path to input voxel mesh array\n",
    "    \"\"\"\n",
    "    arr = np.load(arraypath)['arr_0']\n",
    "    if arr.dtype != 'int8':\n",
    "        arr = arr.astype(np.int8)\n",
    "    else:\n",
    "        pass\n",
    "    arr = np.expand_dims(arr, (0,1))    \n",
    "    arr = torch.Tensor(arr).to(dtype=torch.float32).to(device)\n",
    "    latent_vec = rvae(arr)[1]\n",
    "    \n",
    "    return(latent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_workflow_utils_v3.TO_VAE import TOVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the multiphysics database of homogenized properties\n",
    "\"\"\"\n",
    "\n",
    "# Name of the CSV file that contains the homogenized material properties - \"statdb\" = static database\n",
    "statdb_csvname =  'topology_multiphysics_database_by_partno.csv' #'full_db_cao_may24_PNs_final_plttube_reduced_scaled.csv'\n",
    "\n",
    "# database (db) path\n",
    "sdbpath = os.path.join(source_data_path, statdb_csvname)\n",
    "\n",
    "sdb = pd.read_csv(sdbpath)\n",
    "\n",
    "\n",
    "dyndb_csvname = 'dynamic_static_database_scaled.csv'\n",
    "\n",
    "dyndbpath = os.path.join(source_data_path, dyndb_csvname)\n",
    "\n",
    "dyndb = pd.read_csv(dyndbpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values for m and b were determined using the dyndb, predicting CH_11 from the respective material properties (scaled) below\n",
    "dyn_linreg_dict = {'plateau stress' : {'m': 0.3887291380713647, 'b': 0.010396286544529405},\n",
    "                   'energy absorbed': {'m': 0.42632711358611763, 'b': 0.018541292815752786}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dynamic_property(property='plateau stress', C11=1.0, dyn_linreg_dict = dyn_linreg_dict):\n",
    "    \"\"\"\n",
    "    Calculate predicted dynamic property based on input C[H]_11 value\n",
    "\n",
    "    property (str): set to \"plateau stress\" or \"energy absorbed\" - will call the applicable linear regression coefficients from the dictioary\n",
    "    C11 (float): value of C11\n",
    "    dyn_linreg_dict (dict): dictionary containing the coefficients for the linear regressions that predict C11 from plateau stress and energy absorbed, respectively\n",
    "\n",
    "    Returns:\n",
    "        tuple: tuple of the symbol/variable of the output property, either sig_pl for plateau stress or W for energy absorbed, along with the value\n",
    "    \"\"\"\n",
    "    if property == 'plateau stress':\n",
    "        prop_out = 'sig_pl'\n",
    "    elif property == 'energy absorbed':\n",
    "        prop_out = 'W'\n",
    "    else:\n",
    "        Exception(\"property must be \\'plateau stress\\' or \\'energy absorbed\\'\")\n",
    "    \n",
    "    \n",
    "    m = dyn_linreg_dict[property]['m']\n",
    "    b = dyn_linreg_dict[property]['b']\n",
    "\n",
    "    dynprop_calc = (C11 - b) / m\n",
    "\n",
    "    return prop_out, dynprop_calc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables for VAE configuration\n",
    "\n",
    "# latent_dim is the dimension of the \"encoded\"\n",
    "latent_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_workflow_utils_v3.TO_VAE import TOVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If multiple GPUs are available, set to True\n",
    "gpu_parallel = False\n",
    "\n",
    "if gpu_parallel:\n",
    "    tovae = torch.nn.DataParallel(TOVAE(latent_dim, matprops, matprops_by_module).to(device))\n",
    "else:\n",
    "    tovae = TOVAE(latent_dim, matprops, matprops_by_module).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "If loading the TopoGen-VAE that was shipped with this package, execute these three lines of code\n",
    "\"\"\"\n",
    "cp_dir = directory.topopt_vae_path\n",
    "cp_name = \"TO_VAE_pretrained_model_weights.pth\"\n",
    "cp_path = os.path.join(cp_dir, cp_name)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If loading a model trained in Notebook 3.1 (3_1_Train_TO_VAE), comment out the three lines of code\n",
    "above by highlighting and pressing Ctrl+/, or putting '#' at the beginning of each line\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# cp_dir = os.path.join(directory.nb_3_1_path, 'model_CPs')\n",
    "# cp_name = 'TO_VAE_allprops_28AUG24' # placeholder, replace\n",
    "# cp_path = os.path.join(cp_dir, cp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This command loads the trained model's weights\n",
    "\n",
    "The included pre-trained model weights were produced using a multi-GPU training setup, therefore the key of each layer contains \"module.\", \n",
    "which is how torch.nn.DataParallel() creates state dictionaries. The call of \"convert_state_dict\"\n",
    "If using more than one GPU for topology production, set \"convert_weight_keys_to\" to 'parallel'\n",
    "\"\"\"\n",
    "\n",
    "convert_weight_keys_to = 'non-parallel'\n",
    "\n",
    "model_weights = convert_state_dict(cp_path, convert_to = convert_weight_keys_to)\n",
    "\n",
    "\n",
    "tovae.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TOVAE to evaluation mode\n",
    "tovae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a variable to call the decoder module of the TopoGen-VAE and set it to evaluation mode\n",
    "if convert_weight_keys_to == 'non-parallel':\n",
    "    decoder = tovae.decoder\n",
    "elif convert_weight_keys_to == 'parallel':\n",
    "    decoder = tovae.module.decoder\n",
    "\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Topology Optimization with Optimization Loss Function__\n",
    "\n",
    "#### The Optimization Loss Function (OLF) is an extremely flexible structure for representing an optimization problem. It can contain the objective function and the constraints. As the pre-trained TOVAE predicts all material properties, optimization can be performed to target or constrain as many material properties as desired.\n",
    "\n",
    "#### The loss function is a sum of terms, where each term corresponds to an objective function or a constraint.\n",
    "\n",
    "#### <u>Objective Function</u>: Can have two forms (using $C^{H}_{11}$ as the example target property):\n",
    "1. Maximize $C_{11}^H$ to achieve the highest \\sigma_{pl} possible \n",
    "- <u>Objective function</u>: &emsp; &emsp; $max\\ C_{11}^H$\n",
    "- Alternatively,  &emsp; &emsp; &emsp; &nbsp; &nbsp; $min\\ (-C_{11}^H)$\n",
    "\n",
    "\n",
    "2. Target $C_{11}^H$ to a specific value\n",
    "- <u>Objective function</u>:&nbsp;&nbsp;$min\\left|C_{11,topo}^H-\\widehat{C_{11}^H}\\right|$\n",
    "\n",
    "- <u>Note</u>: There can be as many targeted properties as desired, forming the Targeted Material Property Vector, or $\\widehat{MPV}$.\n",
    "\n",
    "The loss function for MPV made of $n$ material properties $p_{n}$\n",
    "\n",
    "$min     Loss_{opt} =$\n",
    "\n",
    "$ min       \\left[ \\left|{p}_{1}-\\widehat{{p}_{1}}\\right| + \\left|{p}_{2}-\\widehat{{p}_{2}}\\right| + ... + \\left|{p}_{n}-\\widehat{{p}_{n}}\\right| \\right]$\n",
    "\n",
    "\n",
    "#### <u>Constraints</u>: are calculations comparing the Latent Vector's material properties predicted by the MPP network, gated with the Rectified Linear Unit (ReLU) function. The ReLU function is as follows:\n",
    "\n",
    "$\\text{ReLU}(x) =\n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "or \n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "Applying to a constraint such as\n",
    "\n",
    "$ x \\leq C$\n",
    "\n",
    "rearranged as\n",
    "\n",
    "$ x - C \\leq 0$\n",
    "\n",
    "The gradient descent loss function - the OLF - penalizes positive values as it drives the loss function value to a minimum. Therefore, the ReLU function allows for only contributing an error if the constraint is greater than zero. For the constraint above there are two cases:\n",
    "\n",
    "(i) $x < C$: The constraint is not violated and should not contribute to the Loss function, therefore it should be zeroed.\n",
    "(ii) $x > C$: The constraint is violated and should contribute a positive number to the loss function, which Implemented with the ReLU function, the constraint $Z$ is:\n",
    "\n",
    "$Z = ReLU(x - C)$\n",
    "\n",
    "Thus in case (i), Z = 0, and in case (ii), Z > 0 and contributes to the loss function, which the gradient descent optimizer drives to zero. \n",
    "\n",
    "For a \"greater than\" constraint, $ x \\geq C$, or $ x - C \\geq 0$, we mlutiply the term by -1 to get $ C - x \\leq 0$, so the constraint $Z$ is\n",
    "\n",
    "$Z = ReLU(C - x)$\n",
    "\n",
    "In the loss function provided below, the targeted properties are $C^{H}_{11}$, volume fraction (\"volFrac\" in the database), and the axial fluid permeabilities $K^{H}_{11}, K^{H}_{22}, K^{H}_{33}$. The objective function is to maximize $C^{H}_{11}$ while setting a lower bound on all three permeability values. The topology is optimized to have a targeted volume fraction.\n",
    "\n",
    "<u>Note</u>: the MPP can produce predicted material properties that are outside the values of the actual data - in the case of scaled data, outside of the range of [0,1]. We implement bounds such that the material properties cannot exceed 1.0. We implement the \"big M\" method to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for the optimization runs\n",
    "\n",
    "### If targeting a specific MPV, set `optimiz_loss` to  `False` and define `tgt_mpv` with the properties you want, in the order you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of iterations to drive OLF as low as achievable - adjust as preferred, trade-off is duration of each run\n",
    "num_iterations = 25000\n",
    "# Number of runs to perform\n",
    "num_runs = 100\n",
    "\n",
    "# If maximizing a value, not just targeting a MPV, set to True\n",
    "optimiz_loss = True\n",
    "\n",
    "# set target volume fraction and configure loss function\n",
    "tgt_vf = 0.2\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "# tgt_vf_torch = torch.tensor(tgt_vf).unsqueeze(0).unsqueeze(0).cuda()\n",
    "tgt_vf_torch = torch.tensor(tgt_vf).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and create the output folder\n",
    "date = '23SEP24'\n",
    "batch_folder_name = f\"batch0_{date}\"\n",
    "batch_path = os.path.join(nbpath, 'TO_production_runs', batch_folder_name)\n",
    "os.makedirs(batch_path, exist_ok=True)\n",
    "\n",
    "matdir = os.path.join(batch_path, 'mat_files')\n",
    "os.makedirs(matdir, exist_ok=True)\n",
    "plotdir = os.path.join(batch_path, 'array_plots')\n",
    "os.makedirs(plotdir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective_Function(nn.Module):\n",
    "    def __init__(self, matprop, config_dict):\n",
    "        \n",
    "        super(Objective_Function, self).__init__()\n",
    "\n",
    "        # matprop, type='max', weight=1.0\n",
    "        \n",
    "\n",
    "        self.matprop = matprop\n",
    "        self.of_type = config_dict['type']\n",
    "        self.weight =  config_dict['weight']\n",
    "\n",
    "        if self.of_type != 'min' and self.of_type != 'max':\n",
    "            raise Exception(\"of_type must be \\'max\\' or \\'min\\'\")\n",
    "       \n",
    "\n",
    "    def calculate_objfn_value(self, x):\n",
    "\n",
    "        if self.of_type == 'max':\n",
    "            self.objfn_value = -1 * self.weight * x\n",
    "        elif self.of_type == 'min':\n",
    "            self.objfn_value = self.weight * x\n",
    "        # print(self.objfn_value)\n",
    "        # self.objfn_value = torch.Tensor(np.asarray(self.objfn_value)).unsqueeze(0).requires_grad(True)\n",
    "        return self.objfn_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatProp_Constraints(nn.Module):\n",
    "    def __init__(self, matprop, config_dict: dict):\n",
    "        \n",
    "        super(MatProp_Constraints, self).__init__()\n",
    "\n",
    "        self.matprop = matprop\n",
    "\n",
    "        self.upper = config_dict['upper']\n",
    "        self.lower = config_dict['lower']\n",
    "        self.bigM =  config_dict['bigM']\n",
    "        \n",
    "        if 'upper' in self.bigM:\n",
    "            self.W_up =  config_dict['bigM_val']\n",
    "        else:\n",
    "            self.W_up =  config_dict['up_W']\n",
    "        if 'lower' in self.bigM:\n",
    "            self.W_low = config_dict['bigM_val']\n",
    "        else:\n",
    "            self.W_low = config_dict['low_W']\n",
    "            \n",
    "\n",
    "        self.upval = 1.0\n",
    "        self.lowval = 0.0\n",
    "\n",
    "    def calculate_loss_terms(self, x):\n",
    "\n",
    "        total_loss = torch.Tensor([0.0]).to(device).requires_grad_(True)\n",
    "        \n",
    "        if self.upper is not None:      \n",
    "            self.upper_bound_loss = torch.relu(x - self.upval) * self.W_up\n",
    "            total_loss = total_loss + self.upper_bound_loss\n",
    "        \n",
    "        if self.lower is not None:\n",
    "            self.lower_bound_loss = torch.relu(self.lowval - x) * self.W_low\n",
    "            total_loss + total_loss + self.lower_bound_loss\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the configuration for the optimization problem.\n",
    "#### The `config_dict` allows for setting the objective function/variables, which are targeted as maximize or minimize, and the constraints. It takes the following keywords:\n",
    "\n",
    "#### `objective variables`: entry is a nested `dict`:\n",
    "##### <u>first level key(s)</u>: name(s) of material properties to target with objective function, e.g., `'CH_11 scaled'`. Keys in next level `dict` as follows:\n",
    "## <u>__Note</u>:__ For predicting dynamic material properties - plateau stress $\\bold{\\sigma_{pl}}$ and energy absorbed $\\bold{W}$, one objective variable must be $\\bold{C^{H}_{11}}$\n",
    "##### `'type'`: `'max'` or `'min'` \n",
    "##### `'weight'` - default is `1.0`, set to desired weight to contribute to loss function. This value slightly weights the constraint in the gradient descent process, allowing emphasis on objective variable(s)/function(s)\n",
    "\n",
    "#### `constraints`: entry is a nested `dict`:\n",
    "##### <u>first level key(s)</u>: name(s) of material properties to target with objective function, e.g., `'KH_11 scaled'`. Keys in next level `dict` as follows:\n",
    "##### `'upper'`: `True` or `False` - if True, implement upper bound\n",
    "##### `'lower'`: `True` or `False` - if True, implement lower bound\n",
    "##### `'bigM'`: `list` contains `'upper'`, `'lower'`, both, or empty - implements \"big-M\" penalty on specified constraint(s) to ensure it is not violated\n",
    "##### `'upval'`: value of upper-bound constraint. Defaults to 1.0. This ensures that any constraint will not exceed 1.0, the maximum [scaled] value for the material property. Applicable if the target constraint on the material property is a \"greater than or equal to\" constraint. If a \"less than or equal to\" constraint, this value may be set lower to the desired upper bound.\n",
    "##### `'lowval'`: value of lower-bound constraint. Defaults to 0.0.\n",
    "##### `'bigM_val'`: sets value of big-M penalty. Defaults to 10,000 (1e5). Likely not necessary to change this value.\n",
    "##### `'up_W'`: value of weight on upper-bound constraint. Defaults to 1.0. If not using big-M penalty, this value slightly weights the constraint in the gradient descent process, allowing emphasis of certain constraints\n",
    "##### `'low_W'`: value of lower-bound constraint. Defaults to 1.0. If not using big-M penalty, this value slightly weights the constraint in the gradient descent process, allowing emphasis of certain constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is recommended to always include volume fraction as a constraint\n",
    "\n",
    "config_dict = {'objective variables': {'CH_11 scaled': {'type': 'max', 'weight': 1.0}},\n",
    "'constraints': {'volFrac': {'upper': True, 'lower': True, 'bigM': [], \n",
    "                                          'upval':0.4, 'lowval': 0.0, 'bigM_val': 1e5, 'up_W': 1.0, 'low_W': 1.0},\n",
    "                'KH_11 scaled': {'upper': True, 'lower': False, 'bigM': ['upper'], \n",
    "                                          'upval':1.0, 'lowval': 0.0, 'bigM_val': 1e5, 'up_W': 1.0, 'low_W': 1.0},\n",
    "                'KH_22 scaled': {'upper': True, 'lower': False, 'bigM': ['upper'], \n",
    "                                          'upval':1.0, 'lowval': 0.0, 'bigM_val': 1e5, 'up_W': 1.0, 'low_W': 1.0},\n",
    "                'KH_33 scaled': {'upper': True, 'lower': False, 'bigM': ['upper'], \n",
    "                                          'upval':1.0, 'lowval': 0.0, 'bigM_val': 1e5, 'up_W': 1.0, 'low_W': 1.0}\n",
    "}}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If targeting a specific material property vector using L1 Loss, use this format:\n",
    "\n",
    "dictionary keys are the material properties, their values are the decimal values of the target MPV\n",
    "example below:\n",
    "\"\"\"\n",
    "# config_dict = {'CH_11 scaled': 0.5, 'volfrac': 0.3, 'KH_11 scaled': 0.15, 'KH_22 scaled': 0.15, 'KH_33 scaled': 0.15}\n",
    "## set the target MPV in a Tensor as well\n",
    "# tgt_mpv = np.asarray([0.5, 0.3, 0.15, 0.15, 0.15])\n",
    "# tgt_mpv = torch.Tensor(tgt_mpv).unsqueeze(0).to(device)\n",
    "\n",
    "# set to True if CH_11 is an objective variable for predicting dynamic material properties\n",
    "dynamic_topopt = True\n",
    "\n",
    "# set a suffix for naming the produced arrays that reflects the material properties targeted\n",
    "run_suffix = 'C1VFK123'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for holding predicted material properties of latent vector at the end of training\n",
    "\n",
    "cols = []\n",
    "for key in config_dict.keys():\n",
    "    for matprop in config_dict[key].keys():\n",
    "        if matprop == 'volFrac':\n",
    "            cols.append(matprop)\n",
    "        else:\n",
    "            matprop = matprop.split(' ')[0]\n",
    "            cols.append(matprop)\n",
    "            unscaled = f'{matprop} unscaled'\n",
    "            cols.append(unscaled)\n",
    "        \n",
    "\n",
    "predsdf =  pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradOpt_MAE_Loss_module(nn.Module):\n",
    "\n",
    "    def __init__(self, tovae, olf_config: dict, matprop_mpp_dict: dict,):\n",
    "        \"\"\"\n",
    "        Inputs for the gradient descent setup module\n",
    "\n",
    "        Args:\n",
    "            tovae: loaded instance of TO-VAE\n",
    "            olf_config (dict): configuration dictionary for objective loss function\n",
    "            matprop_mpp_dict (dict): dictionary that looks up the MPP that predicts each material property\n",
    "        \"\"\"\n",
    "\n",
    "        super(GradOpt_MAE_Loss_module, self).__init__()\n",
    "\n",
    "        self.olf_config = olf_config\n",
    "\n",
    "        self.matprops = []\n",
    "        self.mpv_dict = {}\n",
    "        self.tgt_mpv = []\n",
    "\n",
    "        idx = 0\n",
    "        for key in olf_config.keys():\n",
    "            self.matprops.append(key)\n",
    "            self.mpv_dict[key] = idx\n",
    "            self.tgt_mpv.append(olf_config[key])\n",
    "            idx +=1\n",
    "\n",
    "        self.tgt_mpv = torch.Tensor(np.asarray(self.tgt_mpv)).unsqueeze(0).to(device)\n",
    "\n",
    "        \n",
    "        # loop through the configuration dictionary\n",
    "        for key in olf_config.keys():\n",
    "            sub = olf_config[key]\n",
    "            for skey in sub.keys():\n",
    "                # add to list of the material properties\n",
    "                self.matprops.append(skey)\n",
    "\n",
    "                #\n",
    "                self.mpv_dict[skey] = idx\n",
    "                idx += 1\n",
    "\n",
    "\n",
    "        self.mpp_dict = matprop_mpp_dict \n",
    "\n",
    "        self.mpps = nn.ModuleList()\n",
    "\n",
    "        self.mpp_idx = []\n",
    "\n",
    "        \n",
    "        for mp in self.matprops:\n",
    "            mpp_id = self.mpp_dict[mp][0] # pulls the name of the MPP based on the \n",
    "            mpp = getattr(tovae, mpp_id) # pulls the MPP module from the TOVAE\n",
    "            self.mpps.append(mpp)        # adds to ModuleList\n",
    "            self.mpp_idx.append(self.mpp_dict[mp][1]) # takes the index of the material property in the output of the MPP, e.g., if the MPP predicts KH_11, KH_22, and KH_33, then KH_22 is at index [1]\n",
    "\n",
    "    def get_mpv_pred(self, latent_vec):\n",
    "        \"\"\"\n",
    "        Produces material property prediction vector of selected material properties, based on the property name\n",
    "        and its index in the output from its MPP. E.g., if the MPP predicts KH_11, KH_22, and KH_33, then KH_22 is at index [1]\n",
    "\n",
    "        Args:\n",
    "            latent_vec (torch.Tensor): latent vector as input\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: tensor of the predicted material properties\n",
    "        \"\"\"\n",
    "\n",
    "        preds = []\n",
    "        for mpp, idx in zip(self.mpps, self.mpp_idx):\n",
    "            pred = mpp(latent_vec.to(device))\n",
    "            pred = list(pred.detach().cpu().numpy()[0,:])[idx]\n",
    "\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = torch.from_numpy(np.asarray(preds)).unsqueeze(0).to(device)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, latent_vec):\n",
    "        \"\"\"\n",
    "        Calls the objective function and constraints, adds them together, returns the total loss value\n",
    "\n",
    "        Args:\n",
    "            latent_vec (torch.Tensor): latent vector\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: predicted MPV\n",
    "        \"\"\"\n",
    "\n",
    "        self.mpv_pred = self.get_mpv_pred(latent_vec)\n",
    "\n",
    "        return self.mpv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradOpt_Optimization_Loss_Module(nn.Module):\n",
    "\n",
    "    def __init__(self, tovae, olf_config: dict, matprop_mpp_dict: dict):\n",
    "        \"\"\"\n",
    "        Inputs for the gradient descent setup module\n",
    "\n",
    "        Args:\n",
    "            tovae: loaded instance of TO-VAE\n",
    "            olf_config (dict): configuration dictionary for objective loss function\n",
    "            matprop_mpp_dict (dict): dictionary that looks up the MPP that predicts each material property\n",
    "        \"\"\"\n",
    "\n",
    "        super(GradOpt_Optimization_Loss_Module, self).__init__()\n",
    "\n",
    "        self.olf_config = olf_config\n",
    "        \n",
    "\n",
    "        self.obj_fn = [Objective_Function(key, config_dict['objective variables'][key]) for key in config_dict['objective variables'].keys()]\n",
    "\n",
    "        self.constraints = [MatProp_Constraints(key, value) for key, value in self.olf_config['constraints'].items()]\n",
    "\n",
    "        self.matprops = []\n",
    "        self.mpv_dict = {}\n",
    "\n",
    "        idx = 0\n",
    "        # loop through the configuration dictionary\n",
    "        for key in olf_config.keys():\n",
    "            sub = olf_config[key]\n",
    "            for skey in sub.keys():\n",
    "                # add to list of the material properties\n",
    "                self.matprops.append(skey)\n",
    "\n",
    "                #\n",
    "                self.mpv_dict[skey] = idx\n",
    "                idx += 1\n",
    "\n",
    "\n",
    "        self.mpp_dict = matprop_mpp_dict\n",
    "\n",
    "        self.mpps = nn.ModuleList()\n",
    "\n",
    "        self.mpp_idx = []\n",
    "\n",
    "        \n",
    "        for mp in self.matprops:\n",
    "            mpp_id = self.mpp_dict[mp][0] # pulls the name of the MPP based on the \n",
    "            mpp = getattr(tovae, mpp_id) # pulls the MPP module from the TOVAE\n",
    "            self.mpps.append(mpp)        # adds to ModuleList\n",
    "            self.mpp_idx.append(self.mpp_dict[mp][1]) # takes the index of the material property in the output of the MPP, e.g., if the MPP predicts KH_11, KH_22, and KH_33, then KH_22 is at index [1]\n",
    "\n",
    "    def get_mpv_pred(self, latent_vec):\n",
    "        \"\"\"\n",
    "        Produces material property prediction vector of selected material properties, based on the property name\n",
    "        and its index in the output from its MPP. E.g., if the MPP predicts KH_11, KH_22, and KH_33, then KH_22 is at index [1]\n",
    "\n",
    "        Args:\n",
    "            latent_vec (torch.Tensor): latent vector as input\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: tensor of the predicted material properties\n",
    "        \"\"\"\n",
    "\n",
    "        preds = []\n",
    "        for mpp, idx in zip(self.mpps, self.mpp_idx):\n",
    "            pred = mpp(latent_vec.to(device))\n",
    "            pred = list(pred.detach().cpu().numpy()[0,:])[idx]\n",
    "\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = torch.from_numpy(np.asarray(preds)).unsqueeze(0).to(device)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, latent_vec):\n",
    "        \"\"\"\n",
    "        Calls the objective function and constraints, adds them together, returns the total loss value\n",
    "\n",
    "        Args:\n",
    "            latent_vec (torch.Tensor): latent vector\n",
    "\n",
    "        Returns:\n",
    "            float: value of the loss function\n",
    "        \"\"\"\n",
    "\n",
    "        self.mpv_pred = self.get_mpv_pred(latent_vec)\n",
    "\n",
    "        loss_val = 0\n",
    "\n",
    "        for i, obj_var in enumerate(self.olf_config['objective variables'].keys()):\n",
    "\n",
    "            obj_var_idx = self.mpv_dict[obj_var]\n",
    "\n",
    "            obj_var_pred = self.mpv_pred[obj_var_idx, :]\n",
    "\n",
    "            obj_var_loss = self.obj_fn[i].calculate_objfn_value(obj_var_pred)\n",
    "\n",
    "            loss_val += obj_var_loss\n",
    "\n",
    "        for j, constr in enumerate(self.olf_config['constraints'].keys()):\n",
    "            constr_idx = self.mpv_dict[constr]\n",
    "\n",
    "            constr_pred = self.mpv_pred[:, constr_idx] # !!! 2x check on indexing [0,:] or [:,0]?\n",
    "\n",
    "            constr_loss = self.constraints[j].calculate_loss_terms(constr_pred)\n",
    "\n",
    "            loss_val += constr_loss\n",
    "\n",
    "        return constr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "matprop groups corresponding to which mlp[x]\n",
    "\n",
    "mpp    group\n",
    "0      ['volFrac',], \n",
    "1      ['CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',],\n",
    "2      ['CH_12 scaled', 'CH_13 scaled','CH_23 scaled',],\n",
    "3      ['EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',],\n",
    "4      ['GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled',],\n",
    "5      ['vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',],\n",
    "6      ['KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled',],\n",
    "7      ['kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "mpp_lookup = {\n",
    "'mpp0': ['volFrac',], \n",
    "'mpp1': ['CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',],\n",
    "'mpp2': ['CH_12 scaled', 'CH_13 scaled','CH_23 scaled',],\n",
    "'mpp3': ['EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',],\n",
    "'mpp4': ['GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled',],\n",
    "'mpp5': ['vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',],\n",
    "'mpp6': ['KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled',],\n",
    "'mpp7': ['kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating dict below again if necessary\n",
    "# reverse_lookup = {}\n",
    "# for key, value in mpp_lookup.items():\n",
    "#     idx = 0\n",
    "#     for par in value:\n",
    "#         reverse_lookup[par] = (key, idx)\n",
    "#         idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary lists all material properties and identifies which MPP predicts them and which element in its output vector is the \n",
    "\n",
    "matprop_mpp_lookup = {'volFrac': ('mpp0', 0),\n",
    "                 'CH_11 scaled': ('mpp1', 0),\n",
    "                 'CH_22 scaled': ('mpp1', 1),\n",
    "                 'CH_33 scaled': ('mpp1', 2),\n",
    "                 'CH_44 scaled': ('mpp1', 3),\n",
    "                 'CH_55 scaled': ('mpp1', 4),\n",
    "                 'CH_66 scaled': ('mpp1', 5),\n",
    "                 'CH_12 scaled': ('mpp2', 0),\n",
    "                 'CH_13 scaled': ('mpp2', 1),\n",
    "                 'CH_23 scaled': ('mpp2', 2),\n",
    "                 'EH_11 scaled': ('mpp3', 0),\n",
    "                 'EH_22 scaled': ('mpp3', 1),\n",
    "                 'EH_33 scaled': ('mpp3', 2),\n",
    "                 'GH_23 scaled': ('mpp4', 0),\n",
    "                 'GH_13 scaled': ('mpp4', 1),\n",
    "                 'GH_12 scaled': ('mpp4', 2),\n",
    "                 'vH_12 scaled': ('mpp5', 0),\n",
    "                 'vH_13 scaled': ('mpp5', 1),\n",
    "                 'vH_23 scaled': ('mpp5', 2),\n",
    "                 'vH_21 scaled': ('mpp5', 3),\n",
    "                 'vH_31 scaled': ('mpp5', 4),\n",
    "                 'vH_32 scaled': ('mpp5', 5),\n",
    "                 'KH_11 scaled': ('mpp6', 0),\n",
    "                 'KH_22 scaled': ('mpp6', 1),\n",
    "                 'KH_33 scaled': ('mpp6', 2),\n",
    "                 'kappaH_11 scaled': ('mpp7', 0),\n",
    "                 'kappaH_22 scaled': ('mpp7', 1),\n",
    "                 'kappaH_33 scaled': ('mpp7', 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "\n",
    "if dynamic_topopt:\n",
    "    dyn_val_dict = {}\n",
    "\n",
    "# If desired to time each run, set to True\n",
    "mark_time = True\n",
    "\n",
    "for num in range(num_runs):\n",
    "    run = num\n",
    "    # Define a dictionary for the run\n",
    "    rundic = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Select a random sample from the dataset as the starting point - \n",
    "    will be embedded in the latent space, producing the latent vector \n",
    "    that is the starting point for gradient descent optimization.\n",
    "\n",
    "    The part number column value corresponds to the database\n",
    "    \"\"\"\n",
    "\n",
    "    starting_pn = sdb.sample(n=1)['full PN'].values[0]\n",
    "    \n",
    "    rundic['seed PN'] = starting_pn\n",
    "    \n",
    "    # Set the filepath to the selected voxel array and then embed using get_latent_vec function\n",
    "    arraypath = os.path.join(voxel_dir, f'{starting_pn}.npz')\n",
    "\n",
    "    # Produce latent vector using the trained TO-VAE via get_latent_vec function\n",
    "    lv_orig = get_latent_vec(tovae, arraypath)\n",
    "\n",
    "    # store Latent Vector in the run dictionary for later reference\n",
    "    rundic['starting LV'] = lv_orig.cpu().detach().numpy()\n",
    "    \n",
    "    # we call the starting latent vector the \"guess\"\n",
    "    # this line creates a copy of the vector that will be integrated into the gradient descent process, i.e., attached to the computation graph\n",
    "    lv_guess = lv_orig.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Set the optimizer to target the guess latent vector\n",
    "    optimizer = torch.optim.Adam([lv_guess], lr=0.01) \n",
    "    n_iter = 1\n",
    "    \n",
    "    # if desired, time the duration, otherwise, comment out this line\n",
    "    if mark_time:\n",
    "        start = time.time()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # instantiate the loss function module:\n",
    "    if optimiz_loss:\n",
    "        gradopt_module = GradOpt_Optimization_Loss_Module(tovae, config_dict, matprop_mpp_lookup) # <><><>< Don't forget to change the name of reverse_lookup\n",
    "        matprops = gradopt_module.matprops\n",
    "    else:\n",
    "        gradopt_module = GradOpt_MAE_Loss_module(tovae, config_dict, matprop_mpp_lookup)\n",
    "        matprops = gradopt_module.matprops\n",
    "\n",
    "    # iteration loop\n",
    "    for iteration in range(num_iterations):\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "       \n",
    "\n",
    "        if optimiz_loss:\n",
    "            # mpv_pred = gradopt_module.get_mpv_pred(lv_guess)   # LEFT OFF HERE... GOING THROUGH THE LOOP...\n",
    "            loss = gradopt_module(lv_guess)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            # Even if using Mean Absolute Error loss to target a specific MPV, the gradopt_module handles producing the predicted material property vector\n",
    "            mpv_pred = gradopt_module.get_mpv_pred(lv_guess)\n",
    "            loss = nn.L1Loss()(tgt_mpv, mpv_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step\n",
    "\n",
    "        n_iter += 1\n",
    "    \n",
    "    if mark_time:\n",
    "        end = time.time()\n",
    "        timetime = end - start\n",
    "        print(f\"iters time elapsed: {timetime}\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # store final latent  vec\n",
    "    rundic['final lv'] = lv_guess.detach().cpu().numpy()\n",
    "\n",
    "    # get final mpv pred, detach and store\n",
    "    mpv_pred_final = gradopt_module.get_mpv_pred(lv_guess).detach().cpu().numpy()\n",
    "    mpv_pred_final = np.squeeze(mpv_pred_final, axis=0)\n",
    "\n",
    "    preds_entry = []\n",
    "    preds_lookup = {}\n",
    "\n",
    "    for i, mp in enumerate(matprops):\n",
    "\n",
    "        preds_entry.append(mpv_pred_final[i])\n",
    "        if mp == 'volFrac':\n",
    "            vf_pred = mpv_pred_final[i]\n",
    "        elif mp == 'CH_11 scaled':\n",
    "            c11_scaled_pred = mpv_pred_final[i]\n",
    "            mp_name = mp.split(' ')[0]\n",
    "            pred_unscaled = unscale(sdb, mp_name, mpv_pred_final[i])\n",
    "            preds_entry.append(pred_unscaled)\n",
    "        else:\n",
    "            mp_name = mp.split(' ')[0]\n",
    "            pred_unscaled = unscale(sdb, mp_name, mpv_pred_final[i])\n",
    "            preds_entry.append(pred_unscaled)\n",
    "\n",
    "    predsdf.loc[len(predsdf)] = preds_entry\n",
    "\n",
    "    rundic['final predicted MPV'] = {prop: value for prop, value in zip(list(predsdf.columns), preds_entry)}\n",
    "\n",
    "    \n",
    "    lv_guess = torch.cat((lv_guess, tgt_vf_torch), dim=1)\n",
    "\n",
    "\n",
    "    # calculate predicted plateau stress and energy absorbed using the final predicted CH_11 -- NOTE THAT THIS CH_11 MAY BE EXTREMELY INACCURATE AND CAN ONLY BE ACCURATE THROUGH FEA VALIDATION\n",
    "    if dynamic_topopt:\n",
    "        sig_pl_scaled = calculate_dynamic_property('plateau stress', c11_scaled_pred)\n",
    "        sig_pl_unscaled = unscale(dyndb, 'plateau_stress_g', sig_pl_scaled[1])\n",
    "\n",
    "        w_scaled = calculate_dynamic_property('energy absorbed', c11_scaled_pred)\n",
    "        w_unscaled = unscale(dyndb, 'energy_absorbed_g', w_scaled[1])\n",
    "\n",
    "        dyn_val_dict[f'run {run}'] = {'CH_11 scaled - TOVAE prediction': c11_scaled_pred,\n",
    "                                      'plateau stress':{'scaled': sig_pl_scaled,\n",
    "                                                        'unscaled': sig_pl_unscaled},\n",
    "                                      'energy absorbed':{'scaled': w_scaled,\n",
    "                                                        'unscaled': w_unscaled}}\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    \n",
    "    # store decoded \n",
    "    decoded = decoder(lv_guess).detach().cpu().numpy()\n",
    "    decoded = np.squeeze(decoded, axis=(0,1))\n",
    "    rundic['decoded array'] = decoded\n",
    "    \n",
    "    # store binary array, targeted to predicted volume fraction threshold\n",
    "    binary_array = target_binarray_threshold(decoded, vf_pred)[0]\n",
    "    rundic['binary array'] = binary_array\n",
    "    \n",
    "    # save binary array as .mat file for FEA homogenization in MATLAB\n",
    "    matname = f'result{run}_{run_suffix}.mat'\n",
    "    matpath = os.path.join(matdir, matname)\n",
    "\n",
    "    matfile_dict = {'arr_0': binary_array}\n",
    "\n",
    "    savemat(matpath, matfile_dict)\n",
    "\n",
    "    # save plot of binary array\n",
    "    binary_plotpath = os.path.join(plotdir, f'{matname[:-4]}_binary')\n",
    "    Plot_Array(binary_array, binary=True, marker_size=5, symbol='square', show=False, export_png=True, plotpath=binary_plotpath)\n",
    "\n",
    "    # save plot of continuous array\n",
    "    continuous_plotpath = os.path.join(plotdir, f'{matname[:-4]}_continuous')\n",
    "    Plot_Array(decoded, binary=False, marker_size=5, symbol='square', show=False, export_png=True, scale_markers=True, plotpath=continuous_plotpath)\n",
    "    \n",
    "    \n",
    "    # add run to full dictionary\n",
    "    output_dict[f'run {run}'] = rundic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary of arrays as desired\n",
    "\n",
    "\n",
    "output_dict_pkl_name = f'{batch_folder_name}_run_dict.pkl'\n",
    "pklpath = os.path.join(batch_path, output_dict_pkl_name)\n",
    "save_dict_to_pickle(pklpath, output_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dynamic results dict\n",
    "\n",
    "output_dyndict_pkl_name = f'{batch_folder_name}_dynamic_properties_run_dict.pkl'\n",
    "pklpath = os.path.join(batch_path, output_dyndict_pkl_name)\n",
    "save_dict_to_pickle(pklpath, dyn_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results dataframe to csv file\n",
    "\n",
    "df_name = f'{batch_folder_name}_results_table.csv'\n",
    "dfpath = os.path.join(batch_path, df_name)\n",
    "predsdf.to_csv(dfpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
