{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Generate topologies using TopoGen-VAE__\n",
    "### <u>Overview</u>: Generate topologies using a trained Topology-Generating Variational Autoencoder (TopoGen-VAE).\n",
    "### This notebook contains two different ways to generate topologies - individually or in sets:\n",
    "### 1. individaully - generate a randomly sampled vector, select a target volume fraction and append to vector, decode into array, convert to binary.\n",
    "### 2. In sets - randomly sample vectors as the topology vector, iteratively attach volume fractions to the vector, decode and convert to binary. This produces sets of topologies with varying volume fractions, which adheres to the structure of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For setting the directory references for the entire package\n",
    "from ML_workflow_utils_v3.PackageDirectories import PackageDirectories as PD   \n",
    "\n",
    "# This code automatically sets the rootpath as the directory the entire package is contained in, which is then called to initialize the PackageDirectories class below\n",
    "import os\n",
    "# check current path if desired\n",
    "# currentpath = os.getcwd()\n",
    "# print(currentpath)\n",
    "\n",
    "os.chdir('../../../')\n",
    "rootpath = os.getcwd()\n",
    "# print(rootpath)\n",
    "\n",
    "# Alternately, rootpath can be set manually\n",
    "# rootpath = 'filepath/containing/entire/ML_package/'\n",
    "\n",
    "directory = PD(rootpath = rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from ML_workflow_utils_v3.Dataset_Preprocessor import Dataset_Preprocessor as DataP\n",
    "source_data_path = directory.source_data_path\n",
    "voxel_dir = directory.voxeltopo_path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpath = directory.nb_2_2_path\n",
    "\n",
    "\"\"\"\n",
    "If loading the TopoGen-VAE that was shipped with this package, execute these three lines of code\n",
    "\"\"\"\n",
    "cp_dir = directory.topogen_vae_path\n",
    "cp_name = \"TopoGen_VAE_pretrained_model_weights.pth\"\n",
    "cp_path = os.path.join(cp_dir, cp_name)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If loading a model trained in Notebook 2.1 (2_1_Train_TopoGen_VAE), comment out the three lines of code\n",
    "above by highlighting and pressing Ctrl+/, or putting '#' at the beginning of each line\n",
    "\"\"\"\n",
    "# cp_dir = os.path.join(directory.nb_2_1_path, 'model_CPs')\n",
    "# cp_name = 'TopoGen_VAE_28AUG24' # placeholder, replace\n",
    "# cp_path = os.path.join(cp_dir, cp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables for VAE configuration\n",
    "\n",
    "# latent_dim is the dimension of the \"encoded\"\n",
    "latent_dim = 16\n",
    "\n",
    "# 1 material properties - Volume Fraction\n",
    "matprops = ['volFrac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using pre-trained model\n",
    "from ML_workflow_utils_v3.TopoGen_VAE_pretrained import TGVAE_pretrained\n",
    "tgvae = TGVAE_pretrained(latent_dim, matprops)\n",
    "\n",
    "# if using model trained in notebook 2_1\n",
    "# from ML_workflow_utils_v3.TopoGen_VAE import TGVAE\n",
    "# tgvae = TGVAE(latent_dim, matprops)\n",
    "\n",
    "\n",
    "from ML_workflow_utils_v3.Model_Weights_Util import convert_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE model\n",
    "\n",
    "# If multiple GPUs are available, set to True\n",
    "gpu_parallel = False\n",
    "\n",
    "if gpu_parallel:\n",
    "    tgvae = torch.nn.DataParallel(tgvae).to(device)\n",
    "else:\n",
    "    tgvae = tgvae.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This command loads the trained model's weights\n",
    "\n",
    "The included pre-trained model weights were produced using a multi-GPU training setup, therefore the key of each layer contains \"module.\", \n",
    "which is how torch.nn.DataParallel() creates state dictionaries. The call of \"convert_state_dict\"\n",
    "If using more than one GPU for topology production, set \"convert_weight_keys_to\" to 'parallel'\n",
    "\"\"\"\n",
    "\n",
    "convert_weight_keys_to = 'non-parallel'\n",
    "\n",
    "model_weights = convert_state_dict(cp_path, convert_to = convert_weight_keys_to)\n",
    "\n",
    "\n",
    "tgvae.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TopoGen-VAE to evaluation mode\n",
    "tgvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a variable to call the decoder module of the TopoGen-VAE and set it to evaluation mode\n",
    "if convert_weight_keys_to == 'non-parallel':\n",
    "    decoder = tgvae.decoder\n",
    "elif convert_weight_keys_to == 'parallel':\n",
    "    decoder = tgvae.module.decoder\n",
    "\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a single topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target volume fraction -- between 0.0 and 1.0. For design purposes, we recommend below 0.5\n",
    "tgt_volfrac_val = 0.4\n",
    "tgt_volfrac = torch.Tensor([tgt_volfrac_val]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding vector\n",
    "latent_vec = torch.randn(latent_dim).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine embedding vector with volume fraction\n",
    "full_vec = torch.cat((latent_vec, tgt_volfrac), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode full vector into topology\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # decoded_array = np.squeeze(decoder(full_vec.cuda()).detach().cpu().numpy(), axis=(0,1))\n",
    "    decoded_array = np.squeeze(decoder(full_vec.to(device)).detach().cpu().numpy(), axis=(0,1))\n",
    "\n",
    "else:\n",
    "    decoded_array = np.squeeze(decoder(full_vec).detach().numpy(), axis=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_workflow_utils_v3.Voxel_Mesh_Utils import target_binarray_threshold, Plot_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert continuous valued output to binary\n",
    "\n",
    "binary_array, threshold, binary_volfrac = target_binarray_threshold(decoded_array, tgt_volfrac_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path for saving outputs\n",
    "savedir = os.path.join(nbpath, 'synthetic_topos_individual')\n",
    "\n",
    "# this name can be anything, but we recommend including volume fraction for reference\n",
    "arrayname = f'test_vf_{str(tgt_volfrac_val)}'\n",
    "arraypath = os.path.join(savedir, arrayname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot continuous array if desired - see DataPrep.py for fields. As shipped, this call will display the plot but not save it as a PNG file.\n",
    "\n",
    "# if desired to save plot, set to True\n",
    "export = False\n",
    "if export:\n",
    "    plotpath = os.path.join(savedir, f'{arraypath}_continuous')\n",
    "else:\n",
    "    plotpath = None\n",
    "# if desired to not display, set to False\n",
    "showplot = False\n",
    "\n",
    "Plot_Array(decoded_array, binary=False, show=showplot, export_png=export, marker_size = 6, symbol='square', x_range=(0,64), scale_markers=True, plotpath = plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot binary array\n",
    "\n",
    "# if desired to save plot, set to True\n",
    "export = False\n",
    "if export:\n",
    "    plotpath = os.path.join(savedir, f'{arraypath}_binary')\n",
    "else:\n",
    "    plotpath = None\n",
    "# if desired to not display, set to False\n",
    "showplot = True\n",
    "\n",
    "Plot_Array(binary_array, binary=True, show=showplot, export_png=export, marker_size = 6, symbol='square', x_range=(0,64), plotpath = plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the array, if desired for later FEA analysis\n",
    "\n",
    "# Select save format\n",
    "# as a .mat file for MATLAB\n",
    "matlab_format = True\n",
    "# as numpy format .npy\n",
    "numpy_format = True\n",
    "# as numpy compressed format .npz\n",
    "numpy_compressed_format = True\n",
    "\n",
    "\n",
    "if matlab_format:\n",
    "    from scipy.io import savemat\n",
    "    matfile_dict = {'arr_0': binary_array,\n",
    "                    'target volume fraction': tgt_volfrac_val}\n",
    "\n",
    "    matpath = f'{arraypath}.mat'\n",
    "    savemat(matpath, matfile_dict)\n",
    "\n",
    "if numpy_format:\n",
    "    np.save(arraypath, binary_array)\n",
    "\n",
    "if numpy_compressed_format:\n",
    "    np.savez(arraypath, binary_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Generate a batch of topologies with varying volume fractions__\n",
    "\n",
    "## __Note__: The names of topologies consist of a noun and an adjective drawn using the NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random synset word without hyphens or underscores\n",
    "\n",
    "def get_random_word(pos):\n",
    "    while True:\n",
    "        synsets = list(wn.all_synsets(pos))\n",
    "        random_synset = random.choice(synsets)\n",
    "        word = random_synset.lemmas()[0].name()\n",
    "        if not re.search(r'[-_/]', word) and len(word) <= 8:\n",
    "            return word.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique topologies to generate\n",
    "num_topos = 25\n",
    "\n",
    "# Generate unique topology names (adjective + noun) for each vector\n",
    "topo_names = []\n",
    "for _ in range(num_topos):\n",
    "    adj = get_random_word(\"a\")  # Adjective\n",
    "    noun = get_random_word(\"n\")  # Noun\n",
    "    UC_topo_name = adj + noun\n",
    "    topo_names.append(UC_topo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topo_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the range and increments of volume fractions for each generated topology. As shipped, the volume fraction range generates 13 increments between 0.1 and 0.61 as follows:\n",
    "##### 0.1, 0.14, 0.18, 0.22, 0.26, 0.3, 0.34, 0.38, 0.42, 0.46, 0.5, 0.54, 0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set volume fractions using numpy.arange, use numpy.round to round all values to 2 decimal places\n",
    "volfracs = np.round(np.arange(0.1,0.61,0.04), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent vector generation\n",
    "Latent vectors can be drawn from a random-normal distribution or can be distributed evenly using \n",
    "[Latin Hypercube Sampling](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Latin,Hypercube.html) on [0,1] with an inverse normal sampling to the Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = 'LHS' # or 'randnorm'\n",
    "\n",
    "\n",
    "if sampling == 'LHS':\n",
    "    from scipy.stats.qmc import LatinHypercube as LHS\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    LHsampler = LHS(d=16)\n",
    "\n",
    "    lhs_array = LHsampler.random(n=num_topos)\n",
    "\n",
    "    vector_array = lhs_norm_array = norm.ppf(lhs_array)\n",
    "\n",
    "elif sampling == 'randnorm':\n",
    "\n",
    "    vector_array = torch.randn(num_topos, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine latent vectors if desired\n",
    "print(vector_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i in range(vector_array.shape[0]):\n",
    "    \n",
    "    if sampling == 'LHS':\n",
    "        vec = np.expand_dims(vector_array[i,:], 0)\n",
    "        vec = torch.from_numpy(vec).to(dtype=torch.float32)\n",
    "        vectors.append(vec)\n",
    "    elif sampling == 'randnorm':\n",
    "        vec = np.expand_dims(vector_array[i,:], 0)\n",
    "        vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of the vectors and their topology names\n",
    "vectordic = {tname: vector for tname, vector in zip(topo_names, vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for holding each topology\n",
    "gen_data_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders for produced batches\n",
    "batch_num = 1\n",
    "date = '15SEP24'\n",
    "\n",
    "# If desired to save the binary arrays as .mat files for FEA in MATLAB, set to True\n",
    "save_as_mat = True\n",
    "\n",
    "# If desired to save the binary arrays as .npz files (numpy format), set to True\n",
    "save_as_npz = True\n",
    "\n",
    "batch = f'batch{batch_num}_{num_topos}topos_{date}'\n",
    "\n",
    "batchpath = os.path.join(nbpath, f'synthetic_topos_batch/{batch}')\n",
    "\n",
    "os.makedirs(batchpath, exist_ok=True)\n",
    "\n",
    "if save_as_npz:\n",
    "    npz_path = os.path.join(batchpath, 'voxel_array_files')\n",
    "    os.makedirs(npz_path, exist_ok=True)\n",
    "\n",
    "if save_as_mat:\n",
    "    from scipy.io import savemat\n",
    "    mat_dir= os.path.join(batchpath, 'mat_files')\n",
    "    os.makedirs(mat_dir, exist_ok=True)\n",
    "\n",
    "# If desired to plot one of each produced array for reference, uncomment these lines\n",
    "# Folder for plots of continuous-valued arrays\n",
    "continuous_plot_path = os.path.join(batchpath, 'continuous_voxel_plots')\n",
    "os.makedirs(continuous_plot_path, exist_ok=True)\n",
    "# Folder for plots of binary arrays\n",
    "binary_plot_path = os.path.join(batchpath, 'binary_voxel_plots')\n",
    "os.makedirs(binary_plot_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While producing the topologies, select a volume fraction to plot from the range of volume fractions, as shipped = 38%\n",
    "plotvf = 0.38\n",
    "# Flags for plotting continuous and/or binary valued arrays\n",
    "plot_continuous = True\n",
    "plot_binary = True\n",
    "\n",
    "# If desired to save each synthetic array as an .npz file, set to True\n",
    "save_as_npz = True\n",
    "\n",
    "for topo, vector in vectordic.items():\n",
    "    gen_data_dic[topo] = {}\n",
    "    gen_data_dic[topo]['base vector'] = vector\n",
    "    \n",
    "    for num, vf in enumerate(volfracs):\n",
    "        voxdic = {}\n",
    "        vfrnd = np.round(vf*100,1)\n",
    "        vf_entry = 'VF'+f'{vfrnd:.0f}'+'%'\n",
    "        # voxdic[vf_entry] = {}\n",
    "\n",
    "        save_name = f'{topo}_{vf}'\n",
    "        \n",
    "        # Modify the base vector for volume fraction\n",
    "        # vf = vf.astype(np.float32)\n",
    "        vf_tensor = torch.tensor([[vf.astype(np.float32)]])\n",
    "        # vf_tensor = torch.tensor([[vf]]).astype(float32)\n",
    "        vec = torch.cat((vector, vf_tensor), dim=1)\n",
    "        # voxdic[vf_entry]['modified vector'] = vec\n",
    "\n",
    "        # Decode the vector into the topology voxel array\n",
    "        # decoded_array = np.squeeze(decoder(vec.cuda()).detach().cpu().numpy(), axis=(0,1))\n",
    "        decoded_array = np.squeeze(decoder(vec.to(device)).detach().cpu().numpy(), axis=(0,1))\n",
    "\n",
    "        \n",
    "        # voxdic[vf_entry]['decoded array'] = dec_array\n",
    "        \n",
    "        # Convert the array to binary based on a threshold\n",
    "        binary_array, threshold, volfrac = target_binarray_threshold(decoded_array, vf, 64)\n",
    "\n",
    "        \n",
    "        # voxdic[vf_entry]['binary array'] = binary_array\n",
    "        \n",
    "        # Compute the volume fraction of the binary array\n",
    "        volfrac = (np.sum(binary_array == 1)) / (64**3)\n",
    "        \n",
    "        # Format the volume fraction\n",
    "        volfrac_pct = str(np.round(volfrac,3) * 100)+' %'\n",
    "        volfrac_str = f'{volfrac:.5f}'\n",
    "        \n",
    "\n",
    "        \n",
    "        voxdic = {'Target volume fraction': vf,\n",
    "                  'Modified vector': vec,\n",
    "                  'Sampling method': sampling,\n",
    "                  'decoded array': decoded_array,\n",
    "                  'binary array': binary_array,\n",
    "                  'Actual volume fraction': {'percent': volfrac_pct,\n",
    "                                     'decimal': volfrac,\n",
    "                                     'string':  volfrac_str},\n",
    "                 }\n",
    "        \n",
    "        if save_as_mat:\n",
    "            mat_mesh_dict = {'arr_0': binary_array}\n",
    "            mat_name = f'{save_name}.mat'\n",
    "            mat_path = os.path.join(mat_dir, mat_name)\n",
    "            savemat(mat_path, mat_mesh_dict)\n",
    "\n",
    "        if save_as_npz:\n",
    "            array_path = os.path.join(npz_path, save_name)\n",
    "\n",
    "            np.savez(array_path, decoded_array)\n",
    "\n",
    "        \n",
    "        gen_data_dic[topo][vf_entry] = voxdic\n",
    "\n",
    "        if vf == plotvf:\n",
    "            # print(f\"plotvf reached! - {vf}\")\n",
    "            if plot_continuous:\n",
    "                plotpath = os.path.join(continuous_plot_path, f'{save_name}_continuous')\n",
    "                # print(plotpath)\n",
    "                Plot_Array(decoded_array, plotpath = plotpath, binary=False, scale_markers=True, export_png=True, show=False)\n",
    "\n",
    "            if plot_binary:\n",
    "                # print('plotting binary!')\n",
    "                plotpath = os.path.join(binary_plot_path, f'{save_name}_binary')\n",
    "                Plot_Array(binary_array, plotpath = plotpath, binary=True, bincolor='black', lincolor='gray', marker_size=5, export_png=True, show=False)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dictionary of generated topologies to a python Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving dictionary to / loading from Python pickle file\n",
    "from ML_workflow_utils_v3.Misc_Utils import save_dict_to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dic_path = os.path.join(batchpath, f'generated_topos_dict_{batch}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_pickle(gen_data_dic_path, gen_data_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __End of Notebook__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
