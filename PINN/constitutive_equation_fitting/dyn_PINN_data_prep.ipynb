{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedat = VoxelDataset(datapre.idxTe, dirs.voxel_PN_filepath, params)\n",
    "teloader = DataLoadertedat, batch_size = batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                  \n",
    "#non-zero params:\n",
    "\n",
    "['volFrac', 'CH_11 scaled', 'CH_12 scaled', 'CH_13 scaled',  'CH_22 scaled', 'CH_23 scaled',\n",
    "'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled', \n",
    "'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']\n",
    "\"\"\"\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpath     = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN'\n",
    "voxel_path = '/home/mgolub4/DLproj/MLTO_2023/0_data/source_MAT_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = os.path.join(nbpath, 'dyn_data', 'dynamic_static_database_scaled_DEC23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyndb = pd.read_csv(dbpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Completed 14 or 15 March, no need to run it again\n",
    "# # found the .mat files for the meshes in the dynamic database and saved them as .npy files in the .../dyn_data/voxel_arrays directory\n",
    "# for topfam in dyndb.topology_family.unique():\n",
    "#     tfdf = dyndb[dyndb.topology_family == topfam]\n",
    "#     for ctype in tfdf.cell_type.unique():\n",
    "#         ctdf = tfdf[tfdf.cell_type == ctype]\n",
    "\n",
    "#         for i in ctdf.index:\n",
    "#             matfolder = os.path.join(voxel_path, topfam)\n",
    "#             dimidx = ctdf.dim_idx[i]\n",
    "#             matfilename = ctdf.cell_type[i] + '_' + f'{dimidx:.1e}' + '.mat'\n",
    "#             mat = loadmat(os.path.join(matfolder, matfilename))\n",
    "#             mesharray = mat[list(mat.keys())[-1]]\n",
    "#             mesharray = mesharray.astype(np.int8)\n",
    "#             npyname = topfam +'_' + matfilename[:-4]\n",
    "#             npypath = os.path.join(voxpath, npyname)\n",
    "#             np.save(npypath, mesharray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxpath = os.path.join(nbpath, 'dyn_data/voxel_arrays')\n",
    "stress_ser_path = os.path.join(nbpath, 'dyn_data/stress_series_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTr, idxRem = TTS(dyndb, stratify = dyndb['topology_family'], random_state=42, train_size = 0.8)\n",
    "idxVal, idxTe = TTS(idxRem, random_state = 42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idxTr.shape)\n",
    "print(idxVal.shape)\n",
    "print(idxTe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dict = {'Xtr': [], 'Xval': [], 'Xte': []}\n",
    "\n",
    "for df, array_split in zip([idxTr, idxVal, idxTe], array_dict.keys()):\n",
    "    arrs= []\n",
    "    for i in df.index:\n",
    "        topfam = df.topology_family[i]\n",
    "        celltype = df.cell_type[i]\n",
    "        dim_idx = df.dim_idx[i]\n",
    "        meshname = topfam + '_' + celltype + '_' + f'{dim_idx:.1e}.npy'\n",
    "        mesh = np.load(os.path.join(voxpath, meshname))\n",
    "        arrs.append(mesh)\n",
    "\n",
    "    arrs = np.expand_dims(np.asarray(arrs), axis=1)\n",
    "    array_dict[array_split] = arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = array_dict['Xtr']\n",
    "Xval = array_dict['Xval']\n",
    "Xte = array_dict['Xte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in split_data:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_suffix = 'gsreg'\n",
    "untruncated = True\n",
    "cutoff = 750\n",
    "\n",
    "target_dict = {'Ytr': [], 'Yval': [], 'Yte': []}\n",
    "\n",
    "# Loop over the splits\n",
    "for split, tgt in zip([idxTr, idxVal, idxTe], target_dict.keys()):\n",
    "    # Go through each split row by row, load each dynamic data series, pull selected\n",
    "    split_data = []\n",
    "    for idx in split.index:\n",
    "        dyn_file_name = split['dyn_file_name_original'][idx]\n",
    "        \n",
    "        if untruncated == True:\n",
    "            file_suffix = '_processed_untrunc'\n",
    "        else:\n",
    "            file_suffix = '_proct_gaus_btrlp_fftlp'\n",
    "\n",
    "        dyn_file_full_name = dyn_file_name + file_suffix + '.csv'\n",
    "\n",
    "        dyn_file_path = os.path.join(stress_ser_path, dyn_file_full_name)\n",
    "\n",
    "        ser_df = pd.read_csv(dyn_file_path)\n",
    "\n",
    "        strain = np.expand_dims(ser_df['Strain'][::10].values, axis=0)\n",
    "\n",
    "        stress_col = f'stress_bottom_{data_suffix}'\n",
    "\n",
    "        stress = np.expand_dims(ser_df[stress_col][::10].values, axis=0)\n",
    "\n",
    "        strs_strn_series = torch.tensor(np.concatenate([stress, strain], axis=0))\n",
    "\n",
    "        split_data.append(strs_strn_series)\n",
    "\n",
    "    # split_data = split_data\n",
    "\n",
    "    target_dict[tgt] = split_data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_sequence([*target_dict['Yte'][:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in target_dict['Yte']:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenslens = []\n",
    "# Loop over the splits\n",
    "for split, tgt in zip([idxTr, idxVal, idxTe], target_dict.keys()):\n",
    "    # Go through each split row by row, load each dynamic data series, pull selected\n",
    "    split_data = []\n",
    "    for idx in split.index:\n",
    "        dyn_file_name = split['dyn_file_name_original'][idx]\n",
    "        \n",
    "        if untruncated == True:\n",
    "            file_suffix = '_processed_untrunc'\n",
    "        else:\n",
    "            file_suffix = '_proct_gaus_btrlp_fftlp'\n",
    "\n",
    "        dyn_file_full_name = dyn_file_name + file_suffix + '.csv'\n",
    "\n",
    "        dyn_file_path = os.path.join(stress_ser_path, dyn_file_full_name)\n",
    "\n",
    "        ser_df = pd.read_csv(dyn_file_path)\n",
    "\n",
    "        lenslens.append(len(ser_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lenslens).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'1A': [0.60,0.65],\n",
    "# '1B': [0.45, 0.5],\n",
    "# '1C': [0.15],\n",
    "# '2A': [0.35, 0.40, 0.45, 0.50],\n",
    "# '2B': [0.50,],\n",
    "# '2C': [0.45,],\n",
    "# '3B': [0.20, 0.25],\n",
    "# '3C': [0.45,],\n",
    "# '4A': [0.15,],\n",
    "# '4B': [0.25,],\n",
    "# '4C': [0.65,],}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "# Assuming your data is a list of variable-length sequences\n",
    "data = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6, 7, 8, 9])]\n",
    "\n",
    "# Pack the sequences\n",
    "packed_sequence = pack_sequence(data, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = os.path.join(stress_ser_path, 'RhoCross_0p14_sr1000_proct_gaus_btrlp_fftlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdf = pd.read_csv(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = np.expand_dims(csvdf['Strain'][::10].values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain = np.expand_dims(csvdf['stress_bottom_gsreg'][::10].values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([stress, strain], axis=0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
