{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import workflow_utils_v3\n",
    "import sys\n",
    "\n",
    "from workflow_utils_v3.FileDirectory import Directory\n",
    "\n",
    "dirs = Directory(rootpath = '/home/mgolub4/DLproj/MLTO_2024/')\n",
    "\n",
    "# Sets directory of entire package\n",
    "# rootpath = '/data/tigusa1/MLTO_UCAH/MLTO_2023/'\n",
    "\n",
    "nbpath = os.path.join(dirs._3_Dynamic_PINN_RNN, 'PINN_training', 'arch1_singleLSTM')\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle\n",
    "from plotly.colors import sequential, qualitative\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "# import torchsummary\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split as TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '30APR24'\n",
    "fname_base = f'Dyn_PINN_v0_{date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready.csv'\n",
    "dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready.csv'\n",
    "dyndb = pd.read_csv(dyndb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_min_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt', 'plateau_stress_g', 'energy_absorbed_g']\n",
    "# dict_entries = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt', 'sig_pl', 'W']\n",
    "\n",
    "\n",
    "# Modifying from above after calculating sig_pl and W from scaled time series\n",
    "max_min_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt'] # 'plateau_stress_g', 'energy_absorbed_g' <<-- in the revised version, these were calculated from the scaled stress/strain series, so pulling their column-wise max-min won't work\n",
    "dict_entries = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt'] # 'sig_pl', 'W'\n",
    " \n",
    "max_min_dict = {}\n",
    "\n",
    "for par, name in zip(max_min_params, dict_entries):\n",
    "    data = dyndb[par]\n",
    "    max = data.max()\n",
    "    min = data.min()\n",
    "    max_min_dict[name] = (max, min) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTr, idxRem = TTS(dyndb, stratify = dyndb['topology_family'], random_state=42, train_size = 0.8)\n",
    "idxVal, idxTe = TTS(idxRem, random_state = 42, test_size=0.5)\n",
    "\n",
    "idxTr = idxTr.reset_index()\n",
    "idxVal = idxVal.reset_index()\n",
    "idxTe = idxTe.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['volFrac', \n",
    "        'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "        'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "        'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "        'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "        'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "        'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "        'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stress_Series:\n",
    "    def __init__(self, series):\n",
    "        self.series = series\n",
    "        self.max = series.max()\n",
    "        self.min = series.min()\n",
    "\n",
    "    def scale(self):\n",
    "        return (self.series - self.min) / (self.max - self.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def min_max_scale_series(batched_time_series):\n",
    "    # Calculate the maximum and minimum values for each series\n",
    "    max_values = torch.max(batched_time_series, dim=1)[0]\n",
    "    min_values = torch.min(batched_time_series, dim=1)[0]\n",
    "\n",
    "    # Calculate the range for each series\n",
    "    series_range = max_values - min_values\n",
    "\n",
    "    # Ensure non-zero range to avoid division by zero\n",
    "    # series_range = torch.where(series_range == 0, torch.tensor(1e-7), series_range)\n",
    "\n",
    "    # Min-max scale each series\n",
    "    scaled_time_series = (batched_time_series - min_values.unsqueeze(1)) / series_range.unsqueeze(1)\n",
    "\n",
    "    return scaled_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyndb.loc[12, 'energy_absorbed_g scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Dataset(Dataset):\n",
    "    def __init__(self, params, split_dataframe,\n",
    "                 feat_vec_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/voxel_embedding_feature_maps', \n",
    "                 stress_series_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/stress_series_data', \n",
    "                 stress_ser_suffix = '_proct_w_constit_eqn_and_scaled_series',\n",
    "                 predicted_parameters=True,\n",
    "                  ):\n",
    "        \n",
    "        self.df = split_dataframe\n",
    "        self.featvec_dir = feat_vec_directory # for pulling the feature vectors\n",
    "        self.stress_ser_dir = stress_series_directory # for pulling the time series files\n",
    "        self.params = params\n",
    "        self.predicted_parameters = predicted_parameters\n",
    "        # self.const_eqn_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]\n",
    "        self.const_eqn_params = ['A_opt scaled', 'B_opt scaled', 'C_opt scaled', 'm_opt scaled', 'n_opt scaled',]\n",
    "\n",
    "        self.stress_ser_suffix = stress_ser_suffix\n",
    "        # self.scale_coeffs_by = scale_coeffs_by\n",
    "\n",
    "        # self.scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dyn_series_fname = self.df['dyn_file_name_original'].iloc[idx]\n",
    "\n",
    "        # sig_pl = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['plateau_stress_g scaled'].values[0].astype(np.float32)\n",
    "        sig_pl = np.asarray(self.df.loc[idx, 'plateau_stress_g scaled'].astype(np.float32))\n",
    "\n",
    "        # W = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['energy_absorbed_g scaled'].values[0].astype(np.float32)\n",
    "        W = np.asarray(self.df.loc[idx, 'energy_absorbed_g scaled'].astype(np.float32))\n",
    "\n",
    "\n",
    "        # feature vector from convolutional neural network convolutional layers output\n",
    "        featvec_fname = self.df.loc[idx, 'conv_feat_vec']+ '.npy'\n",
    "        featvec_path = os.path.join(self.featvec_dir, featvec_fname)\n",
    "        featvec = np.load(featvec_path)\n",
    "        featvec = np.squeeze(featvec, axis=0).astype(np.float32)\n",
    "\n",
    "        # constitutive equation parameters\n",
    "        # constit_eqn_coeffs = np.asarray(self.df[self.const_eqn_params].iloc[idx])\n",
    "\n",
    "        # for i, scaler in enumerate(self.scale_coeffs_by):\n",
    "        #     constit_eqn_coeffs[i] = constit_eqn_coeffs[i] / scaler\n",
    "            \n",
    "\n",
    "\n",
    "        # predicted parameters\n",
    "        if self.predicted_parameters:\n",
    "            paramvec = np.asarray([self.df.loc[idx, f'pred {par}'] for par in self.params]).astype(np.float32)\n",
    "        else:\n",
    "            paramvec = np.asarray([self.df.loc[idx, f'{par}']for par in self.params]).astype(np.float32)\n",
    "\n",
    "        # stress_series -- for now (April 24), Imma use the truncated datasets, because I think padded batches for RNNs in pytorch will take care of differing lengths\n",
    "        stress_ser_fname = dyn_series_fname + self.stress_ser_suffix\n",
    "        stress_ser_path = os.path.join(self.stress_ser_dir, stress_ser_fname+'.csv')\n",
    "\n",
    "        stress_series_data = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_gsreg_scaled']).astype(np.float32)\n",
    "        stress_series_coneq = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_constitutive_equation_scaled']).astype(np.float32)\n",
    "        \n",
    "        # stress_series = Stress_Series(stress_series)\n",
    "        # scaled_stress_series = stress_series.scale()\n",
    "        # stress_series_dic = {'stress_series': stress_series.series, 'max': stress_series.max, 'min': stress_series.min}\n",
    "\n",
    "        # stress_series_dic = {'stress_series': scaled_stress_series, 'max': stress_series.max, 'min': stress_series.min}\n",
    "\n",
    "        strain = np.asarray(pd.read_csv(stress_ser_path)['Strain']).astype(np.float32)\n",
    "        strain_end = len(strain)\n",
    "        strain_end = np.float32(strain_end)\n",
    "                   \n",
    "\n",
    "        return featvec, paramvec, W, sig_pl, strain_end, strain, stress_series_data, stress_series_coneq#, idx\n",
    "            #   0       1         2  3       4           5       6                   7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using this to check that the length of the original strain is the right ending index for the strain_end, to prevent (I hope, I pray...) negative W values from torch.trapz\n",
    "# ls = [0,1,2,3,4,5]\n",
    "# ls2 = [0,1,2,3,4,5,0]\n",
    "# ln = len(ls)\n",
    "# print(ls[:ln])\n",
    "# print(ls2[:ln])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain\n",
    "# return nonseries_list, padded_strain_ser, padded_stress_ser\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def padded_collate(batch):\n",
    "    # get list of stress series -- uneven lengths, but will be padded at end\n",
    "    stress_series_data = [torch.tensor(elem[-2], requires_grad=True) for elem in batch]\n",
    "    stress_series_coneq = [torch.tensor(elem[-1], requires_grad=True) for elem in batch]\n",
    "\n",
    "    # get max and min values as a list of lists, where the first value is the maximum and second is the minimum value of the accompanying stress series\n",
    "\n",
    "    strn_ends = [item[4] for item in batch]\n",
    "    # print(strn_ends)\n",
    "    strn_ends = np.asarray(strn_ends)\n",
    "\n",
    "    # get strain series, convert to tensor\n",
    "    strain_ser = [torch.tensor(item[-3], requires_grad=True) for item in batch]\n",
    "    \n",
    "    # get other data (featvec, paramvec, constit_eqn_coeffs, W, sig_pl) from data\n",
    "    nonserbatch = [item[0:-3] for item in batch] # all until last element (stress series) of list given by PINN_Dataset\n",
    "\n",
    "    nonseries_list = []\n",
    "\n",
    "    # nonser_len = len(nonserbatch)\n",
    "    for i in range(5):\n",
    "        data_list = []\n",
    "        # for j, data in enumerate(nonserbatch):\n",
    "        for data in nonserbatch:\n",
    "\n",
    "            # print(j, type(data))\n",
    "            # print(type(data))\n",
    "            data_list.append(data[i])\n",
    "        # print(data_list)\n",
    "        data_list = torch.tensor(np.asarray(data_list), requires_grad=True)\n",
    "        nonseries_list.append(data_list)\n",
    "\n",
    "    \n",
    "    stress_ser_data_padded = pad_sequence(stress_series_data, batch_first=True, padding_value=0)\n",
    "    stress_ser_coneq_padded = pad_sequence(stress_series_coneq, batch_first=True, padding_value=0)\n",
    "\n",
    "    padded_strain_ser = pad_sequence(strain_ser, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    return nonseries_list, padded_strain_ser, stress_ser_data_padded, stress_ser_coneq_padded, strn_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "trdata = PINN_Dataset(params, idxTr)\n",
    "trloader = DataLoader(trdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "valdata = PINN_Dataset(params, idxVal)\n",
    "valloader = DataLoader(valdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "tedata = PINN_Dataset(params, idxTe)\n",
    "teloader = DataLoader(tedata, batch_size=batch_size, collate_fn = padded_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, coeff_max_min_dict = max_min_dict, offset = -0.01):\n",
    "        super(PINN_loss, self).__init__()\n",
    "\n",
    "        self.offset = offset\n",
    "        self.coeff_max_min_dict = coeff_max_min_dict\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, stress_series_predicted, stress_series_data, stress_series_coneq, W_data, sig_pl_data, strain, strain_end): # , , stress_max_min_array, <<-- don't think I need these because I'm calculating the loss as scaled\n",
    "        \n",
    "        stress_data_loss = nn.L1Loss()(stress_series_predicted.squeeze(-1), stress_series_data)\n",
    "        stress_coneq_loss = nn.L1Loss()(stress_series_predicted.squeeze(-1), stress_series_coneq)\n",
    "\n",
    "        W_pred = self.predicted_W(stress_series_predicted, strain, strain_end)\n",
    "\n",
    "        sig_pl_pred = self.predict_sig_pl(stress_series_predicted)\n",
    "\n",
    "        # print(f\"W_pred: {W_pred.detach().cpu().numpy()}\\tSig_pl: pred - {sig_pl_pred.detach().cpu().numpy()}\\t\") #data - {sig_pl_data.detach().cpu().numpy()}\")\n",
    "\n",
    "\n",
    "        W_loss = nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "        sig_pl_loss = nn.L1Loss()(sig_pl_pred, sig_pl_data)\n",
    "\n",
    "\n",
    "        total_loss = stress_data_loss + stress_coneq_loss + W_loss + sig_pl_loss\n",
    "\n",
    "        output_dic = {\n",
    "            'total_loss': total_loss,\n",
    "            'stress_data_loss': stress_data_loss,\n",
    "            'stress_coneq_loss': stress_coneq_loss,\n",
    "            'W_loss': W_loss,\n",
    "            'sig_pl loss': sig_pl_loss,\n",
    "            'predicted_sig_pl': sig_pl_pred, \n",
    "            'W_pred': W_pred, \n",
    "            'stress_series_predicted': stress_series_predicted, \n",
    "        }\n",
    "\n",
    "        return total_loss, stress_data_loss, stress_coneq_loss, W_loss, sig_pl_loss, W_pred, stress_series_predicted, output_dic\n",
    "\n",
    "\n",
    "\n",
    "    def unscale_coeffs(self, coeffs, coeff_max_min_dict, coeff_list =['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]):\n",
    "        unscaled_coeffs = torch.zeros_like(coeffs)\n",
    "\n",
    "        for i in range(coeffs.shape[0]):\n",
    "            for j, name in enumerate(coeff_list):\n",
    "                max = coeff_max_min_dict[name][0]\n",
    "                min = coeff_max_min_dict[name][1]\n",
    "\n",
    "                unscaled_coeffs[i][j] = (coeffs[i][j] * (max - min)) + min\n",
    "\n",
    "        return unscaled_coeffs\n",
    "    \n",
    "    def unscale_dyn_params(array, coeff_max_min_dict, dyn_param = 'sig_pl'):\n",
    "        max = coeff_max_min_dict[dyn_param][0]\n",
    "        min = coeff_max_min_dict[dyn_param][1]\n",
    "\n",
    "        array = array*(max - min) + min\n",
    "\n",
    "        return array\n",
    "\n",
    "\n",
    "    def predict_sig_pl(self, stress_series_predicted): # CALLED\n",
    "         \n",
    "        sig_pl_pred = torch.mean(stress_series_predicted[:, 200:400, 0], dim=1)\n",
    "\n",
    "        return sig_pl_pred\n",
    "    \n",
    "    def predicted_W(self, stress_series_pred, strain, strain_end): # CALLED\n",
    "\n",
    "        Ws_pred = []\n",
    "        # print(stress_pred.shape)\n",
    "        for i in range(stress_series_pred.shape[0]):\n",
    "            strain_end_idx = int(strain_end[i])\n",
    "            stress = stress_series_pred[i, :strain_end_idx, 0]\n",
    "            strain_ser = strain[i, :strain_end_idx]\n",
    "            # print(stress.shape, strain_ser.shape)\n",
    "            W_pred = torch.trapz(stress, strain_ser)\n",
    "\n",
    "            Ws_pred.append(W_pred)\n",
    "\n",
    "        Ws_pred = torch.stack(Ws_pred, dim=0)\n",
    "\n",
    "        return Ws_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, hidden_size=256, num_lstm_layers=4, stress_series_dim=1, coeff_dim=5):\n",
    "        numparams = len(params)\n",
    "        input_vec_dim = 1024 + numparams\n",
    "        \n",
    "        # self.series_in_dim = series_input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.stress_series_dim = stress_series_dim\n",
    "        self.coeff_dim = coeff_dim\n",
    "        super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "        self.lstm_linear = nn.Sequential(nn.Linear(hidden_size, 128), nn.ReLU(), nn.Linear(128, stress_series_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, feature_vec, property_vec, strain_series):\n",
    "        input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "        batch_size = strain_series.size(0)\n",
    "        h = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        c = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        \n",
    "        stress_ser = []\n",
    "\n",
    "        for i in range(strain_series.size(1)):\n",
    "            stress, (h, c) = self.stress_ser_predictor(input_vec, (h, c))\n",
    "\n",
    "            stress = self.lstm_linear(stress)\n",
    "            stress_ser.append(stress)\n",
    "       \n",
    "        stress_ser = torch.stack(stress_ser, dim=1)\n",
    "\n",
    "        return stress_ser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = Dynamic_Stress_PINN(params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# summary(pinn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns-->         featvec, paramvec, constit_eqn_coeffs, W, sig_pl, || strain,            || stress_series,\n",
    "# dataloader returns:->     ^--------------nonseries_list------------------^  || padded_strain_ser, || padded_stress_ser\n",
    "\n",
    "# dataset    returns:   featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain,               stress_series_dic\n",
    "# dataloader returns:   nonseries_list,                                   padded_strain_ser,    padded_stress_ser, max_mins\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "gamma = 1.0\n",
    "max_grad_norm = 1.0\n",
    "def pinn_train(pinn, dataloader, loss_func, optimizer, alpha=alpha, beta=beta, gamma=gamma): #PINN_loss\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    # running_loss = 0.0\n",
    "    total_run_loss = 0.0\n",
    "    stress_data_loss_run = 0.0\n",
    "    stress_coneq_loss_run = 0.0\n",
    "    # dyn_param_run_loss = 0.0\n",
    "    W_loss_run = 0.0\n",
    "    sig_pl_loss_run = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    for non_series_data, strain_series, stress_series_data, stress_series_coneq, strain_ends in pbar:\n",
    "        global feature_vec, param_vec, strn_ser_glob, strs_ser_data_glob, strs_ser_coneq_glob, strn_ends_glob\n",
    "        feature_vec = non_series_data[0].cuda()\n",
    "        param_vec   = non_series_data[1].cuda()\n",
    "        strs_ser_data_glob = stress_series_data = stress_series_data.cuda()\n",
    "        strs_ser_coneq_glob = stress_series_coneq = stress_series_coneq.cuda()\n",
    "        strn_ends_glob = strain_ends\n",
    "\n",
    "        W = non_series_data[2].cuda()\n",
    "        # print(f\"W shape: {W.shape}\")\n",
    "        sig_pl = non_series_data[3].cuda()\n",
    "        strn_ser_glob = strain_series = strain_series.cuda()\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        global stress_series_pred\n",
    "\n",
    "        stress_series_pred = pinn(feature_vec, param_vec, strain_series)\n",
    " \n",
    "        global total_loss, stress_data_loss, stress_coneq_loss, W_loss, sig_pl_loss, W_pred, stress_series_predicted, output_dic\n",
    "        total_loss, stress_data_loss, stress_coneq_loss, W_loss, sig_pl_loss, W_pred, stress_series_predicted, output_dic = loss_func()(stress_series_pred, stress_series_data, stress_series_coneq, W, sig_pl, strain_series, strain_ends)\n",
    "                \n",
    "        total_loss.backward() #retain_graph=True\n",
    "\n",
    "        optimizer.step()\n",
    "        # running_loss += loss_val.item()\n",
    "        total_run_loss += total_loss.item()\n",
    "        stress_data_loss_run += stress_data_loss.item()\n",
    "        stress_coneq_loss_run += stress_coneq_loss.item()\n",
    "        W_loss_run += W_loss.item()\n",
    "        sig_pl_loss_run += sig_pl_loss.item()\n",
    "        # pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "        pbar.set_description(f'Losses:\\tTotal: {total_run_loss / (pbar.n + 1):.4f}\\tstress_data: {stress_data_loss_run / (pbar.n + 1):.4f}\\tstress_coneq: {stress_coneq_loss_run / (pbar.n + 1):.4f}\\tW: {W_loss_run / (pbar.n + 1):.4f}\\tsig_pl: {sig_pl_loss_run / (pbar.n + 1):.4f}')\n",
    "        # print(\"Data index is: f{idx}\")\n",
    "    return total_run_loss / len(dataloader), stress_data_loss_run / len(dataloader), stress_coneq_loss_run / len(dataloader),W_loss_run / len(dataloader), sig_pl_loss_run / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_validate(pinn, dataloader, loss_func=PINN_loss):\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    # running_loss = 0.0\n",
    "    total_run_loss = 0.0\n",
    "    stress_data_loss_run = 0.0\n",
    "    stress_coneq_loss_run = 0.0\n",
    "    # dyn_param_run_loss = 0.0\n",
    "    W_loss_run = 0.0\n",
    "    sig_pl_loss_run = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    with torch.no_grad():\n",
    "        for non_series_data, strain_series, stress_series_data, stress_series_coneq, strain_ends in pbar:\n",
    "\n",
    "            feature_vec = non_series_data[0].cuda()\n",
    "            param_vec   = non_series_data[1].cuda()\n",
    "            stress_series_data = stress_series_data.cuda()\n",
    "            stress_series_coneq = stress_series_coneq.cuda()\n",
    "\n",
    "            W = non_series_data[2].cuda()\n",
    "            # print(f\"W shape: {W.shape}\")\n",
    "            sig_pl = non_series_data[3].cuda()\n",
    "            strain_series = strain_series.cuda()\n",
    "\n",
    "            # global stress_series_pred\n",
    "\n",
    "            stress_series_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "    \n",
    "            total_loss, stress_data_loss, stress_coneq_loss, W_loss, sig_pl_loss, W_pred, stress_series_predicted, output_dic = loss_func()(stress_series_pred, stress_series_data, stress_series_coneq, W, sig_pl, strain_series, strain_ends)\n",
    "        \n",
    "            # running_loss += loss_val.item()\n",
    "            total_run_loss += total_loss.item()\n",
    "            stress_data_loss_run += stress_data_loss.item()\n",
    "            stress_coneq_loss_run += stress_coneq_loss.item()\n",
    "            W_loss_run += W_loss.item()\n",
    "            sig_pl_loss_run += sig_pl_loss.item()\n",
    "            # pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "            pbar.set_description(f'Losses:\\tTotal: {total_run_loss / (pbar.n + 1):.4f}\\tstress_data: {stress_data_loss_run / (pbar.n + 1):.4f}\\tstress_coneq: {stress_coneq_loss_run / (pbar.n + 1):.4f}\\tW: {W_loss_run / (pbar.n + 1):.4f}\\tsig_pl: {sig_pl_loss_run / (pbar.n + 1):.4f}')\n",
    "            # print(\"Data index is: f{idx}\")\n",
    "    return total_run_loss / len(dataloader), stress_data_loss_run / len(dataloader), stress_coneq_loss_run / len(dataloader), W_loss_run / len(dataloader), sig_pl_loss_run / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training hyper(?)parameters\n",
    "model = pinn\n",
    "EPOCHS = 200\n",
    "\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "cp_name = f'CP_{fname_base}.pth'\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)\n",
    "print(best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 75\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "# earlystop_min_delta = 0.000075\n",
    "earlystop_min_delta = 0.00075 # For L1Loss (MAE)\n",
    "\n",
    "# os.makedirs(best_weights_path, exist_ok=True)\n",
    "best_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs_completed=0\n",
    "\n",
    "lrate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, weight_decay=0.001)#optim.Adam(pinn.parameters(), lr=0.00001)\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n",
    "lossfunc = PINN_loss\n",
    "lossfunc_name = 'PINN_loss'\n",
    "\n",
    "try:\n",
    "#    return total_run_loss / len(dataloader), stress_data_loss_run / len(dataloader), stress_coneq_loss_run / len(dataloader), W_loss_run / len(dataloader), sig_pl_loss_run / len(dataloader)\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train the model\n",
    "        train_loss = pinn_train(model, trloader, lossfunc, optimizer)\n",
    "        tr_loss_val = train_loss[0]\n",
    "        tr_data_loss = train_loss[1]\n",
    "        tr_coneq_loss = train_loss[2]\n",
    "        tr_W_loss = train_loss[3]\n",
    "        tr_sigpl_loss = train_loss[4]\n",
    "        \n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = pinn_validate(model, valloader, lossfunc)\n",
    "        val_loss_val = val_loss[0]\n",
    "        val_data_loss = val_loss[1]\n",
    "        val_coneq_loss = val_loss[2]\n",
    "        val_W_loss = val_loss[3]\n",
    "        val_sigpl_loss = val_loss[4]\n",
    "\n",
    "        print(f'training:\\ttotal loss: {tr_loss_val:.5f}, data loss: {tr_data_loss:.5f}\\tconst. eqn series loss: {tr_coneq_loss:.5f}\\tW loss: {tr_W_loss:.5f}\\tplateau stress loss: {tr_sigpl_loss:.5f}')\n",
    "        print(f'validation:\\ttotal loss: {val_loss_val:.5f}, data loss: {val_data_loss:.5f}\\tconst. eqn series loss: {val_coneq_loss:.5f}\\tW loss: {val_W_loss:.5f}\\tplateau stress loss: {val_sigpl_loss:.5f}')\n",
    "        \n",
    "\n",
    "\n",
    "        # Save the model's weights if validation loss is improved\n",
    "        improvement_delta = best_val_loss - val_loss_val\n",
    "\n",
    "        if val_loss_val < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss_val) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss_val:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss_val\n",
    "            torch.save(model.state_dict(), best_weights_path)  # Save model weights to file\n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "\n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "\n",
    "\n",
    "        # Collect model training history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss_val)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "            model.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {tr_loss_val:.5f}\\tValidation Loss: {val_loss_val:.5f}')\n",
    "\n",
    "        epochs_completed +=1\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    model.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "    model.load_state_dict(torch.load(best_weights_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "\n",
    "histno=1\n",
    "histpath = os.path.join(nbpath,'model_jsons',f'{cp_name[:-4]}_training_history{histno}_{epochs_completed}ep.json')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with open(histpath, 'w') as f:\n",
    "    json.dump(hist_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
