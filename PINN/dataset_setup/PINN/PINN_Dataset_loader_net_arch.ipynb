{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpath     = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN'\n",
    "voxel_path = '/home/mgolub4/DLproj/MLTO_2024/0_data/voxel_arrays_npy_by_partno'\n",
    "\n",
    "dbpath = os.path.join(nbpath, 'dyn_data', 'dyn_stat_database_PINN_ready.csv')\n",
    "\n",
    "\n",
    "dyndb = pd.read_csv(dbpath)\n",
    "\n",
    "stress_ser_path = os.path.join(nbpath, 'dyn_data/stress_series_data')\n",
    "\n",
    "csvs = [p for p in sorted(os.listdir(stress_ser_path)) if p.endswith('untrunc.csv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_eqn_params = ['A_opt',\n",
    "       'B_opt', 'C_opt', 'm_opt', 'n_opt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(dyndb[constit_eqn_params].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['volFrac', \n",
    "        'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "        'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "        'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "        'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "        'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "        'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "        'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PINN_Dataset(Dataset):\n",
    "    def __init__(self, params, split_dataframe,\n",
    "                 feat_vec_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/voxel_embedding_feature_maps', \n",
    "                 stress_series_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/stress_series_data', \n",
    "                 stress_ser_suffix = '_proct_gaus_btrlp_fftlp',\n",
    "                 predicted_parameters=True,\n",
    "                  ):\n",
    "        self.df = split_dataframe\n",
    "        self.featvec_dir = feat_vec_directory # for pulling the feature vectors\n",
    "        self.stress_ser_dir = stress_series_directory # for pulling the time series files\n",
    "        self.params = params\n",
    "        self.predicted_parameters = predicted_parameters\n",
    "        self.const_eqn_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]\n",
    "        self.stress_ser_suffix = stress_ser_suffix\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dyn_series_fname = self.df['dyn_file_name_original'].iloc[idx]\n",
    "\n",
    "        sig_pl = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['plateau_stress_g'].values[0]\n",
    "        W = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['energy_absorbed_g'].values[0]\n",
    "\n",
    "        # feature vector from convolutional neural network convolutional layers output\n",
    "        featvec_fname = self.df['conv_feat_vec'].iloc[idx] + '.npy'\n",
    "        featvec_path = os.path.join(self.featvec_dir, featvec_fname)\n",
    "        featvec = np.load(featvec_path)\n",
    "\n",
    "        # constitutive equation parameters\n",
    "        constit_eqn_coeffs = np.asarray(self.df[self.const_eqn_params].iloc[idx])\n",
    "\n",
    "        # predicted parameters\n",
    "        if self.predicted_parameters:\n",
    "            paramvec = np.asarray([self.df[f'pred {par}'].iloc[idx] for par in self.params])\n",
    "        else:\n",
    "            paramvec = np.asarray([self.df[f'{par}'].iloc[idx] for par in self.params])\n",
    "\n",
    "        # stress_series -- for now (April 24), Imma use the truncated datasets, because I think padded batches for RNNs in pytorch will take care of differing lengths\n",
    "        stress_ser_fname = dyn_series_fname + self.stress_ser_suffix\n",
    "        stress_ser_path = os.path.join(self.stress_ser_dir, stress_ser_fname+'.csv')\n",
    "        stress_series = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_gsreg'])\n",
    "        strain = np.asarray(pd.read_csv(stress_ser_path)['Strain'])\n",
    "                   \n",
    "        return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTr, idxRem = TTS(dyndb, stratify = dyndb['topology_family'], random_state=42, train_size = 0.8)\n",
    "idxVal, idxTe = TTS(idxRem, random_state = 42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = PINN_Dataset(params, idxTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dat))[6].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PINN_loss(dynamic_stress_PINN, data, constit_eqn_coeffs:tuple, offset=-0.01):\n",
    "\n",
    "    A_pred, B_pred, C_pred, m_pred, n_pred = constit_eqn_coeffs\n",
    "\n",
    "    strain = data[6]\n",
    "\n",
    "    stress_pred = dynamic_stress_PINN[0]\n",
    "\n",
    "    stress_series_constit_eqn = A_pred * (strain + offset)**m_pred + B_pred*((strain + offset)/(C_pred-(strain + offset)))**n_pred\n",
    "\n",
    "    sig_pl_pred = np.mean(stress_series_constit_eqn[200:400])\n",
    "    sig_pl_data = data[5]\n",
    "\n",
    "    stress_start = -1*offset+1e3 # ensures the calculation starts at the right point of the stress series\n",
    "    W_pred = np.trapz(stress_pred[stress_start:], strain[stress_start:])\n",
    "    W_data = data[4]\n",
    "\n",
    "    loss_data_1 = nn.L1Loss(sig_pl_pred, sig_pl_data) + nn.L1Loss(W_pred, W_data)\n",
    "\n",
    "    loss_physics = nn.L1Loss(stress_pred, stress_series_constit_eqn)\n",
    "\n",
    "    loss_data_2 = nn.L1Loss(constit_eqn_coeffs, data[3])\n",
    "\n",
    "    return loss_data_1 + loss_physics + loss_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        numparams = len(params)\n",
    "        linear_in_dim = 1024 + numparams\n",
    "        linear_out_dims = 5\n",
    "        super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.Sequential\n",
    "\n",
    "\n",
    "        self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "            nn.Linear(linear_in_dim, 1024),\n",
    "            nn.Linear(1024, 512),nn.ReLU(),\n",
    "            nn.Linear(512, 256),nn.ReLU(),\n",
    "            nn.Linear(256, 128),nn.ReLU(),\n",
    "            nn.Linear(128, linear_out_dims)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_vector = x[0]\n",
    "        property_vector = x[1]\n",
    "        input_vec = torch.cat([feature_vector, property_vector])\n",
    "\n",
    "        stress_ser = self.stress_series_predictor(input_vec) # MAKE SURE YOU KNOW IT KNOWS WHEN TO STOP... PROBABLY TAKEN CARE OF BY PADDED SET\n",
    "\n",
    "        constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "\n",
    "        return stress_ser, constit_eqn_coeffs\n",
    "\n",
    "\n",
    "# return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, series_input_dim=1, hidden_size=128, num_lstm_layers=4, lstm_output_dim=1):\n",
    "        numparams = len(params)\n",
    "        linear_in_dim = 1024 + numparams\n",
    "        linear_out_dims = 5\n",
    "        # self.series_in_dim = series_input_dim\n",
    "        # self.hidden_size = hidden_size\n",
    "        # self.num_lstm_layers = num_lstm_layers\n",
    "        # self.lstm_output_dim = lstm_output_dim\n",
    "        super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.LSTM(series_input_dim, hidden_size, num_lstm_layers, batch_first=True),\n",
    "        self.lstm_linear = nn.Linear(hidden_size, lstm_output_dim)\n",
    "\n",
    "\n",
    "        self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "            nn.Linear(linear_in_dim, 1024),\n",
    "            nn.Linear(1024, 512),nn.ReLU(),\n",
    "            nn.Linear(512, 256),nn.ReLU(),\n",
    "            nn.Linear(256, 128),nn.ReLU(),\n",
    "            nn.Linear(128, linear_out_dims)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_vector = x[0]\n",
    "        property_vector = x[1]\n",
    "        input_vec = torch.cat([feature_vector, property_vector])\n",
    "\n",
    "        stress_ser, _ = self.stress_series_predictor(input_vec) # MAKE SURE YOU KNOW IT KNOWS WHEN TO STOP... PROBABLY TAKEN CARE OF BY PADDED SET\n",
    "        stress_ser = self.lstm_linear(stress_ser)\n",
    "\n",
    "        constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "\n",
    "        return stress_ser, constit_eqn_coeffs\n",
    "\n",
    "\n",
    "# return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
