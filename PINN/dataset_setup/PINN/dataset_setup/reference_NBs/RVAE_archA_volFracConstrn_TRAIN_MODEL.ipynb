{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regressor-VAE - Trained for Data Generation\n",
    "### Using VolFrac as a constraint.\n",
    "\n",
    "## <u>__31 December</u>:__ After reading the beta-VAE paper again, and watching part of DeepFindr's \"GNN Project #4.2 - GVAE Training and Adjacency reconstruction\" video, I saw the chart between z dimension and beta value... so I'm going to try an encoded dimension to... TBD... and see if $\\beta$ = 1.0 does any better... \n",
    "#### Encoded (Z) dimensions and Betas tested:\n",
    "##### (128) $\\beta$ = 1.0, 0.1, 0.01...\n",
    "##### (200) $\\beta$ = 0.1, 0.01, 0.001... 0.0001 gets results...\n",
    "##### (256) $\\beta$ = 0.01\n",
    "### __Conclusion...__ I think the z dimension doesn't matter that much in this case... $\\beta$ needs to be small...\n",
    "\n",
    "## <u>__24 December</u>:__ FAIL... You didn't train on 0.1-0.5 volFrac... you forgot to change the directory of the data split... you __DUMMY__\n",
    "### ... soooo... trying again.... you fool. ATTENTION TO DETAIL (saves lives lol)\n",
    "\n",
    "## <u>__23 December</u>:__ Trying out Latin Hypercube sampling with the dataset of only 0.1-0.5 volume fraction... trained the model in this notebook...\n",
    "\n",
    "### <u>__14 December__</u>: Training a new one without KLD Annealing, want to see if the generated data is all that different\n",
    "\n",
    "### <u>__13 December__</u>: Continuing training without KLD annealing, another 100 epochs or so... I want the val_loss to be around 0.02, ... 0.05?... comparable to previous models...?\n",
    "\n",
    "### <u>__12 December__</u>: Trained using KLD Annealing, behavior exactly what I expected... Val_Loss, and specifically Reconstruction Loss, is stupid high if beta is > 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import workflow_utils\n",
    "\n",
    "from workflow_utils.FileDirectory import Directory\n",
    "\n",
    "dirs = Directory(rootpath = '/data/tigusa1/MLTO_UCAH/MLTO_2023/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_filt_bl = 32\n",
    "dec_filt_bl = 32\n",
    "enc_dim = 16\n",
    "# enc_dim = 256 # !!!!!!!!!!!!!! <><><>< <<<----- 31 December\n",
    "\n",
    "outer_layer_dim=64\n",
    "\n",
    "input_shape = (1, 64,64,64)\n",
    "input_dim = input_shape[-1]\n",
    "\n",
    "nbpath = os.path.join(dirs._10_VAE_regressor, 'rVAE_archA')\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "# 1 param - Volume Fraction\n",
    "params = ['volFrac']\n",
    "Y_file_suffix= 'VF' \n",
    "parlen = len(params)\n",
    "archver = 'A'\n",
    "archver = f'DataGen_VolFrac_rVAE_Arch{archver}_enc{enc_dim}_{input_dim}in_{outer_layer_dim}outerLyr_sigmoid_large_Z_larger_beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(archver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from workflow_utils.DataPrep import Plot_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(torch.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow_utils.DatasetCreator import DatasetCreator as DC\n",
    "\n",
    "source_data_path = dirs.source_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell sets up the DatasetCreator class. In the version of this package as delivered, the X data does not need to be reprocessed.\n",
    "# The use of testset_seed and having seed=10 ensures that the dataset is split in a replicable way.\n",
    "# \"\"\"\n",
    "\n",
    "split_dir = dirs.split_seed_68_forVAE_nonaugm\n",
    "csvname = 'data_3phys_w_dyn_topos_scaled.csv'\n",
    "volfrac_range = (0.01, 0.5)\n",
    "seed = 68\n",
    "\n",
    "\n",
    "\n",
    "dataobj = DC(nntype='cnn', data_source_directory = source_data_path, created_directory=split_dir, yfn = csvname)\n",
    "dataobj.LoadSpreadsheet()\n",
    "dataobj.FilterVolFrac(volfrac_range = volfrac_range, bound='none')\n",
    "dataobj.TrainTestSplit(testset_seed=seed, tf_te_subset=['lattice', 'topopt'], train_size=0.9, omit_tf=False, subset_tf=False, lens=[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling ProcessXYcnn with Ys_only set to True: this will produce Y arrays in train/validation/test split without re-splitting the X data.\n",
    "This flag saves time by enabling use of the same split of data while selecting different parameter(s) as the regression target(s).s\n",
    "\n",
    "If it is desired to create a new split, use the code notebook \"Split_and_Assemble_Dataset/split_dataset.ipynb\", which will enable saving\n",
    "the assembled X data as separate arrays ready for loading.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dataobj.ProcessXYcnn(params=params, Ys_only=True, skip_Ys=True, Y_file_suffix=Y_file_suffix, translate=False, translate_by=(0.25,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load in the compressed data - used in other notebooks, included here for reference.\n",
    "# Xtr =   np.load(f'{split_dir}Xtr_compressed.npy')\n",
    "# Xval =  np.load(f'{split_dir}Xval_compressed.npy')\n",
    "# Xte =   np.load(f'{split_dir}Xte_compressed.npy')\n",
    "\n",
    "Xtr =   np.load(f'{split_dir}Xtr_fullres_unpad.npy')\n",
    "Xval =  np.load(f'{split_dir}Xval_fullres_unpad.npy')\n",
    "Xte =   np.load(f'{split_dir}Xte_fullres_unpad.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytr  = np.load(f'{split_dir}Ytr{Y_file_suffix}.npy')\n",
    "Yval = np.load(f'{split_dir}Yval{Y_file_suffix}.npy')\n",
    "Yte  = np.load(f'{split_dir}Yte{Y_file_suffix}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xval.shape, Yval.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset creator class modified for Regression VAE\n",
    "\n",
    "class RVAEDataset(data.Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, Xarray, Yarray):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            size - Number of data points we want to generate\n",
    "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.Xarray=Xarray\n",
    "        self.Yarray=Yarray\n",
    "        self.size = Xarray.shape[0]\n",
    "        self.setup_dataset(self.Xarray, self.Yarray)\n",
    "\n",
    "    # def generate_continuous_xor(self):\n",
    "    def setup_dataset(self, Xarray, Yarray):\n",
    "        data = Xarray\n",
    "        paramvec = Yarray\n",
    "\n",
    "        self.data = data\n",
    "        self.paramvec = paramvec\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.data[idx]\n",
    "        data_paramvec = self.paramvec[idx]\n",
    "        return data_point, data_paramvec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling/loading for regression\n",
    "# For classification, see other notebook\n",
    "\n",
    "\n",
    "trdat = RVAEDataset(torch.from_numpy(Xtr).to(dtype=torch.float32), torch.from_numpy(Ytr).to(dtype=torch.float32))\n",
    "trloader = DataLoader(trdat, batch_size=batch_size, shuffle=True) #, shuffle=True\n",
    "\n",
    "valdat = RVAEDataset(torch.from_numpy(Xval).to(dtype=torch.float32), torch.from_numpy(Yval).to(dtype=torch.float32))\n",
    "valloader = DataLoader(valdat, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tedat = RVAEDataset(torch.from_numpy(Xte).to(dtype=torch.float32), torch.from_numpy(Yte).to(dtype=torch.float32))\n",
    "teloader = DataLoader(tedat, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = nn.Sequential(\n",
    "                nn.Conv3d(1, outer_layer_dim, kernel_size=5, padding=2),   \n",
    "                nn.ReLU(),\n",
    "                nn.Conv3d(outer_layer_dim, 32, kernel_size=5, padding=0), \n",
    "                nn.BatchNorm3d(32),\n",
    "                nn.ReLU(),\n",
    "                # ResidualBlock(32, 32, kernel_size=3, padding=1),\n",
    "                nn.MaxPool3d(2),\n",
    "                nn.Conv3d(32, 32, kernel_size=3, padding=0), \n",
    "                nn.BatchNorm3d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool3d(2),\n",
    "                nn.Conv3d(32, 64, kernel_size=4, padding=0),\n",
    "                nn.ReLU(),\n",
    "                # ResidualBlock(64, 64, kernel_size=3, padding=1),\n",
    "                nn.Conv3d(64, 128, kernel_size=4, padding=0),\n",
    "                nn.BatchNorm3d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool3d(2),\n",
    "                nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(256),\n",
    "                nn.Conv3d(256, 512, kernel_size=3, padding=0),\n",
    "                nn.BatchNorm3d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.AvgPool3d(2),\n",
    "                nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder = nn.Sequential(\n",
    "#             nn.Conv3d(1, 512, kernel_size=5, padding=0),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "#             nn.Conv3d(512, enc_filt_bl*2, kernel_size=5, padding=0),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "    \n",
    "#             nn.MaxPool3d(kernel_size=2),\n",
    "    \n",
    "#             nn.Conv3d(enc_filt_bl*2, enc_filt_bl*2, kernel_size=5, padding=0),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "            \n",
    "#             nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    \n",
    "#             nn.Linear(enc_filt_bl*2*10*10*10, enc_dim*4),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.Dropout(p=0.25),\n",
    "#             nn.Linear(enc_dim*4, enc_dim*3),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "# encoder.to(device)\n",
    "# torchsummary.summary(encoder, input_size=(1,68,68,68))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoder = nn.Sequential(\n",
    "#             nn.Linear(enc_dim, enc_dim*4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(enc_dim*4, enc_dim*8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(enc_dim*8, dec_filt_bl*2*10*10*10),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Unflatten(1, (dec_filt_bl*2, 10,10,10)),\n",
    "            \n",
    "            \n",
    "#             nn.ConvTranspose3d(dec_filt_bl*2, dec_filt_bl*2, kernel_size=5, padding=0),\n",
    "#             # nn.BatchNorm3d(num_features = dec_filt_bl*4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Upsample(scale_factor=2, mode='trilinear'),   \n",
    "\n",
    "\n",
    "#             nn.ConvTranspose3d(dec_filt_bl*2, 512, kernel_size=5, padding=0),\n",
    "#             # nn.BatchNorm3d(num_features = dec_filt_bl),\n",
    "#             nn.ReLU(),\n",
    "    \n",
    "#             nn.Upsample(scale_factor=2, mode='trilinear'), \n",
    "    \n",
    "#             nn.ConvTranspose3d(512, 1, kernel_size=5, padding=0), \n",
    "            \n",
    "#             nn.ReLU() # nn.ReLU(), # Removing, trying MAE loss... blurry could be sharpened...\n",
    "#             # nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "# decoder.to(device)\n",
    "# torchsummary.summary(decoder, input_size=(enc_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingLayer(nn.Module):\n",
    "    def forward(self, mean, log_var):\n",
    "        epsilon = torch.randn_like(log_var) # samples are of len(log_var)\n",
    "        return mean + torch.exp(0.5 * log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    # def __init__(self, input_shape, latent_dim, ft_bank_baseline, dropout_alpha):\n",
    "    def __init__(self, input_shape, enc_dim, enc_filt_bl, dec_filt_bl, outer_layer_dim):\n",
    "    \n",
    "        super(RVAE, self).__init__()\n",
    "        # self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, outer_layer_dim, kernel_size=5, padding=0), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv3d(outer_layer_dim, enc_filt_bl*2, kernel_size=5, padding=0),\n",
    "\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "    \n",
    "            nn.Conv3d(enc_filt_bl*2, enc_filt_bl*2, kernel_size=5, padding=0),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    \n",
    "            nn.Linear(enc_filt_bl*2*9*9*9, enc_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(enc_dim*4, enc_dim*3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.z_mean = nn.Linear(enc_dim*3, enc_dim)\n",
    "        self.z_log_var = nn.Linear(enc_dim*3, enc_dim)\n",
    "        \n",
    "        self.param_mean =  nn.Sequential(nn.Linear(enc_dim*3, enc_dim), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(enc_dim, len(params))\n",
    "                                    )\n",
    "                                     \n",
    "        self.param_log_var = nn.Sequential(nn.Linear(enc_dim*3, enc_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(enc_dim, len(params))\n",
    "                                      )\n",
    "                                     \n",
    "        \n",
    "        self.sampling = SamplingLayer()\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(enc_dim+len(params), enc_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(enc_dim*4, enc_dim*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(enc_dim*8, dec_filt_bl*2*9*9*9),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (dec_filt_bl*2, 9,9,9)),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose3d(dec_filt_bl*2, dec_filt_bl*2, kernel_size=5, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='trilinear'),   \n",
    "\n",
    "\n",
    "            nn.ConvTranspose3d(dec_filt_bl*2, outer_layer_dim, kernel_size=5, padding=0),\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.Upsample(scale_factor=2, mode='trilinear'), \n",
    "    \n",
    "            nn.ConvTranspose3d(outer_layer_dim, 1, kernel_size=5, padding=0), \n",
    "            \n",
    "            # nn.ReLU() # nn.ReLU(), # Removing, trying MAE loss... blurry could be sharpened...\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # self.apply(self._init_weights)  # Apply custom weight initialization to decoder layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_log_var = self.encoder(x)\n",
    "        z_mean = self.z_mean(mean_log_var)\n",
    "        z_log_var = self.z_log_var(mean_log_var)\n",
    "                                       \n",
    "        z = self.sampling(z_mean, z_log_var)\n",
    "        \n",
    "        param_mean = self.param_mean(mean_log_var)\n",
    "        param_log_var = self.param_log_var(mean_log_var)\n",
    "        r = self.sampling(param_mean, param_log_var)\n",
    "        \n",
    "        zr = torch.cat((z,r), dim=1)\n",
    "        \n",
    "        # reconstructed = self.decoder(z)\n",
    "        reconstructed = self.decoder(zr)\n",
    "        \n",
    "        \n",
    "        return reconstructed, z_mean, z_log_var, r, param_mean, param_log_var\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv3d, nn.ConvTranspose3d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ChatGPT\n",
    "# Define your training and validation functions\n",
    "def rvae_train(net, dataloader, optimizer, beta):\n",
    "    net.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    \n",
    "    for voxel, paramvec in pbar:\n",
    "        voxel = voxel.to('cuda')  # Move inputs to GPU\n",
    "        paramvec = paramvec.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #  # Use the beta_scheduler within the training loop\n",
    "        # beta = beta_scheduler(epoch, pbar.n, len(dataloader),annealing_type='cosine')\n",
    "        # beta_history.append(beta)  # Record beta values for plotting\n",
    "        \n",
    "        reconstructed, z_mean, z_log_var, r, r_mean, r_log_var = net(voxel)\n",
    "        loss = RVAE_Loss_wG(reconstructed, voxel, z_mean, z_log_var, paramvec, r, r_mean, r_log_var, beta=beta)\n",
    "        loss[0].backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss[0].item()\n",
    "        pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "    return running_loss / len(dataloader), loss[1], loss[2], loss[3]\n",
    "\n",
    "\n",
    "def rvae_validate(net, dataloader, beta):\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    with torch.no_grad():\n",
    "        for voxel, paramvec in pbar:\n",
    "            voxel = voxel.to('cuda')  # Move inputs to GPU\n",
    "            paramvec = paramvec.to('cuda')\n",
    "            reconstructed, z_mean, z_log_var, r, r_mean, r_log_var = net(voxel)\n",
    "            loss = RVAE_Loss_wG(reconstructed, voxel, z_mean, z_log_var, paramvec, r, r_mean, r_log_var, beta=beta)\n",
    "            running_loss += loss[0].item()\n",
    "            pbar.set_description(f'Val Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "    return running_loss / len(dataloader), loss[1], loss[2], loss[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle_length = 200\n",
    "# plateau_start = 50\n",
    "# plateau_interval = 20\n",
    "\n",
    "# # Define a function to schedule the beta value with different annealing types\n",
    "# def beta_scheduler(epoch, batch_idx, num_batches, annealing_type='linear', \n",
    "#                    plateau_start=plateau_start, plateau_interval=plateau_interval, cycle_length=cycle_length):\n",
    "#     if epoch < plateau_start or (epoch - plateau_start) % plateau_interval != 0:\n",
    "#         # Before plateau or during non-plateau intervals, apply the selected annealing type\n",
    "#         if annealing_type == 'linear':\n",
    "#             beta = min(1.0, epoch / plateau_start)\n",
    "#         elif annealing_type == 'logistic':\n",
    "#             k = 10.0\n",
    "#             beta = 1.0 / (1.0 + torch.exp(-k * (epoch - plateau_start) / plateau_start))\n",
    "#         elif annealing_type == 'cosine':\n",
    "#             beta = 0.5 * (1.0 + torch.cos(torch.tensor((epoch % cycle_length) * 2.0 * 3.14159 / cycle_length)))\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid annealing_type. Choose from 'linear', 'logistic', or 'cosine'.\")\n",
    "#     else:\n",
    "#         # During plateaus, set beta to the maximum value\n",
    "#         beta = 1.0\n",
    "\n",
    "#     return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # modified from https://github.com/haofuml/cyclical_annealing/blob/master/plot/plot_schedules.ipynb\n",
    "# def frange_cycle_linear(start, stop, n_epoch, low_plat_len, low_plat_value, n_cycle=4, ratio=0.5):\n",
    "#     L = np.ones(n_epoch)\n",
    "#     # for i in range\n",
    "#     period = n_epoch/n_cycle\n",
    "#     step = (stop-start)/(period*ratio) # linear schedule\n",
    "\n",
    "#     for c in range(n_cycle):\n",
    "\n",
    "#         v , i = start , 0\n",
    "        \n",
    "#         while i <= low_plat_len:\n",
    "#             L[int(i+c*period)] = low_plat_value\n",
    "#             i += 1\n",
    "#         while v <= stop and (int(i+c*period) < n_epoch):\n",
    "#             L[int(i+c*period)] = v\n",
    "#             v += step\n",
    "#             i += 1\n",
    "#     return L    \n",
    "\n",
    "# def frange_cycle_sigmoid(start, stop, n_epoch, low_plat_len, low_plat_value, n_cycle=4, ratio=0.5):\n",
    "#     L = np.ones(n_epoch)\n",
    "#     period = n_epoch/n_cycle\n",
    "#     step = (stop-start)/(period*ratio) # step is in [0,1]\n",
    "    \n",
    "#     # transform into [-6, 6] for plots: v*12.-6.\n",
    "\n",
    "#     for c in range(n_cycle):\n",
    "\n",
    "#         v , i = start , 0\n",
    "#         while i <= low_plat_len:\n",
    "#             L[int(i+c*period)] = low_plat_value\n",
    "#             i += 1\n",
    "#         while v <= stop:\n",
    "#             L[int(i+c*period)] = 1.0/(1.0+ np.exp(- (v*12.-6.)))\n",
    "#             v += step\n",
    "#             i += 1\n",
    "#     return L    \n",
    "\n",
    "\n",
    "# #  function  = 1 âˆ’ cos(a), where a scans from 0 to pi/2\n",
    "\n",
    "# def frange_cycle_cosine(start, stop, n_epoch, low_plat_len, low_plat_value, n_cycle=4, ratio=0.5):\n",
    "#     L = np.ones(n_epoch)\n",
    "#     period = n_epoch/n_cycle\n",
    "#     step = (stop-start)/(period*ratio) # step is in [0,1]\n",
    "    \n",
    "#     # transform into [0, pi] for plots: \n",
    "\n",
    "#     for c in range(n_cycle):\n",
    "\n",
    "#         v , i = start , 0\n",
    "#         while i <= low_plat_len:\n",
    "#             L[int(i+c*period)] = low_plat_value\n",
    "#             i += 1\n",
    "#         while v <= stop:\n",
    "#             L[int(i+c*period)] = 0.5-.5*math.cos(v*math.pi)\n",
    "#             v += step\n",
    "#             i += 1\n",
    "#     return L    \n",
    "\n",
    "# # def frange(start, stop, step, n_epoch):\n",
    "# #     L = np.ones(n_epoch)\n",
    "# #     v , i = start , 0\n",
    "# #     while v <= stop:\n",
    "# #         L[i] = v\n",
    "# #         v += step\n",
    "# #         i += 1\n",
    "# #     return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This modified BCE loss function is from https://arxiv.org/pdf/1608.04236.pdf\n",
    "# # Use of gamma penalizes false negatives while reducing penalty for false positives\n",
    "# usegamma=True\n",
    "# class BCEpenalized(nn.Module):\n",
    "#     def __init__(self,gamma=0.97, usegamma=usegamma):\n",
    "#         super(BCEpenalized, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "        \n",
    "#     def forward(self, output, target, *args):\n",
    "#         if usegamma:\n",
    "#             loss = -self.gamma * target * torch.log(output) - (1 - self.gamma) * (1 - target) * torch.log(1 - output)\n",
    "#         else:\n",
    "#             loss = - target * torch.log(output) - (1 - target) * torch.log(1 - output)\n",
    "            \n",
    "#         return loss.mean(), None, None\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Regression_Loss(r, r_mean, r_log_var, param_vector):\n",
    "#     num_elements = r_mean.size(0)  # Get the number of elements in the tensors\n",
    "#     regression_loss = 0.5 * torch.pow(r_mean - param_vector, 2) / torch.exp(r_log_var) + 0.5 * r_log_var\n",
    "#     regression_loss = torch.sum(regression_loss) / num_elements \n",
    "#     return regression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 0.0001\n",
    "def RVAE_Loss_wG(reconstructed, input_voxel_array, z_mean, z_log_var, param_vector, r, r_mean, r_log_var, beta=0.0001, alpha=1.0):\n",
    "    \n",
    "    # reconstruction_loss = BCEpenalized()(reconstructed, input_voxel_array)\n",
    "    reconstruction_loss = nn.BCELoss()(reconstructed, input_voxel_array)\n",
    "    \n",
    "    # reconstruction_loss = nn.MSELoss(reduction='mean')(reconstructed, input_voxel_array)\n",
    "    \n",
    "    # regression_loss = Regression_Loss(r, r_mean, r_log_var, param_vector)\n",
    "    regression_loss = nn.MSELoss()(r, param_vector)\n",
    "    \n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    \n",
    "    # loss_value =  reconstruction_loss + (1-alpha)*kl_loss + beta*regression_loss\n",
    "    loss_value =  reconstruction_loss + beta*kl_loss + alpha*regression_loss\n",
    "    \n",
    "    \n",
    "    return loss_value, reconstruction_loss, kl_loss, regression_loss\n",
    "\n",
    "lossfunc_name = 'vae_regression_loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>__Model parameters__</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE model\n",
    "rvae = torch.nn.DataParallel(RVAE(input_shape, enc_dim, enc_filt_bl, dec_filt_bl, outer_layer_dim).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(rvae.module.param_mean, input_size=(48,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(rvae.module.encoder, input_size=(1,64,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(rvae.module.decoder, input_size=(17,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>__OPTIMIZER HERE__ </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = 0.0001\n",
    "\n",
    "lratestr = f'{lrate:.0e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optim.Adam(rvae.parameters(), lr=lrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_name = f'Model_weights_{archver}.pth'\n",
    "\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with model weight saving\n",
    "EPOCHS= 500\n",
    "patience = 100\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "# earlystop_min_delta = 0.000075\n",
    "earlystop_min_delta = 0.001 # was .00075, but loss for VAE has been around 5e-2 or so, so the magnitude of change should be only one order of magnitude away. Or something. # For L1Loss (MAE)\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rvae.load_state_dict(torch.load(best_weights_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta schedule for KLD Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_schedule = frange_cycle_cosine(0.0, 1.0, 300, low_plat_len=20, low_plat_value=0.0001, n_cycle=5, ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(300), beta_schedule, '-', label='Cyclical', marker= 's', color='k', markevery=None ,lw=2,  mec='k', mew=1 , markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss_criterion = vae_loss_mod\n",
    "# bcepen = BCEpenalized()\n",
    "# loss_criterion = bcepen\n",
    "\n",
    "epochs_completed=0\n",
    "\n",
    "kl_start_epoch = 0\n",
    "\n",
    "beta = 0.01\n",
    "\n",
    "try:                \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        \n",
    "        if epoch >= kl_start_epoch:\n",
    "            # loss_criterion = vae_loss_mod\n",
    "            # Train the model\n",
    "            train_loss_output = rvae_train(rvae, trloader, optimizer, beta=beta) #beta_schedule[epoch]\n",
    "            train_loss = train_loss_output[0]\n",
    "            reconst_loss = train_loss_output[1]\n",
    "            kl_loss = train_loss_output[2]\n",
    "            reg_loss = train_loss_output[3]\n",
    "            \n",
    "\n",
    "            # Validate the model\n",
    "            val_loss_output = rvae_validate(rvae, valloader, beta=beta) # beta_schedule[epoch]\n",
    "            val_loss = val_loss_output[0]\n",
    "            vreconst_loss = val_loss_output[1]\n",
    "            vkl_loss = val_loss_output[2]\n",
    "            vreg_loss = val_loss_output[3]\n",
    "\n",
    "            # print(\"train_loss_output:\", train_loss_output)\n",
    "            # print(\"val_loss_output:\", val_loss_output)\n",
    "            print(f'train reconst loss: {reconst_loss:.5f}, KL loss: {kl_loss:.5f}, regr loss: {reg_loss:.5f}')\n",
    "            print(f'  val reconst loss: {vreconst_loss:.5f}, KL loss: {vkl_loss:.5f}, regr loss: {vreg_loss:.5f}')\n",
    "            \n",
    "        else:\n",
    "            train_loss_output = train(rvae, trloader, loss_criterion, optimizer)\n",
    "            train_loss = train_loss_output[0]\n",
    "            # reconst_loss = train_loss_output[1]\n",
    "            # kl_loss = train_loss_output[2]\n",
    "\n",
    "\n",
    "            # Validate the model\n",
    "            \n",
    "            val_loss_output = validate(rvae, valloader, loss_criterion)\n",
    "            val_loss = val_loss_output[0]\n",
    "            # vreconst_loss = val_loss_output[1]\n",
    "            # vkl_loss = val_loss_output[2]\n",
    "\n",
    "            # print('train_loss:', train_loss_output, train_loss)\n",
    "            print(f'train reconst loss: {train_loss:.5f}')\n",
    "            print(f'  val reconst loss: {val_loss:.5f}')\n",
    "            \n",
    "\n",
    "        \n",
    "        # Save the model's weights if validation loss is improved\n",
    "        improvement_delta = best_val_loss - val_loss\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(rvae.state_dict(), best_weights_path)  # Save model weights to file\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "            \n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "            \n",
    "        # Collect model training history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "            rvae.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss:.5f}\\tValidation Loss: {val_loss:.5f}')\n",
    "        \n",
    "        epochs_completed +=1\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    rvae.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'vall_loss {lossfunc_name}': val_losses}\n",
    "    rvae.load_state_dict(torch.load(best_weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'vall_loss {lossfunc_name}': val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_completed = 19\n",
    "histno = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histpath = f'{nbpath}/model_jsons/{archver}_hist{histno}_{epochs_completed}ep.json'\n",
    "print(histpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{histpath}', 'w') as f:\n",
    "        json.dump(hist_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rvae.load_state_dict(torch.load(best_weights_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How output works:\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_log_var = self.encoder(x)\n",
    "        z_mean = self.z_mean(mean_log_var)\n",
    "        z_log_var = self.z_log_var(mean_log_var)\n",
    "                                       \n",
    "        z = self.sampling(z_mean, z_log_var)\n",
    "        \n",
    "        r_mean = self.r_mean(mean_log_var)\n",
    "        r_log_var = self.r_log_var(mean_log_var)\n",
    "        r = self.sampling(r_mean, r_log_var)\n",
    "        \n",
    "        zr = torch.cat((z,r), dim=1)\n",
    "        \n",
    "        # reconstructed = self.decoder(z)\n",
    "        reconstructed = self.decoder(zr)\n",
    "        \n",
    "        \n",
    "*****   return reconstructed, z_mean, z_log_var, r, r_mean, r_log_var   ******\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte60_pred_out = rvae(torch.from_numpy(np.expand_dims(Xte[60], axis=0)).to(dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte144_pred_out = rvae(torch.from_numpy(np.expand_dims(Xte[144], axis=0)).to(dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotCell(Xte[60,0,:,:,:]) #intCrossX \t0.27\n",
    "# PlotCell(Xte[144,0,:,:,:]) #outline_faceCrossX_intRhombX \t0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = rvae.module.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = torch.randn(1,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_vec_for_volfrac = tensor([[ 1.0427,  2.4776,  1.1436, -1.1580,  0.0983, -0.0051, -2.0001, -1.4030,\n",
    "          1.6013, -1.2122, -0.8949, -1.3882, -0.3895,  1.4816, -1.3332, -0.7524,\n",
    "          0.1500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_array = np.squeeze(decoder(vec.cuda()).detach().cpu().numpy(), axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if convert_to_numpy:\n",
    "#     modvec = modvec.numpy()\n",
    "#     decoded_array = decoded_array.numpy()\n",
    "#     array_thresholded = array_thresholded.numpy()\n",
    "\n",
    "volfrac = (np.sum(array_thresholded == 1)) / (64**3)\n",
    "\n",
    "volfrac_str = f'vf{volfrac:.3f}'\n",
    "volfrac_nodec = f'0p{volfrac_str[-3:]}'\n",
    "\n",
    "print(f\"Volume Fraction: {volfrac_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxdic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volfracs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "volfracs = [0.15, 0.3, 0.45, 0.6, 0.75, 1.0]\n",
    "\n",
    "threshold=0.75\n",
    "\n",
    "for vf in volfracs:\n",
    "    vf_entry = 'VF'+str(vf*100)+'%'\n",
    "    voxdic[vf_entry] = {}\n",
    "    vec[0, 16] = vf\n",
    "    dec_array = np.squeeze(decoder(vec.cuda()).detach().cpu().numpy(), axis=(0,1))\n",
    "    \n",
    "    voxdic[vf_entry]['decoded array'] = dec_array\n",
    "    \n",
    "    thresh_array = (dec_array >= threshold).astype(int)\n",
    "    \n",
    "    voxdic[vf_entry]['thresholded array'] = thresh_array\n",
    "    \n",
    "    volfrac = (np.sum(thresh_array == 1)) / (64**3)\n",
    "    \n",
    "    volfrac_pct = str(volfrac * 100)+'%'\n",
    "    \n",
    "    volfrac_str = f'{volfrac:.2f}'\n",
    "    # volfrac_nodec = f'0p{volfrac_str[-3:]}'\n",
    "    \n",
    "    voxdic[vf_entry]['volume fraction pct'] = volfrac_pct\n",
    "    voxdic[vf_entry]['volume fraction str'] = volfrac_str\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in voxdic.keys():\n",
    "    print(key, voxdic[key]['volume fraction pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_continuous_array(array, colorscale='sunset', width=800, height=800, marker_scaler=8):\n",
    "#     z,x,y = array.nonzero()\n",
    "#     value_list = []\n",
    "#     predf = pd.DataFrame({'x':x, 'y':y, 'z':z})\n",
    "#     plotto = int(len(predf)/2)\n",
    "#     for i in range(len(x)):\n",
    "#         value_list.append(array[z[i], x[i], y[i]]) # this needs to look up the 3d array loc\n",
    "#     predf[\"activation_value\"] = value_list\n",
    "#     predf['markersize_scaler'] = (predf[\"activation_value\"] - predf[\"activation_value\"].min()) / (predf[\"activation_value\"].max() - predf[\"activation_value\"].min())\n",
    "    \n",
    "#     p = px.scatter_3d(x=predf['x'][:],  #plotto\n",
    "#                     y=predf['y'][:], \n",
    "#                     z=predf['z'][:], \n",
    "#                     color=predf['activation_value'][:], \n",
    "#                     opacity=1.0,\n",
    "#                     color_continuous_scale= \"{0}\".format(colorscale),\n",
    "#                     width=width,\n",
    "#                     height=height)\n",
    "                    \n",
    "#     p.update_traces(marker=dict(size=12,\n",
    "#                                 line=dict(width=0.1,\n",
    "#                                         color='grey')),)\n",
    "#     p.update_traces(marker_size = predf['markersize_scaler']*marker_scaler)\n",
    "#     # p.update_layout(margin=margins)\n",
    "#     p.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_continuous_array_range(array, colorscale='sunset', width=800, height=800, marker_scaler=8, x_range=None,\n",
    "                                y_range=None, z_range=None):\n",
    "    z, x, y = array.nonzero()\n",
    "    \n",
    "    # Filter data based on specified ranges\n",
    "    if x_range is not None:\n",
    "        x_mask = (x >= x_range[0]) & (x <= x_range[1])\n",
    "        x = x[x_mask]\n",
    "        y = y[x_mask]\n",
    "        z = z[x_mask]\n",
    "        \n",
    "    if y_range is not None:\n",
    "        y_mask = (y >= y_range[0]) & (y <= y_range[1])\n",
    "        x = x[y_mask]\n",
    "        y = y[y_mask]\n",
    "        z = z[y_mask]\n",
    "        \n",
    "    if z_range is not None:\n",
    "        z_mask = (z >= z_range[0]) & (z <= z_range[1])\n",
    "        x = x[z_mask]\n",
    "        y = y[z_mask]\n",
    "        z = z[z_mask]\n",
    "\n",
    "    value_list = []\n",
    "    predf = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        value_list.append(array[z[i], x[i], y[i]])\n",
    "    \n",
    "    predf[\"activation_value\"] = value_list\n",
    "    predf['markersize_scaler'] = (predf[\"activation_value\"] - predf[\"activation_value\"].min()) / (predf[\"activation_value\"].max() - predf[\"activation_value\"].min())\n",
    "    \n",
    "    p = px.scatter_3d(x=predf['x'], \n",
    "                      y=predf['y'], \n",
    "                      z=predf['z'], \n",
    "                      color=predf['activation_value'], \n",
    "                      opacity=1.0,\n",
    "                      color_continuous_scale=colorscale,\n",
    "                      width=width,\n",
    "                      height=height)\n",
    "                    \n",
    "    p.update_traces(marker=dict(size=12, line=dict(width=0.1, color='grey')))\n",
    "    p.update_traces(marker_size=predf['markersize_scaler'] * marker_scaler)\n",
    "    \n",
    "    p.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_continuous_array(array, x_range=(0, array.shape[1] // 2), y_range=None, z_range=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxdic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_continuous_array_range(voxdic['VF40.0%']['decoded array'], x_range=(0,32), y_range=(0,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in voxdic.keys():\n",
    "#     # plot_continuous_array(voxdic[key]['decoded array'])\n",
    "#     plot_continuous_array_ramge(voxdic[key]['decoded array'], x_range=(0,32))\n",
    "    \n",
    "#     # PlotCell(voxdic[key]['thresholded array'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv-py",
   "language": "python",
   "name": "gnnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
