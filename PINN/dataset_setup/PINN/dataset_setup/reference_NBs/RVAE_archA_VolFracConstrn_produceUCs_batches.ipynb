{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>__VAE__</u> - generate pseudo-random unit cells to augment our existing database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>__27 February</u>:__  Making the mat files for some selected interesting looking topos from batch 8, though most of them look exactly the same as batch 7... 6, 5, 4... you get it.\n",
    "\n",
    "### <u>__23 February</u>:__ Re-Thresholding the binary arrays in the 20FEB batch to 0.1...\n",
    "\n",
    "### ... also, running another 200 topos, since the last batch was only 2,200 arrays\n",
    "\n",
    "### <u>__20 February</u>:__ Making another 200 topos because I found that the fixed model is sooooooo much better at achieving targeted volume fractions.\n",
    "\n",
    "### <u>__11 February</u>:__ Trained a new model where the KLD loss is much smaller and train/val track together, so now I'm creating 400 topos to see if they're noticeably different/more interesting... I think, mainly I'm looking for not having most of them be lattice, though in fairness, lattice is like 126 of 157 topos...\n",
    "\n",
    "### [ <u>__25 December</u>:__ Generating the binary voxel plots of the big batch, batch4_23DEC ]\n",
    "\n",
    "### <u>__24 December</u>:__ Generating topos using the ArchA model trained on 0.1-0.5 volume fractions. I determined that ArchB doesn't produce as wide a range of volume fractions as Arch A (on average like 4% vs 25%), \n",
    "###           so I also want to see how the $\\Delta$ VolFrac is for a model trained on only 0.1-0.5 VolFrac\n",
    "\n",
    "### <u>__22 December</u>:__ DOnno what happened below on 20DEC, but now I'm using... using Latin Hypercube sampling and a threshold of 0.5\n",
    "##### Maybe that note should've been in the ArchB notebook? ... unclear\n",
    "\n",
    "### <u>__20 December</u>:__ Starting over with a new batch... using Latin Hypercube sampling and a threshold of 0.5\n",
    "\n",
    "### <u>__14 December</u>:__ Something went wonky with the first batch, as evidenced by the spherical interpolations plots not lining up with the voxel plots, so I'm starting over...\n",
    "###    Also using a lower threshold - 0.6\n",
    "\n",
    "###    (8:23pm:) Also running another batch with a different model, just to see if/how different it is... \n",
    "###    (21DEC:) I think the different model was the one without KLD annealing\n",
    "\n",
    "### <u>__13 December</u>:__ Trained R-VAE to learn VolFrac, proved it works, time to generate data\n",
    "###     Plotted voxel continuous and binary arrays, need to generate .MAT files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchno = '8'\n",
    "date = '23FEB24'\n",
    "\n",
    "# model_cp = 'vae_torch_enc16_64in_sigmoid_rebalanced_splits_11NOV.pth'\n",
    "# model_cp = 'RVAE_torch_DataGen_VolFrac_RegrVAE_enc16_64in_64outerLyr_sigmoidBCE_rvae.pth'\n",
    "model_cp = 'ModWghts_rVAE_test_ArchA_enc16_64in_64_meanKLDloss_largerbeta.pth'\n",
    "\n",
    "# model_cp = 'Model_weights_DataGen_VolFrac_rVAE_ArchA_enc16_64in_64outerLyr_sigmoid_50pctVF_split_vsmall_KLD.pth'\n",
    "\n",
    "# batchnote = 'LHS_50pctVF'\n",
    "batchnote = 'LHS_newmodel'\n",
    "sampling_type = 'LHS'\n",
    "batch_folder = f'batch{batchno}_{date}_{batchnote}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import workflow_utils\n",
    "\n",
    "from workflow_utils.FileDirectory import Directory\n",
    "\n",
    "dirs =  Directory(rootpath = '/data/tigusa1/MLTO_UCAH/MLTO_2023/')\n",
    "nbpath = dirs._10_VAE_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_filt_bl = 32\n",
    "dec_filt_bl = 32\n",
    "enc_dim = 16\n",
    "outer_layer_dim=64\n",
    "\n",
    "input_shape = (1, 64,64,64)\n",
    "input_dim = input_shape[-1]\n",
    "\n",
    "nbpath = os.path.join(dirs._10_VAE_regressor, 'rVAE_archA')\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "# 1 param - Volume Fraction\n",
    "params = ['volFrac']\n",
    "Y_file_suffix= 'VF' \n",
    "parlen = len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archnote = f'_{outer_layer_dim}outerLyr_sigmoidBCE_KLannealing_non'\n",
    "# archver = f'DataGen_VolFrac_RegrVAE_enc{enc_dim}_64in{archnote}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "produced_path = os.path.join(nbpath, 'generated_data', batch_folder)\n",
    "os.makedirs(produced_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from workflow_utils.DataPrep import Plot_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingLayer(nn.Module):\n",
    "    def forward(self, mean, log_var):\n",
    "        epsilon = torch.randn_like(log_var)\n",
    "        return mean + torch.exp(0.5 * log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    # def __init__(self, input_shape, latent_dim, ft_bank_baseline, dropout_alpha):\n",
    "    def __init__(self, input_shape, enc_dim, enc_filt_bl, dec_filt_bl, outer_layer_dim):\n",
    "    \n",
    "        super(RVAE, self).__init__()\n",
    "        # self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, outer_layer_dim, kernel_size=5, padding=0), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv3d(outer_layer_dim, enc_filt_bl*2, kernel_size=5, padding=0),\n",
    "\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "    \n",
    "            nn.Conv3d(enc_filt_bl*2, enc_filt_bl*2, kernel_size=5, padding=0),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    \n",
    "            nn.Linear(enc_filt_bl*2*9*9*9, enc_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(enc_dim*4, enc_dim*3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.z_mean = nn.Linear(enc_dim*3, enc_dim)\n",
    "        self.z_log_var = nn.Linear(enc_dim*3, enc_dim)\n",
    "        \n",
    "        self.param_mean =  nn.Sequential(nn.Linear(enc_dim*3, enc_dim), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(enc_dim, len(params))\n",
    "                                    )\n",
    "                                     \n",
    "        self.param_log_var = nn.Sequential(nn.Linear(enc_dim*3, enc_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(enc_dim, len(params))\n",
    "                                      )\n",
    "                                     \n",
    "        \n",
    "        self.sampling = SamplingLayer()\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(enc_dim+len(params), enc_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(enc_dim*4, enc_dim*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(enc_dim*8, dec_filt_bl*2*9*9*9),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (dec_filt_bl*2, 9,9,9)),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose3d(dec_filt_bl*2, dec_filt_bl*2, kernel_size=5, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='trilinear'),   \n",
    "\n",
    "\n",
    "            nn.ConvTranspose3d(dec_filt_bl*2, outer_layer_dim, kernel_size=5, padding=0),\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.Upsample(scale_factor=2, mode='trilinear'), \n",
    "    \n",
    "            nn.ConvTranspose3d(outer_layer_dim, 1, kernel_size=5, padding=0), \n",
    "            \n",
    "            # nn.ReLU() # nn.ReLU(), # Removing, trying MAE loss... blurry could be sharpened...\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # self.apply(self._init_weights)  # Apply custom weight initialization to decoder layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_log_var = self.encoder(x)\n",
    "        z_mean = self.z_mean(mean_log_var)\n",
    "        z_log_var = self.z_log_var(mean_log_var)\n",
    "                                       \n",
    "        z = self.sampling(z_mean, z_log_var)\n",
    "        \n",
    "        param_mean = self.param_mean(mean_log_var)\n",
    "        param_log_var = self.param_log_var(mean_log_var)\n",
    "        r = self.sampling(param_mean, param_log_var)\n",
    "        \n",
    "        zr = torch.cat((z,r), dim=1)\n",
    "        \n",
    "        # reconstructed = self.decoder(z)\n",
    "        reconstructed = self.decoder(zr)\n",
    "        \n",
    "        \n",
    "        return reconstructed, z_mean, z_log_var, r, param_mean, param_log_var\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv3d, nn.ConvTranspose3d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>__Model parameters__</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE model\n",
    "\n",
    "rvae = torch.nn.DataParallel(RVAE(input_shape, enc_dim, enc_filt_bl, dec_filt_bl,  outer_layer_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_path = os.path.join(cp_dir, model_cp)\n",
    "# best_weights_path = os.path.join(cp_dir, 'vae_torch_vae_enc16_64in_relu.pth')\n",
    "print(best_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "This is for taking a state_dict that was created by a non-nn.DataParallel process and turning it into one with a nn.DataParallel structure by adding\n",
    "'module.' to every key\n",
    "\"\"\"\n",
    "non_data_parallel_pth = False\n",
    "\n",
    "if non_data_parallel_pth:\n",
    "    non_data_parallel_weights = torch.load(best_weights_path)\n",
    "\n",
    "    # original saved file with DataParallel\n",
    "    data_parallel_weights = OrderedDict()\n",
    "    for k, v in non_data_parallel_weights.items():\n",
    "        name = 'module.'+k # remove `module.`\n",
    "        data_parallel_weights[name] = v\n",
    "    # # load params\n",
    "    # model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    data_parallel_weights = torch.load(best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvae.load_state_dict(data_parallel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rvae)\n",
    "\n",
    "# rvae_params = list(rvae.parameters())\n",
    "\n",
    "# for i in rvae_params:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Unit Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vae.load_state_dict(torch.load(best_weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = rvae.module.decoder # for DataParallel\n",
    "# decoder = vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Generation__\n",
    "\n",
    "### __20FEB__: generating 200 topos... see top of notebook\n",
    "\n",
    "### __11FEB__: generating 400 topos with new model where the KLD loss is better and the beta scaling factor is less small, i.e., 0.01 instead of 0.0001\n",
    "\n",
    "### __25DEC__: generating the binary voxel plots of Batch 4\n",
    "\n",
    "### __24DEC__: generating 300 topos with ... the usual... but with the model trained on 50% volfrac\n",
    "\n",
    "#### __23DEC__: generating 250 topos with 23 intervals of volFrac to see if ArchA is better at linking VolFrac to topo\n",
    "\n",
    "#### __13DEC__: Generating ~~20?~~~ 23 intervals of volume fractions\n",
    "\n",
    "#### __14DEC__: Generating 23 intervals of volume fractions... new vectors, 400 topos\n",
    "\n",
    "#### __...8:23pm__: Generating 23 intervals of volume fractions... new vectors, 400 topos... using a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random synset word without hyphens or underscores\n",
    "\n",
    "def get_random_word(pos):\n",
    "    while True:\n",
    "        synsets = list(wn.all_synsets(pos))\n",
    "        random_synset = random.choice(synsets)\n",
    "        word = random_synset.lemmas()[0].name()\n",
    "        if not re.search(r'[-_/]', word) and len(word) <= 8:\n",
    "            return word.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topos = 200\n",
    "\n",
    "# Generate unique topology names (adjective + noun) for each vector\n",
    "topo_names = []\n",
    "for _ in range(num_topos):\n",
    "    adj = get_random_word(\"a\")  # Adjective\n",
    "    noun = get_random_word(\"n\")  # Noun\n",
    "    UC_topo_name = adj + noun\n",
    "    topo_names.append(UC_topo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{produced_path}/batch4exprt_LHS_23DEC_topo_names.txt' ,'w') as f:\n",
    "#     for item in topo_names:\n",
    "#         f.write(item + \"\\n\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volfracs = np.round(np.arange(0.1,1.0,0.04), 2)\n",
    "# volfracs = np.round(np.arange(0.1,0.51,0.04), 2)\n",
    "volfracs = np.round(np.arange(0.1,0.61,0.04), 2)\n",
    "volfracs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volfracs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vf in volfracs:\n",
    "#     print(np.round(vf*100,1))\n",
    "#     print(vf)\n",
    "#     print(vf*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.qmc import LatinHypercube as LHS\n",
    "from scipy.stats import norm\n",
    "\n",
    "LHsampler = LHS(d=16)\n",
    "\n",
    "lhs_array = LHsampler.random(n=num_topos)\n",
    "\n",
    "vector_array = lhs_norm_array = norm.ppf(lhs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{produced_path}/{batch_folder}_LHSvectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{produced_path}/{batch_folder}_LHSvectors.npy', lhs_norm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_array = np.load(f'{produced_path}/{batch_folder}_LHSvectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i in range(vector_array.shape[0]):\n",
    "    vec = np.expand_dims(vector_array[i,:], 0)\n",
    "    vec = torch.from_numpy(vec).to(dtype=torch.float32)\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = [torch.randn(1, 16) for _ in range(num_topos)] #Old way\n",
    "\n",
    "\n",
    "# Create a dictionary with vectors and their corresponding part names\n",
    "vectordic = {tname: vector for tname, vector in zip(topo_names, vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_numpy = False\n",
    "\n",
    "# threshold = 0.75\n",
    "threshold = 0.1\n",
    "\n",
    "\n",
    "for topo, vector in vectordic.items():\n",
    "    gen_data_dic[topo] = {}\n",
    "    gen_data_dic[topo]['base vector'] = vector\n",
    "    \n",
    "    for num, vf in enumerate(volfracs):\n",
    "        voxdic = {}\n",
    "        vfrnd = np.round(vf*100,1)\n",
    "        vf_entry = 'VF'+f'{vfrnd:.0f}'+'%'\n",
    "        # voxdic[vf_entry] = {}\n",
    "        \n",
    "        # Modify the base vector for volume fraction\n",
    "        vf = vf.astype(np.float32)\n",
    "        vf_tensor = torch.tensor([[vf]])\n",
    "        # vf_tensor = torch.tensor([[vf]]).astype(float32)\n",
    "        vec = torch.cat((vector, vf_tensor), dim=1)\n",
    "        # voxdic[vf_entry]['modified vector'] = vec\n",
    "\n",
    "        # Decode the vector into the topology voxel array\n",
    "        dec_array = np.squeeze(decoder(vec.cuda()).detach().cpu().numpy(), axis=(0,1))\n",
    "        \n",
    "        # voxdic[vf_entry]['decoded array'] = dec_array\n",
    "        \n",
    "        # Convert the array to binary based on a threshold\n",
    "        binary_array = (dec_array >= threshold).astype(int)\n",
    "        \n",
    "        # voxdic[vf_entry]['binary array'] = binary_array\n",
    "        \n",
    "        # Compute the volume fraction of the binary array\n",
    "        volfrac = (np.sum(binary_array == 1)) / (64**3)\n",
    "        \n",
    "        # Format the volume fraction\n",
    "        volfrac_pct = str(np.round(volfrac,3) * 100)+' %'\n",
    "        volfrac_str = f'{volfrac:.5f}'\n",
    "        \n",
    "\n",
    "        \n",
    "        voxdic = {'Target volume fraction': vf,\n",
    "                  'Modified vector': vec,\n",
    "                  'Sampling method': sampling_type,\n",
    "                  'decoded array': dec_array,\n",
    "                  'binary array 0.1 threshold': binary_array,\n",
    "                  'Actual volume fraction': {'percent': volfrac_pct,\n",
    "                                     'decimal': volfrac,\n",
    "                                     'string':  volfrac_str},\n",
    "                 }\n",
    "        \n",
    "        gen_data_dic[topo][vf_entry] = voxdic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volfrac_df = pd.DataFrame(columns = ['topo', 'volfrac delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_list = []\n",
    "vf_range_list = []\n",
    "\n",
    "loaded_dict = gen_data_dic\n",
    "\n",
    "for key in list(loaded_dict.keys()):\n",
    "    # print(key)\n",
    "    topo_list.append(key)\n",
    "    vfkeys = list(loaded_dict[key].keys())[1:]\n",
    "    highvf = vfkeys[-1]\n",
    "    lowvf = vfkeys[0]\n",
    "    vf_range = loaded_dict[key][highvf]['Actual volume fraction']['decimal'] -  loaded_dict[key][lowvf]['Actual volume fraction']['decimal']\n",
    "    \n",
    "    vf_range_list.append(vf_range)\n",
    "    # for subkey in list(loaded_dict[key].keys())[1:]:\n",
    "    #     print('Target, Actual:', '\\t', loaded_dict[key][subkey]['Target volume fraction'], '\\t', loaded_dict[key][subkey]['Actual volume fraction']['percent'])\n",
    "\n",
    "volfrac_df['topo'] = topo_list\n",
    "volfrac_df['volfrac delta'] = vf_range_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volfrac_df['volfrac delta pct'] = volfrac_df['volfrac delta']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(volfrac_df['volfrac delta pct'] > 45).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in list(loaded_dict.keys())[:250]:\n",
    "#     print(key)\n",
    "#     vfkeys = list(loaded_dict[key].keys())[1:]\n",
    "#     highvf = vfkeys[-1]\n",
    "#     lowvf = vfkeys[0]\n",
    "# #     print('Target, Actual:', '\\t', loaded_dict[key][lowvf]['Target volume fraction'], '\\t', loaded_dict[key][lowvf]['Actual volume fraction']['percent'])\n",
    "# #     print('Target, Actual:', '\\t', loaded_dict[key][highvf]['Target volume fraction'], '\\t', loaded_dict[key][highvf]['Actual volume fraction']['percent'])\n",
    "    \n",
    "#     print('Low VF:', '\\t', loaded_dict[key][lowvf]['Actual volume fraction']['percent'])\n",
    "#     print('High VF:', '\\t', loaded_dict[key][highvf]['Actual volume fraction']['percent'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volfrac_df['volfrac delta pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrdict_path = os.path.join(produced_path, synthcells_batch{batchno}_dict_{date}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ChatGPT to the rescue! for Torch.Tensor is non-JSON serializable\n",
    "# # Narp, takes toooooo long\n",
    "\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def convert_to_serializable(data):\n",
    "#     if isinstance(data, torch.Tensor):\n",
    "#         return {\"torch_tensor\": data.tolist()}\n",
    "#     elif isinstance(data, np.ndarray):\n",
    "#         return {\"numpy_array\": data.tolist()}\n",
    "#     elif isinstance(data, np.float64):\n",
    "#         return {\"numpy_float\": float(data)}\n",
    "#     elif isinstance(data, dict):\n",
    "#         return {key: convert_to_serializable(value) for key, value in data.items()}\n",
    "#     else:\n",
    "#         return data\n",
    "\n",
    "# def convert_to_original_type(data):\n",
    "#     if isinstance(data, dict):\n",
    "#         if \"torch_tensor\" in data:\n",
    "#             return torch.tensor(data[\"torch_tensor\"])\n",
    "#         elif \"numpy_array\" in data:\n",
    "#             return np.array(data[\"numpy_array\"], dtype=np.float64)\n",
    "#         elif \"numpy_float\" in data:\n",
    "#             return np.float64(data[\"numpy_float\"])\n",
    "#         else:\n",
    "#             return {key: convert_to_original_type(value) for key, value in data.items()}\n",
    "#     else:\n",
    "#         return data\n",
    "\n",
    "# def save_dict_to_json(file_path, data_dict):\n",
    "#     serializable_dict = {key: convert_to_serializable(value) for key, value in data_dict.items()}\n",
    "#     with open(file_path, 'w') as json_file:\n",
    "#         json.dump(serializable_dict, json_file)\n",
    "\n",
    "# def load_dict_from_json(file_path):\n",
    "#     with open(file_path, 'r') as json_file:\n",
    "#         serializable_dict = json.load(json_file)\n",
    "#     original_dict = {key: convert_to_original_type(value) for key, value in serializable_dict.items()}\n",
    "#     return original_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the dictionary to a JSON file\n",
    "# save_dict_to_json(arrdict_path, arrdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the JSON file and convert the values back to their original formats\n",
    "# loaded_dict = load_dict_from_json(arrdict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendatadic_pkl_path = os.path.join(nbpath, f'generated_data/{batch_folder}/synthcells_batch{batchno}_dict_betterKLD_model_{date}.pkl')\n",
    "\n",
    "gendatadic_pkl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works in a reasonable amount of time, thanks again ChatGPT\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_dict_to_pickle(file_path, data_dict):\n",
    "    with open(file_path, 'wb') as pickle_file:\n",
    "        pickle.dump(data_dict, pickle_file)\n",
    "\n",
    "def load_dict_from_pickle(file_path):\n",
    "    with open(file_path, 'rb') as pickle_file:\n",
    "        loaded_dict = pickle.load(pickle_file)\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single dict saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a pickle file\n",
    "save_dict_to_pickle(gendatadic_pkl_path, gen_data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the pickle file\n",
    "loaded_dict = load_dict_from_pickle(gendatadic_pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-thresholding binary arrays to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_array_range(loaded_dict['CariousLamarck']['VF10%']['binary array'], export_png=False, show=True, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_array_range(loaded_dict['CariousLamarck']['VF10%']['binary array 0.1 threshold'], export_png=False, show=True, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for topo in list(loaded_dict.keys())[:]:\n",
    "    \n",
    "    topodict = loaded_dict[topo]\n",
    "    \n",
    "    for volfrac in list(topodict.keys())[1:]:\n",
    "        \n",
    "        vfdict = topodict[volfrac]\n",
    "        \n",
    "        cont_array = vfdict['decoded array']\n",
    "        \n",
    "        bin_array_rethresh = (cont_array >= threshold).astype(int)\n",
    "        \n",
    "        rethr_vfrac = (np.sum(bin_array_rethresh == 1)) / (64**3)\n",
    "        rethr_volfrac_pct = str(np.round(rethr_vfrac,3) * 100)+' %'\n",
    "        rethr_volfrac_str = f'{rethr_vfrac:.5f}'\n",
    "        \n",
    "        rethr_volfracs = {'percent': rethr_volfrac_pct,\n",
    "                                     'decimal': rethr_vfrac,\n",
    "                                     'string':  rethr_volfrac_str}\n",
    "        \n",
    "       \n",
    "        loaded_dict[topo][volfrac]['Rethresholded volume fraction'] = rethr_volfracs\n",
    "        \n",
    "        loaded_dict[topo][volfrac]['binary array 0.1 threshold'] = bin_array_rethresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gendatadic_pkl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/overwrite\n",
    "\n",
    "# save_dict_to_pickle(gendatadic_pkl_path, loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loaded_dict['LifelessMaoism'].keys(),\n",
    "\n",
    "# loaded_dict['LifelessMaoism']['VF10%'].keys(),\n",
    "\n",
    "# loaded_dict['LifelessMaoism']['VF10%']['Actual volume fraction']['percent'],\n",
    "\n",
    "# loaded_dict['LifelessMaoism']['VF10%']['Rethresholded volume fraction']['percent'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of splitting the dict into two halves to save it... needed to do this with 800 topos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Saving the generated data as separate pkl files because with 800 topos the kernel died...\n",
    "# \"\"\"\n",
    "\n",
    "# dic_len = len(topo_names)\n",
    "# dic_half = dic_len//2\n",
    "# keys = list(gen_data_dic.keys())\n",
    "\n",
    "\n",
    "# gendatadic_keys_firsthalf = {key: gen_data_dic[key] for key in keys[:dic_half]}\n",
    "# gendatadic_keys_secondhalf = {key: gen_data_dic[key] for key in keys[dic_half:]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gendatadic_firsthalf_pkl_path = os.path.join(nbpath, f'generated_data/{batch_folder}/synthcells_batch{batchno}_800{sampling_type}_dict_{date}_firsthalf.pkl')\n",
    "# gendatadic_secondhalf_pkl_path = os.path.join(nbpath, f'generated_data/{batch_folder}/synthcells_batch{batchno}_800{sampling_type}_dict_{date}_secondhalf.pkl')\n",
    "# print(gendatadic_firsthalf_pkl_path)\n",
    "# print(gendatadic_secondhalf_pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a pickle file\n",
    "# save_dict_to_pickle(gendatadic_firsthalf_pkl_path, gendatadic_keys_firsthalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dict_to_pickle(gendatadic_secondhalf_pkl_path, gendatadic_keys_secondhalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_first_half = load_dict_from_pickle(gendatadic_firsthalf_pkl_path)\n",
    "\n",
    "len(list(load_first_half.keys()))\n",
    "\n",
    "load_second_half = load_dict_from_pickle(gendatadic_secondhalf_pkl_path)\n",
    "\n",
    "loaded_dict = {**load_first_half, **load_second_half}\n",
    "\n",
    "print(len(list(load_first_half.keys())), len(list(load_second_half.keys())), len(list(loaded_dict.keys())))\n",
    "\n",
    "del load_first_half, load_second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_path = os.path.join(produced_path, 'MAT_files')\n",
    "os.makedirs(mat_path, exist_ok=True)\n",
    "cont_plot_path = os.path.join(produced_path, 'continuous_voxel_plots')\n",
    "os.makedirs(cont_plot_path, exist_ok=True)\n",
    "bin_plot_path = os.path.join(produced_path, 'binary_voxel_plots')\n",
    "os.makedirs(bin_plot_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_data_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topo = 'DarkAbel'\n",
    "# for key in list(gen_data_dic[topo].keys())[1:]:\n",
    "#     print(f\"target VF: {key[2:]}, actual VF: {gen_data_dic[topo][key]['Actual volume fraction']['percent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_array_range(loaded_dict['ActinoidMedicaid']['VF38%']['binary array'], show=True, export_png=False, binary=True, x_range=(0,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict['LifelessMaoism']['VF10%'].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(loaded_dict.keys()))*23//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(loaded_dict.keys()))*23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half = len(list(loaded_dict.keys()))*23//2\n",
    "\n",
    "unit_count=0\n",
    "\n",
    "\n",
    "# Save as .mat files\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topodict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 FEB - a few selected interesting topos from batch 8\n",
    "selected_topos = ['FeverishJeth', 'OverbusyCellini', 'InnocentAugend', 'DatelessInfamy','EruditeSteel','FootsoreNetting','RingedMyrtle', 'RustlessMarimba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dic = loaded_dict # !!!!! For loaded_dict that was rethresholded\n",
    "\n",
    "# To export to MATLAB files\n",
    "# for key in list(gen_data_dic.keys())[:]:\n",
    "for key in selected_topos:\n",
    "\n",
    "    topodict = gen_data_dic[key]\n",
    "\n",
    "    for subkey in list(topodict.keys())[1:]:\n",
    "        vfincr_dict = topodict[subkey]\n",
    "        \n",
    "        # increm = subdict['sub_increment']\n",
    "        volfrac = topodict[subkey]['Actual volume fraction']['decimal']\n",
    "        # volfrac = topodict[subkey]['Rethresholded volume fraction']['decimal'] # !!!!!\n",
    "        \n",
    "        volfrac_formatted = f'{volfrac:.2e}'\n",
    "        \n",
    "        # fullVF = subdict['volFrac']\n",
    "        # volfrac = f'{fullVF:.3f}'\n",
    "        name = f'synth{sampling_type}_{key}_{volfrac_formatted}_{subkey[2:4]}VFtgt'\n",
    "        \n",
    "        matfile_dict = {'name': name,\n",
    "                        'volFrac': volfrac,\n",
    "                        'binary_voxel_array': vfincr_dict['binary array 0.1 threshold']}\n",
    "\n",
    "        # if unit_count <= half:\n",
    "            # filepath = os.path.join(mat_path, '0_half', name + '.mat')\n",
    "        # elif unit_count > half:\n",
    "            # filepath = os.path.join(mat_path, 'half_end', name + '.mat')\n",
    "            \n",
    "        filepath = os.path.join(mat_path, name + '.mat')\n",
    "            \n",
    "            \n",
    "        # print(filepath)\n",
    "        savemat(filepath, matfile_dict)\n",
    "        \n",
    "        unit_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(gen_data_dic.keys()))*23//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(loaded_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_data_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in gen_data_dic.keys():\n",
    "#     print(len(gen_data_dic[key].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadmat('/data/tigusa1/MLTO_UCAH/MLTO_2023/0_data/source_MAT_files/topopt/final_struc_4A_75iso_4.2e-01.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadmat(os.path.join(mat_path, 'synth_GeorgianSermon_5.84e-01.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_array_range(array, colorscale='sunset', width=800, height=800, marker_scaler=8, x_range=None, y_range=None, z_range=None, \n",
    "                                plotpath = None, show=False, export_png=True, export_html=False, binary=False):\n",
    "    z, x, y = array.nonzero()\n",
    "    \n",
    "    # Filter data based on specified ranges\n",
    "    if x_range is not None:\n",
    "        x_mask = (x >= x_range[0]) & (x <= x_range[1])\n",
    "        x = x[x_mask]\n",
    "        y = y[x_mask]\n",
    "        z = z[x_mask]\n",
    "        \n",
    "    if y_range is not None:\n",
    "        y_mask = (y >= y_range[0]) & (y <= y_range[1])\n",
    "        x = x[y_mask]\n",
    "        y = y[y_mask]\n",
    "        z = z[y_mask]\n",
    "        \n",
    "    if z_range is not None:\n",
    "        z_mask = (z >= z_range[0]) & (z <= z_range[1])\n",
    "        x = x[z_mask]\n",
    "        y = y[z_mask]\n",
    "        z = z[z_mask]\n",
    "\n",
    "    \n",
    "    arr_plot_df = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "    \n",
    "\n",
    "    if binary:\n",
    "        p = px.scatter_3d(x=arr_plot_df['x'], y=arr_plot_df['y'], z=arr_plot_df['z'], width=width, height=height)\n",
    "        p.update_traces(marker=dict(size=12, line=dict(width=0.1, color='black')))\n",
    "        p.update_traces(marker_size=3)                                        \n",
    "        \n",
    "    else:\n",
    "        value_list = []\n",
    "        for i in range(len(x)):\n",
    "            value_list.append(array[z[i], x[i], y[i]])\n",
    "\n",
    "        arr_plot_df[\"activation_value\"] = value_list\n",
    "        arr_plot_df['markersize_scaler'] = (arr_plot_df[\"activation_value\"] - arr_plot_df[\"activation_value\"].min()) / (arr_plot_df[\"activation_value\"].max() - arr_plot_df[\"activation_value\"].min())\n",
    "\n",
    "        p = px.scatter_3d(x=arr_plot_df['x'], \n",
    "                          y=arr_plot_df['y'], \n",
    "                          z=arr_plot_df['z'], \n",
    "                          color=arr_plot_df['activation_value'], \n",
    "                          opacity=1.0,\n",
    "                          color_continuous_scale=colorscale,\n",
    "                          width=width,\n",
    "                          height=height)\n",
    "\n",
    "        p.update_traces(marker=dict(size=12, line=dict(width=0.1, color='grey')))\n",
    "        p.update_traces(marker_size=arr_plot_df['markersize_scaler'] * marker_scaler)\n",
    "    \n",
    "        \n",
    "    \n",
    "    if show:\n",
    "        p.show()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if export_png:\n",
    "        p.write_image(plotpath+'.png')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if export_html:\n",
    "        p.write_image(plotpath+'.html')\n",
    "    else:\n",
    "        pass\n",
    "    del p, arr_plot_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_data_dic['DarkAbel']['VF38%']['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loaded dictionary\n",
    "# gen_data_dic = loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_keys = []\n",
    "for file in os.listdir(cont_plot_path):\n",
    "    topo_name = file.split('_')[0]\n",
    "    topo_keys.append(topo_name)\n",
    "topo_key_set = set(topo_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __20 FEB__: Stopped @ CreakyHinault, number 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(gen_data_dic.keys()):\n",
    "    print(i, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(gen_data_dic.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dic = loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfkey = 'VF38%'\n",
    "\n",
    "plot_range = (32, 64)\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# For plotting continuous voxel arrays\n",
    "\n",
    "for key in list(gen_data_dic.keys())[:]:\n",
    "    if key in topo_key_set:\n",
    "        # count +=1\n",
    "        pass\n",
    "    else:\n",
    "        # count +=1\n",
    "        # pass\n",
    "        vecdict = gen_data_dic[key]\n",
    "\n",
    "        topo_name = key\n",
    "\n",
    "        array = gen_data_dic[key][vfkey]['decoded array']\n",
    "\n",
    "        plot_path = cont_plot_path\n",
    "\n",
    "\n",
    "        for incr in plot_range:\n",
    "            if incr==32:\n",
    "                plotincr = 'half'\n",
    "            else:\n",
    "                plotincr = 'full'\n",
    "            filename = f'{key}_{vfkey[:-1]}_ref_{plotincr}'\n",
    "\n",
    "            plot_filepath = os.path.join(plot_path, filename)\n",
    "            plot_array_range(array, x_range=(0,incr), plotpath = plot_filepath, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_plot_path = os.path.join(produced_path, 'binary_voxel_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting binary voxel arrays\n",
    "vfkey = 'VF38%'\n",
    "\n",
    "# If the dict was loaded from a saved pkl file\n",
    "gen_data_dic = loaded_dict\n",
    "\n",
    "for key in list(gen_data_dic.keys())[:]:\n",
    "    vecdict = gen_data_dic[key]\n",
    "    \n",
    "    topo_name = key\n",
    "    \n",
    "    array = gen_data_dic[key][vfkey]['binary array']\n",
    "    actual_vf = gen_data_dic[key][vfkey]['Actual volume fraction']['percent'][:4]\n",
    "    \n",
    "    plot_path = bin_plot_path\n",
    "    \n",
    "    plot_range = (32, 64)\n",
    "    \n",
    "    for incr in plot_range:\n",
    "        if incr==32:\n",
    "            plotincr = 'half'\n",
    "        else:\n",
    "            plotincr = 'full'\n",
    "        filename = f'{key}_{vfkey[:-1]}_ref_{plotincr}_BinVox_VF{actual_vf}'\n",
    "        \n",
    "        plot_filepath = os.path.join(plot_path, filename)\n",
    "        plot_array_range(array, x_range=(0,incr), plotpath = plot_filepath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfrange_plot_path = os.path.join(produced_path, 'vfrange_plots')\n",
    "os.makedirs(vfrange_plot_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_data_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volfrac_uc_names = ['FinicalCall', 'ImplicitBialy','CelticMaltese','CleanSnout','FitThought', 'MiscibleHeadshot',\n",
    "                    'PowerfulFeijoa','StormyShock','TerminalMarocain','TolerantKite','UpliftedAeolian',\n",
    "                    'ZigzagLogomach','AuralAlexic',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dic = loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for producing plots of all the volume fractions for a given topo\n",
    "\n",
    "for topo in volfrac_uc_names:\n",
    "    topodic = source_dic[topo]\n",
    "    \n",
    "    for key in list(topodic.keys())[1:]:\n",
    "        vf_tgt = str(topodic[key]['Target volume fraction'])[:5]\n",
    "        vf_actual = topodic[key]['Actual volume fraction']['string'][:5]\n",
    "        \n",
    "        for arrtype in ['decoded array',]:#'binary array']:\n",
    "            if arrtype.split(' ')[0] == 'binary':\n",
    "                binary=True\n",
    "                suffix='bin'\n",
    "            else:\n",
    "                binary=False\n",
    "                suffix='cont'\n",
    "                \n",
    "            filename = f'{topo}_tgt{vf_tgt}_actual{vf_actual}_{suffix}'\n",
    "            \n",
    "            array = topodic[key][arrtype]\n",
    "            \n",
    "            plot_range = (32, 64)\n",
    "            \n",
    "            for incr in plot_range:\n",
    "                if incr==32:\n",
    "                    plotincr = 'half'                \n",
    "                else:\n",
    "                    plotincr = 'full'\n",
    "                incr_filename = filename + '_' + plotincr\n",
    "                plotpath = os.path.join(vfrange_plot_path, incr_filename)\n",
    "                \n",
    "                plot_array_range(array, x_range=(0,incr), plotpath = plotpath, binary=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pngpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLTO_2023/10_VAE_regressor/vae_produced_UCs/MAT_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(mat_path, 'BandagedPatrial_mult_neg_0.1_vf0.831.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadmat(os.path.join(mat_path, 'BandagedPatrial_mult_neg_0.1_vf0.831.mat'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv-py",
   "language": "python",
   "name": "gnnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
