{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpath     = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN'\n",
    "voxel_path = '/home/mgolub4/DLproj/MLTO_2024/0_data/voxel_arrays_npy_by_partno'\n",
    "\n",
    "# dbpath = os.path.join(nbpath, 'dyn_data', 'dynamic_static_database_scaled_DEC23_constit_eqn_fit_params.csv')\n",
    "dbpath = os.path.join(nbpath, 'dyn_data', 'dyn_stat_database_PINN_ready.csv') # dynamic_static_database_scaled_APR24_constit_eqn_fit_params\n",
    "\n",
    "\n",
    "dyndb = pd.read_csv(dbpath)\n",
    "\n",
    "stress_ser_path = os.path.join(nbpath, 'dyn_data/stress_series_data')\n",
    "\n",
    "# csvs = [p for p in sorted(os.listdir(stress_ser_path)) if p.endswith('untrunc.csv')]\n",
    "csvs = [p for p in sorted(os.listdir(stress_ser_path)) if p.endswith('_proct_gaus_btrlp_fftlp.csv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensuring that I only pull the csvnames of the actual simulation data that I use in the dynamic database (100 of 102)\n",
    "\n",
    "truncsvs = []\n",
    "for csv in csvs:\n",
    "    truncsvs.append(csv.rsplit('_',4)[0])\n",
    "\n",
    "dyndbcsvs = [name for name in truncsvs if name in dyndb['dyn_file_name_original'].values]\n",
    "\n",
    "len(dyndbcsvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcols = ['plateau_stress_g_from_scaled', 'energy_absorbed_g_from_scaled', 'sig_pl_offsetmask', 'sig_pl_offsetmask_scaled', 'W_pred_offsetmask', 'W_pred_offsetmask_scaled']\n",
    "\n",
    "for col in newcols:\n",
    "    dyndb[col] = pd.Series(np.zeros(len(dyndb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffcols =[ 'A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]\n",
    "offset = -0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb[dyndb['dyn_file_name_original'] == 'VF_35_64_VF35p92'][coeffcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb[dyndb['dyn_file_name_original'] == name][coeffcols]['A_opt'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting off negative values for strain once offset is applied\n",
    "\n",
    "# mask = strain >= offset*-1\n",
    "\n",
    "# strainoff = strain + offset\n",
    "\n",
    "# strainoff = np.where(mask, strainoff, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_data_params_scaled = ['plateau_stress_g_from_scaled', 'energy_absorbed_g_from_scaled',]\n",
    "\n",
    "for name in dyndbcsvs:\n",
    "    series_csvname = name + '_proct_gaus_btrlp_fftlp.csv'\n",
    "    new_filename = name + '_proct_w_constit_eqn_and_scaled_series.csv'\n",
    "    colname = name\n",
    "\n",
    "    dyndb_index = dyndb[dyndb['dyn_file_name_original'] == name].index[0]\n",
    "    \n",
    "    series_csvpath = os.path.join(stress_ser_path, series_csvname)\n",
    "    newseries_csvpath = os.path.join(stress_ser_path, new_filename)\n",
    "\n",
    "    # print(series_csvpath)\n",
    "\n",
    "    seriesdf = pd.read_csv(series_csvpath)\n",
    "\n",
    "    # 1 (ha, ha, ha...) Min-Max scale the simulation data, add it back to the csv\n",
    "    stress_data = seriesdf['stress_bottom_gsreg']\n",
    "    strain = seriesdf['Strain']\n",
    "\n",
    "    mask = strain >= offset*-1\n",
    "    strainoff = strain + offset\n",
    "    strainoff = np.where(mask, strainoff, 0)\n",
    "\n",
    "    data_min = stress_data.min()\n",
    "    data_max = stress_data.max()\n",
    "\n",
    "    stress_data_scaled = (stress_data - data_min) / (data_max - data_min)\n",
    "\n",
    "    seriesdf['stress_bottom_gsreg_scaled'] = stress_data_scaled\n",
    "    \n",
    "    # # 1a save as the original, the equivalent of just adding a column --> LATER\n",
    "    # seriesdf.to_csv(series_csvpath)\n",
    "\n",
    "    #  2 (ha, ha, ha...) calculate sig_pl and W of scaled data\n",
    "    sig_pl_data_scaled = np.mean(stress_data_scaled[200:400])\n",
    "    W_data_scaled= np.trapz(stress_data_scaled, strain)\n",
    "\n",
    "    # 2a enter into dyndb\n",
    "\n",
    "    for col, val in zip(derived_data_params_scaled, [sig_pl_data_scaled, W_data_scaled]):\n",
    "        # dyndb[dyndb['dyn_file_name_original'] == name][col] = val\n",
    "        dyndb.loc[dyndb_index, col] = val\n",
    "\n",
    "\n",
    "    # 3 - constitutive equation calculations and derived parameters\n",
    "    # 3a calc constitutive equation stress series and derived parameters\n",
    "\n",
    "    coeffs = dyndb[dyndb['dyn_file_name_original'] == name][coeffcols]\n",
    "    \n",
    "    A_opt = coeffs['A_opt'].values[0]\n",
    "    B_opt = coeffs['B_opt'].values[0]\n",
    "    C_opt = coeffs['C_opt'].values[0]\n",
    "    m_opt = coeffs['m_opt'].values[0]\n",
    "    n_opt = coeffs['n_opt'].values[0]\n",
    "\n",
    "    constit_eqn_stress = A_opt * strainoff**m_opt + B_opt * (strainoff / (C_opt - strainoff))**n_opt\n",
    "    \n",
    "    # ... add to time series dataframe\n",
    "    seriesdf['stress_bottom_constitutive_equation'] = constit_eqn_stress\n",
    "\n",
    "\n",
    "    # 3b scale series\n",
    "    # stress_max = constit_eqn_stress.max()\n",
    "    # stress_min = constit_eqn_stress.min()\n",
    "\n",
    "    # constit_eqn_stress_scaled = (constit_eqn_stress - stress_min) / (stress_max - stress_min)\n",
    "    constit_eqn_stress_scaled = (constit_eqn_stress - data_min) / (data_max - data_min) # \"data_min\" and \"data_max\" are from the source data --> since the constitutive equation is to have the same scale as the source data, they should be scaled from the source data's max and min\n",
    "    # ... add to dataframe\n",
    "    seriesdf['stress_bottom_constitutive_equation_scaled'] = constit_eqn_stress_scaled\n",
    "\n",
    "    # ... and save\n",
    "    seriesdf.to_csv(newseries_csvpath)\n",
    "\n",
    "    # 3c calculated sig_pl and W from constitutive equation --> un/scaled\n",
    "    #  'sig_pl_offsetmask', 'sig_pl_offsetmask_scaled', 'W_pred_offsetmask', 'W_pred_offsetmask_scaled']\n",
    "    sig_pl_eqn = np.mean(constit_eqn_stress[200:400])\n",
    "    sig_pl_eqn_scaled = np.mean(constit_eqn_stress_scaled[200:400])\n",
    "    W_eqn = np.trapz(constit_eqn_stress, strainoff)\n",
    "    W_eqn_scaled = np.trapz(constit_eqn_stress_scaled, strainoff)\n",
    "\n",
    "    # ... add to dyndb\n",
    "    for col, val in zip(['sig_pl_offsetmask', 'sig_pl_offsetmask_scaled', 'W_pred_offsetmask', 'W_pred_offsetmask_scaled'],\n",
    "                        [sig_pl_eqn, sig_pl_eqn_scaled, W_eqn, W_eqn_scaled]):\n",
    "        # dyndb[dyndb['dyn_file_name_original'] == name][col] = val\n",
    "        dyndb.loc[dyndb_index, col] = val\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyndb.to_csv(dbpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling by absolute min and max (min and max across all stress series), as I did below,  doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stressdf.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax = stressdf.max().max()\n",
    "print(absmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absmin = stressdf.min().min()\n",
    "print(absmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in stressdf.columns:\n",
    "    \n",
    "    col_scaled = (stressdf[col] - absmin) / (absmax - absmin)\n",
    "\n",
    "    stressdf[f'{col}_abscaled'] = col_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stressdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
