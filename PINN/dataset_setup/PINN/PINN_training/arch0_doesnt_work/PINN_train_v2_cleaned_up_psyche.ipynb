{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import workflow_utils_v3\n",
    "import sys\n",
    "\n",
    "from workflow_utils_v3.FileDirectory import Directory\n",
    "\n",
    "dirs = Directory(rootpath = '/home/mgolub4/DLproj/MLTO_2024/')\n",
    "\n",
    "# Sets directory of entire package\n",
    "# rootpath = '/data/tigusa1/MLTO_UCAH/MLTO_2023/'\n",
    "\n",
    "nbpath = os.path.join(dirs._3_Dynamic_PINN_RNN, 'PINN_training')\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle\n",
    "from plotly.colors import sequential, qualitative\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "# import torchsummary\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split as TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '29APR24'\n",
    "fname_base = f'Dyn_PINN_v0_{date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready.csv'\n",
    "dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready_coeffs_scaled.csv'\n",
    "dyndb = pd.read_csv(dyndb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_min_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt', 'plateau_stress_g', 'energy_absorbed_g']\n",
    "# dict_entries = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt', 'sig_pl', 'W']\n",
    "\n",
    "\n",
    "# Modifying from above after calculating sig_pl and W from scaled time series\n",
    "max_min_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt'] # 'plateau_stress_g', 'energy_absorbed_g' <<-- in the revised version, these were calculated from the scaled stress/strain series, so pulling their column-wise max-min won't work\n",
    "dict_entries = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt'] # 'sig_pl', 'W'\n",
    " \n",
    "max_min_dict = {}\n",
    "\n",
    "for par, name in zip(max_min_params, dict_entries):\n",
    "    data = dyndb[par]\n",
    "    max = data.max()\n",
    "    min = data.min()\n",
    "    max_min_dict[name] = (max, min) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTr, idxRem = TTS(dyndb, stratify = dyndb['topology_family'], random_state=42, train_size = 0.8)\n",
    "idxVal, idxTe = TTS(idxRem, random_state = 42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['volFrac', \n",
    "        'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "        'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "        'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "        'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "        'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "        'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "        'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stress_Series:\n",
    "    def __init__(self, series):\n",
    "        self.series = series\n",
    "        self.max = series.max()\n",
    "        self.min = series.min()\n",
    "\n",
    "    def scale(self):\n",
    "        return (self.series - self.min) / (self.max - self.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def min_max_scale_series(batched_time_series):\n",
    "    # Calculate the maximum and minimum values for each series\n",
    "    max_values = torch.max(batched_time_series, dim=1)[0]\n",
    "    min_values = torch.min(batched_time_series, dim=1)[0]\n",
    "\n",
    "    # Calculate the range for each series\n",
    "    series_range = max_values - min_values\n",
    "\n",
    "    # Ensure non-zero range to avoid division by zero\n",
    "    # series_range = torch.where(series_range == 0, torch.tensor(1e-7), series_range)\n",
    "\n",
    "    # Min-max scale each series\n",
    "    scaled_time_series = (batched_time_series - min_values.unsqueeze(1)) / series_range.unsqueeze(1)\n",
    "\n",
    "    return scaled_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Dataset(Dataset):\n",
    "    def __init__(self, params, split_dataframe,\n",
    "                 feat_vec_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/voxel_embedding_feature_maps', \n",
    "                 stress_series_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/stress_series_data', \n",
    "                #  stress_ser_suffix = '_proct_gaus_btrlp_fftlp',\n",
    "                stress_ser_suffix = '_proct_w_constit_eqn_and_scaled_series',\n",
    "                #  scale_coeffs_by=[1e9, 1e11, 1, 1, 1],\n",
    "                 predicted_parameters=True,\n",
    "                  ):\n",
    "        self.df = split_dataframe\n",
    "        self.featvec_dir = feat_vec_directory # for pulling the feature vectors\n",
    "        self.stress_ser_dir = stress_series_directory # for pulling the time series files\n",
    "        self.params = params\n",
    "        self.predicted_parameters = predicted_parameters\n",
    "        # self.const_eqn_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]\n",
    "        self.const_eqn_params = ['A_opt scaled', 'B_opt scaled', 'C_opt scaled', 'm_opt scaled', 'n_opt scaled',]\n",
    "\n",
    "        self.stress_ser_suffix = stress_ser_suffix\n",
    "        # self.scale_coeffs_by = scale_coeffs_by\n",
    "\n",
    "        # self.scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dyn_series_fname = self.df['dyn_file_name_original'].iloc[idx]\n",
    "\n",
    "        sig_pl = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['plateau_stress_g scaled'].values[0].astype(np.float32)\n",
    "        W = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['energy_absorbed_g scaled'].values[0].astype(np.float32)\n",
    "\n",
    "        # feature vector from convolutional neural network convolutional layers output\n",
    "        featvec_fname = self.df['conv_feat_vec'].iloc[idx] + '.npy'\n",
    "        featvec_path = os.path.join(self.featvec_dir, featvec_fname)\n",
    "        featvec = np.load(featvec_path)\n",
    "        featvec = np.squeeze(featvec, axis=0).astype(np.float32)\n",
    "\n",
    "        # constitutive equation parameters\n",
    "        constit_eqn_coeffs = np.asarray(self.df[self.const_eqn_params].iloc[idx])\n",
    "\n",
    "        # for i, scaler in enumerate(self.scale_coeffs_by):\n",
    "        #     constit_eqn_coeffs[i] = constit_eqn_coeffs[i] / scaler\n",
    "            \n",
    "\n",
    "\n",
    "        # predicted parameters\n",
    "        if self.predicted_parameters:\n",
    "            paramvec = np.asarray([self.df[f'pred {par}'].iloc[idx] for par in self.params]).astype(np.float32)\n",
    "        else:\n",
    "            paramvec = np.asarray([self.df[f'{par}'].iloc[idx] for par in self.params]).astype(np.float32)\n",
    "\n",
    "        # stress_series -- for now (April 24), Imma use the truncated datasets, because I think padded batches for RNNs in pytorch will take care of differing lengths\n",
    "        stress_ser_fname = dyn_series_fname + self.stress_ser_suffix\n",
    "        stress_ser_path = os.path.join(self.stress_ser_dir, stress_ser_fname+'.csv')\n",
    "        # stress_series = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_gsreg']).astype(np.float32)\n",
    "        stress_series = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_gsreg_scaled']).astype(np.float32)\n",
    "        \n",
    "        stress_series = Stress_Series(stress_series)\n",
    "        # scaled_stress_series = stress_series.scale()\n",
    "        stress_series_dic = {'stress_series': stress_series.series, 'max': stress_series.max, 'min': stress_series.min}\n",
    "\n",
    "        # stress_series_dic = {'stress_series': scaled_stress_series, 'max': stress_series.max, 'min': stress_series.min}\n",
    "\n",
    "        strain = np.asarray(pd.read_csv(stress_ser_path)['Strain']).astype(np.float32)\n",
    "                   \n",
    "\n",
    "        return featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain, stress_series_dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain\n",
    "# return nonseries_list, padded_strain_ser, padded_stress_ser\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def padded_collate(batch):\n",
    "    # get list of stress series -- uneven lengths, but will be padded at end\n",
    "    stress_ser = [torch.tensor(item[-1]['stress_series'], requires_grad=True) for item in batch]\n",
    "    # get max and min values as a list of lists, where the first value is the maximum and second is the minimum value of the accompanying stress series\n",
    "    max_mins = [[item[-1]['max'],item[-1]['min']] for item in batch]\n",
    "    max_mins = np.asarray(max_mins)\n",
    "\n",
    "    # get strain series, convert to tensor\n",
    "    strain_ser = [torch.tensor(item[-2], requires_grad=True) for item in batch]\n",
    "    \n",
    "    # get other data (featvec, paramvec, constit_eqn_coeffs, W, sig_pl) from data\n",
    "    nonserbatch = [item[0:-2] for item in batch] # all until last element (stress series) of list given by PINN_Dataset\n",
    "\n",
    "    nonseries_list = []\n",
    "\n",
    "    # nonser_len = len(nonserbatch)\n",
    "    for i in range(5):\n",
    "        data_list = []\n",
    "        for data in nonserbatch:\n",
    "            # print(type(data), len(data))\n",
    "            data_list.append(data[i])\n",
    "        data_list = torch.tensor(np.asarray(data_list), requires_grad=True)\n",
    "        nonseries_list.append(data_list)\n",
    "\n",
    "    \n",
    "    padded_stress_ser = pad_sequence(stress_ser, batch_first=True, padding_value=0)\n",
    "    padded_strain_ser = pad_sequence(strain_ser, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    return nonseries_list, padded_strain_ser, padded_stress_ser, max_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trdata = PINN_Dataset(params, idxTr)\n",
    "trloader = DataLoader(trdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "valdata = PINN_Dataset(params, idxVal)\n",
    "valloader = DataLoader(valdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "tedata = PINN_Dataset(params, idxTe)\n",
    "teloader = DataLoader(tedata, batch_size=batch_size, collate_fn = padded_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(trdata.__getitem__(0)[0][:]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset    returns:   featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain,               stress_series_dic\n",
    "\n",
    "# dataloader returns:   nonseries_list,                                   padded_strain_ser,    padded_stress_ser, max_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, coeff_max_min_dict = max_min_dict, offset = -0.01):\n",
    "        super(PINN_loss, self).__init__()\n",
    "\n",
    "        self.offset = offset\n",
    "        self.coeff_max_min_dict = coeff_max_min_dict\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, stress_series_predicted, constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, \n",
    "                 W_data, sig_pl_data, strain): # , , stress_max_min_array, <<-- don't think I need these because I'm calculating the loss as scaled\n",
    "        \n",
    "        stress_series_eqn_calculated_scaled_output = self.constitutive_equation(constitutive_equation_coefficients_predicted, strain)\n",
    "        # print(stress_series_eqn_calculated_scaled[0].shape)\n",
    "        stress_series_eqn_calculated_scaled = stress_series_eqn_calculated_scaled_output[0].T\n",
    "\n",
    "        predicted_sig_pl = self.eqn_predict_sig_pl(stress_series_eqn_calculated_scaled)\n",
    "\n",
    "        W_pred = self.eqn_predicted_W(stress_series_predicted, strain)\n",
    "        print(f\"W_pred: {W_pred.detach().cpu().numpy()}\\tSig_pl: {predicted_sig_pl.detach().cpu().numpy()}\")\n",
    "        \n",
    "        W_loss = nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "        sig_pl_loss = nn.L1Loss()(predicted_sig_pl, sig_pl_data)\n",
    "        \n",
    "\n",
    "        constit_equation_coeff_loss = nn.L1Loss()(constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data) #, self.offset, self.coeff_max_min_dict\n",
    "\n",
    "        stress_series_predicted = torch.squeeze(stress_series_predicted, dim=-1)\n",
    "        physics_loss = nn.L1Loss()(stress_series_predicted, stress_series_eqn_calculated_scaled)\n",
    "\n",
    "        total_loss = W_loss + sig_pl_loss + constit_equation_coeff_loss + physics_loss\n",
    "\n",
    "        output_dic = {\n",
    "            'total_loss': total_loss,\n",
    "            'W_loss': W_loss,\n",
    "            'sig_pl loss': sig_pl_loss,\n",
    "            'constit_equation_coeff_loss': constit_equation_coeff_loss,\n",
    "            'physics_loss': physics_loss,\n",
    "            'stress_series_eqn_calculated_scaled':stress_series_eqn_calculated_scaled, \n",
    "            'predicted_sig_pl': predicted_sig_pl, \n",
    "            'W_pred': W_pred, \n",
    "            'stress_series_predicted': stress_series_predicted, \n",
    "            'stress_series_eqn_calculated_scaled_output':stress_series_eqn_calculated_scaled_output\n",
    "        }\n",
    "\n",
    "        return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output, output_dic\n",
    "# return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, \n",
    "# stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "\n",
    "\n",
    "    def unscale_coeffs(self, coeffs, coeff_max_min_dict, coeff_list =['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]):\n",
    "        unscaled_coeffs = torch.zeros_like(coeffs)\n",
    "\n",
    "        for i in range(coeffs.shape[0]):\n",
    "            for j, name in enumerate(coeff_list):\n",
    "                max = coeff_max_min_dict[name][0]\n",
    "                min = coeff_max_min_dict[name][1]\n",
    "\n",
    "                unscaled_coeffs[i][j] = (coeffs[i][j] * (max - min)) + min\n",
    "\n",
    "        return unscaled_coeffs\n",
    "    \n",
    "    def unscale_dyn_params(array, coeff_max_min_dict, dyn_param = 'sig_pl'):\n",
    "        max = coeff_max_min_dict[dyn_param][0]\n",
    "        min = coeff_max_min_dict[dyn_param][1]\n",
    "\n",
    "        array = array*(max - min) + min\n",
    "\n",
    "        return array\n",
    "    \n",
    "    def constitutive_equation(self, coeffs_pred, strain):\n",
    "\n",
    "        coeffs_pred_unscaled = self.unscale_coeffs(coeffs_pred, self.coeff_max_min_dict)\n",
    "        A_pred = coeffs_pred_unscaled[:, 0]\n",
    "        B_pred = coeffs_pred_unscaled[:, 1]\n",
    "        C_pred = coeffs_pred_unscaled[:, 2]\n",
    "        m_pred = coeffs_pred_unscaled[:, 3]\n",
    "        n_pred = coeffs_pred_unscaled[:, 4]\n",
    "\n",
    "        mask = torch.ge(strain.T, self.offset*-1)\n",
    "        strain = strain.T + self.offset\n",
    "        strain = torch.where(mask, strain, torch.tensor(0))\n",
    "\n",
    "\n",
    "        # stress_series_eqn_calculated = A_pred * (strain.T + self.offset)**m_pred + B_pred*((strain.T + self.offset)/(C_pred-(strain.T + self.offset)))**n_pred\n",
    "        stress_series_eqn_calculated = A_pred * (strain + self.offset)**m_pred + B_pred*((strain + self.offset)/(C_pred-(strain + self.offset)))**n_pred\n",
    "\n",
    "        stress_series_eqn_calculated = torch.nan_to_num(stress_series_eqn_calculated, nan = 0.0)\n",
    "\n",
    "\n",
    "        max = torch.max(stress_series_eqn_calculated, dim=0)[0]\n",
    "\n",
    "        min = torch.min(stress_series_eqn_calculated, dim=0)[0]\n",
    "\n",
    "        stress_ser_range = max - min\n",
    "\n",
    "        # stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(1)) / stress_ser_range.unsqueeze(1)\n",
    "        stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(0)) / stress_ser_range.unsqueeze(0)\n",
    "\n",
    "        out_dic = {\n",
    "            'stress_ser_eqn_calculated_scaled': stress_ser_eqn_calculated_scaled,\n",
    "            'coeffs_pred_unscaled': coeffs_pred_unscaled,\n",
    "            'coeffs_pred': coeffs_pred,\n",
    "            'max': max,\n",
    "            'min': min,\n",
    "            'stress_ser_range': stress_ser_range,\n",
    "            'stress_series_eqn_calculated': stress_series_eqn_calculated,\n",
    "            'strain': strain\n",
    "        }\n",
    "\n",
    "\n",
    "        return stress_ser_eqn_calculated_scaled, coeffs_pred_unscaled, coeffs_pred, max, min, stress_ser_range, stress_series_eqn_calculated, strain, out_dic\n",
    "        \n",
    "        # return stress_series_eqn_calculated\n",
    "    \n",
    "    def eqn_predict_sig_pl(self, stress_series_eqn_calculated): # CALLED\n",
    "         \n",
    "        sig_pl_pred = torch.mean(stress_series_eqn_calculated[:, 200:400], dim=1)\n",
    "\n",
    "        return sig_pl_pred\n",
    "    \n",
    "    def eqn_predicted_W(self, stress_pred, strain): # CALLED\n",
    "        stress_start_idx = int(-1*self.offset*1e3)\n",
    "        stress_pred = torch.squeeze(stress_pred, dim=-1)\n",
    "        # print(strain.shape, stress_pred.shape)\n",
    "        W_pred = torch.trapz(stress_pred[:, stress_start_idx:], strain[:, stress_start_idx:], dim=1)\n",
    "        # print(f\"W_pred shape: {W_pred.shape}\")\n",
    "        return W_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, hidden_size=256, num_lstm_layers=4, lstm_output_dim=1):\n",
    "        numparams = len(params)\n",
    "        input_vec_dim = 1024 + numparams\n",
    "        linear_out_dims = 5\n",
    "        # self.series_in_dim = series_input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_output_dim = lstm_output_dim\n",
    "        super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "        self.lstm_linear = nn.Sequential(nn.Linear(hidden_size, lstm_output_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "        self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "            nn.Linear(input_vec_dim, 1024),\n",
    "            nn.Linear(1024, 512),nn.ReLU(),\n",
    "            nn.Linear(512, 256),nn.ReLU(),\n",
    "            nn.Linear(256, 128),nn.ReLU(),\n",
    "            nn.Linear(128, linear_out_dims)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, feature_vec, property_vec, strain_series):\n",
    "        input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "        batch_size = strain_series.size(0)\n",
    "        h = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        c = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        \n",
    "        stress_ser = []\n",
    "\n",
    "        for i in range(strain_series.size(1)):\n",
    "            stress, (h, c) = self.stress_ser_predictor(input_vec, (h, c))\n",
    "            stress = self.lstm_linear(stress)\n",
    "            stress_ser.append(stress)\n",
    "       \n",
    "        stress_ser = torch.stack(stress_ser, dim=1)\n",
    "\n",
    "        constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "        print(f'constit_eqn_coeffs predicted\\n{constit_eqn_coeffs.detach().cpu().numpy()}')\n",
    "\n",
    "        return stress_ser, constit_eqn_coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_Stress_PINN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, hidden_size=256, num_lstm_layers=4, lstm_output_dim=1):\n",
    "        numparams = len(params)\n",
    "        input_vec_dim = 1024 + numparams\n",
    "        linear_out_dims = 5\n",
    "        # self.series_in_dim = series_input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_output_dim = lstm_output_dim\n",
    "        super(Dynamic_Stress_PINN2, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "        self.lstm_linear = nn.Sequential(nn.BatchNorm1d(hidden_size), nn.Linear(hidden_size, lstm_output_dim),  nn.ReLU())\n",
    "\n",
    "\n",
    "        # self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "        #     nn.Linear(input_vec_dim, 512),\n",
    "        #     nn.Linear(512, 64),nn.ReLU(),\n",
    "        #     nn.Linear(64, linear_out_dims)\n",
    "        # )\n",
    "        self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "            nn.Linear(input_vec_dim, 512),\n",
    "            nn.BatchNorm1d(512),  # BatchNorm1d for 1D input (e.g., after linear layers)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, linear_out_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.LSTM):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        init.orthogonal_(param)\n",
    "\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                init.uniform_(module.weight, a=0, b=1)\n",
    "\n",
    "\n",
    "    def forward(self, feature_vec, property_vec, strain_series):\n",
    "        input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "        batch_size = strain_series.size(0)\n",
    "        h = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        c = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        \n",
    "        stress_ser = []\n",
    "\n",
    "        for i in range(strain_series.size(1)):\n",
    "            stress, (h, c) = self.stress_ser_predictor(input_vec, (h, c))\n",
    "            stress = self.lstm_linear(stress)\n",
    "            stress_ser.append(stress)\n",
    "       \n",
    "        stress_ser = torch.stack(stress_ser, dim=1)\n",
    "\n",
    "        constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "        print(f'constit_eqn_coeffs predicted\\n{constit_eqn_coeffs.detach().cpu().numpy()}')\n",
    "\n",
    "        return stress_ser, constit_eqn_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinn = Dynamic_Stress_PINN(params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn2 = Dynamic_Stress_PINN2(params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# summary(pinn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns-->         featvec, paramvec, constit_eqn_coeffs, W, sig_pl, || strain,            || stress_series,\n",
    "# dataloader returns:->     ^--------------nonseries_list------------------^  || padded_strain_ser, || padded_stress_ser\n",
    "\n",
    "# dataset    returns:   featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain,               stress_series_dic\n",
    "# dataloader returns:   nonseries_list,                                   padded_strain_ser,    padded_stress_ser, max_mins\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "beta = 2.0\n",
    "gamma = 1.0\n",
    "\n",
    "def pinn_train(pinn, dataloader, loss_func, optimizer, alpha=alpha, beta=beta, gamma=gamma): #PINN_loss\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    # running_loss = 0.0\n",
    "    total_run_loss = 0.0\n",
    "    \n",
    "    dyn_param_run_loss = 0.0\n",
    "    coeff_run_loss = 0.0\n",
    "    phys_run_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    for non_series_data, strain_series, stress_series, max_mins in pbar:\n",
    "        global feature_vec, param_vec, strs_ser_glob, coeffs_glob, strn_ser_glob\n",
    "        feature_vec = non_series_data[0].cuda()\n",
    "        param_vec   = non_series_data[1].cuda()\n",
    "        strs_ser_glob = stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "        coeffs_glob = const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "        W = non_series_data[3].cuda()\n",
    "        # print(f\"W shape: {W.shape}\")\n",
    "        sig_pl = non_series_data[4].cuda()\n",
    "        strain_series = strn_ser_glob = strain_series.cuda()\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        global stress_series_pred\n",
    "        global constit_eqn_coeffs_pred\n",
    "        stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "        # print(stress_series_pred.shape, constit_eqn_coeffs_pred.shape)\n",
    "\n",
    "        # loss_val, W_loss, sig_pl_loss, const_eqn_coeff_loss, phys_loss, *args = loss_func()(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "                                                                    # PINN_loss(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs_data, W_data, sig_pl_data, strain, offset=-0.01):\n",
    "        #total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "        \n",
    "        global loss_val, W_loss, sig_pl_loss, const_eqn_coeff_loss, phys_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "        loss_val, W_loss, sig_pl_loss, const_eqn_coeff_loss, phys_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output = loss_func()(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "        loss_val = alpha*W_loss + alpha*sig_pl_loss + beta*const_eqn_coeff_loss + gamma*phys_loss\n",
    "        dyn_param_loss = W_loss + sig_pl_loss\n",
    "        loss_val.backward() #retain_graph=True\n",
    "        # torch.nn.utils.clip_grad_norm_(pinn.parameters(), max_grad_norm)\n",
    "\n",
    "        for name, param in pinn.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f'Gradient {name}: {param.grad.norm().item()}')\n",
    "                # print(f'Gradient {name}: {param.grad}')\n",
    "\n",
    "                \n",
    "        optimizer.step()\n",
    "        # running_loss += loss_val.item()\n",
    "        total_run_loss += loss_val.item()\n",
    "        dyn_param_run_loss += dyn_param_loss.item()\n",
    "        coeff_run_loss += const_eqn_coeff_loss.item()\n",
    "        phys_run_loss += phys_loss.item()\n",
    "        # pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "        pbar.set_description(f'Losses:\\tTotal: {total_run_loss / (pbar.n + 1):.4f}\\tDyn Params: {dyn_param_run_loss / (pbar.n + 1):.4f}\\tCoeff: {coeff_run_loss / (pbar.n + 1):.4f}\\tPhysics: {phys_run_loss / (pbar.n + 1):.4f}\\t')\n",
    "        # print(\"Data index is: f{idx}\")\n",
    "    return total_run_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_validate(pinn, dataloader, loss_func=PINN_loss):\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    with torch.no_grad():\n",
    "        for non_series_data, strain_series, stress_series, max_mins in pbar:\n",
    "            feature_vec = non_series_data[0].cuda()\n",
    "            param_vec   = non_series_data[1].cuda()\n",
    "            stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "            const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "            W = non_series_data[3].cuda()\n",
    "            sig_pl = non_series_data[4].cuda()\n",
    "            strain_series = strain_series.cuda()\n",
    "\n",
    "            stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "\n",
    "            loss_val, dyn_param_loss, const_eqn_coeff_loss, phys_loss = loss_func()(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "            # return total_loss, loss_data_1, loss_data_2, loss_physics\n",
    "            \n",
    "            running_loss += loss_val.item()\n",
    "            pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}\\t ')\n",
    "        return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training hyper(?)parameters\n",
    "model = pinn2\n",
    "EPOCHS = 125\n",
    "\n",
    "lossfunc = torch.nn.L1Loss() # this is MAE loss\n",
    "lossfunc_name = 'MAE'\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "cp_name = f'CP_{fname_base}.pth'\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)\n",
    "print(best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 75\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "# earlystop_min_delta = 0.000075\n",
    "earlystop_min_delta = 0.00075 # For L1Loss (MAE)\n",
    "\n",
    "# os.makedirs(best_weights_path, exist_ok=True)\n",
    "best_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs_completed=0\n",
    "\n",
    "lrate = 1e-7\n",
    "optimizer = optim.SGD(model.parameters(), lr=lrate)#optim.Adam(pinn.parameters(), lr=0.00001)\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n",
    "lossfunc = PINN_loss\n",
    "\n",
    "try:\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train the model\n",
    "        train_loss = pinn_train(model, trloader, lossfunc, optimizer)\n",
    "        train_loss_val = train_loss[0]\n",
    "        train_dyn_param_loss = train_loss[1]\n",
    "        train_const_eqn_coeff_loss = train_loss[2]\n",
    "        train_phys_loss = train_loss[3]\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = pinn_train(model, valloader, lossfunc)\n",
    "        val_loss_val = val_loss[0]\n",
    "        val_dyn_param_loss = val_loss[1]\n",
    "        val_const_eqn_coeff_loss = val_loss[2]\n",
    "        val_phys_loss = val_loss[3]\n",
    "\n",
    "        print(f'training:\\ttotal loss: {train_loss_val:.5f}, dynamic param loss: {train_dyn_param_loss:.5f}\\nconstitutive equation coefficient loss: {train_const_eqn_coeff_loss:.5f}, physics_loss: {train_phys_loss:.5f}')\n",
    "        print(f'validation:\\ttotal_loss:{val_loss_val:.5f}, dynamic param loss: {val_dyn_param_loss:.5f}\\nconstitutive equation coefficient loss: {val_const_eqn_coeff_loss:.5f}, physics_loss: {val_phys_loss:.5f}')\n",
    "\n",
    "\n",
    "        # Save the model's weights if validation loss is improved\n",
    "        improvement_delta = best_val_loss - val_loss_val\n",
    "\n",
    "        if val_loss_val < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_weights_path)  # Save model weights to file\n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "\n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "\n",
    "\n",
    "        # Collect model training history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "            model.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss_val:.5f}\\tValidation Loss: {val_loss_val:.5f}')\n",
    "\n",
    "        epochs_completed +=1\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    model.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "    model.load_state_dict(torch.load(best_weights_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled_output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress_series_eqn_calculated_scaled_output[-1].T[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(batch[2][:, 200:400].detach().cpu().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(batch[1][0,:].detach().cpu().numpy(),batch[2][0,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(batch[1].detach().cpu().numpy(),batch[2].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(stress_series_eqn_calculated_scaled.detach().cpu().numpy(), stress_series_eqn_calculated_scaled_output[-1].T.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px.scatter(x = stress_series_eqn_calculated_scaled_output[-1].T.detach().cpu().numpy()[0,:], y=stress_series_eqn_calculated_scaled.detach().cpu().numpy()[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.trapz(stress_series_eqn_calculated_scaled, stress_series_eqn_calculated_scaled_output[-1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(stress_series_eqn_calculated_scaled_output[-1].cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(stress_series_eqn_calculated_scaled.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(stress_series_eqn_calculated_scaled_output[-1].T, stress_series_eqn_calculated_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress_series_eqn_calculated_scaled_output[-1].T[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strn_ser_glob[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(stress_series_eqn_calculated_scaled[0,:], strn_ser_glob[0,:], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strn_ser_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress_series_eqn_calculated_scaled[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns-->         featvec, paramvec, constit_eqn_coeffs, W, sig_pl, || strain,            || stress_series,\n",
    "# dataloader returns:->     ^--------------nonseries_list------------------^  || padded_strain_ser, || padded_stress_ser\n",
    "\n",
    "# dataset    returns:   featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain,               stress_series_dic\n",
    "# dataloader returns:   nonseries_list,                                   padded_strain_ser,    padded_stress_ser, max_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn2.constit_eqn_coeff_predictor[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test2 = pinn2(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss2 = PINN_loss()(out_test2[0].cpu(), out_test2[1].cpu(), batch[0][2], batch[0][3], batch[0][4], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss2[-1][-2][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss2[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss2[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in pinn2.named_parameters():\n",
    "    print(f\"Gradient {name}:\\t {param.grad.shape}\\t{param.grad.norm().item()} \\t {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = pinn2(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = PINN_loss()(out_test[0].cpu(), out_test[1].cpu(), batch[0][2], batch[0][3], batch[0][4], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in pinn2.named_parameters():\n",
    "    print(f\"Gradient {name}:\\t {param.grad.shape}\\t{param.grad.norm().item()} \\t {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invec = torch.cat([batch[0][0], batch[0][1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn2.constit_eqn_coeff_predictor(invec.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pinn2(batch[0][0].cuda(), batch[0][1].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].mT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain, stress_series_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> __What have I learned below?__</u> __what was the question I was pursuing anyway?__\n",
    "## Answer: \n",
    "### Why is W_pred (predicted energy absorbed) __EVER__ less than 0???\n",
    "### Answer to the answer:\n",
    "# BECAUSE OF torch (or numpy) .trapz\n",
    "### BECAUSE, the way I've padded my data, specifically my strain values, when it gets to the end, it goes from the highest strain value in the series, to 0.00\n",
    "### Trapz calculates based on $(x_{i} - x_{i-1}) /2$ , which means that if x_{i} is 0.000 and x_{i-1} is NOT, i.e., is like 0.590, then the calculated value of $\\Delta y$\n",
    "### is multiplied by a negative number\n",
    "# __TO FIX THIS__, I am going to, above, find the index for each strain series where the padding starts, and then when calculating .trapz, break out each series and calculate\n",
    "## .trapz [:padding_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval = PINN_loss()(out[0], out[1], batch[0][2].cuda(), batch[0][3].cuda(), batch[0][4].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['predicted_sig_pl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.array([[ 0,  1,  2, -3,  3],\n",
    "                    [-1,  1,  2, -1,  1]]) <0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(lossval[-1]['stress_series_predicted'].detach().cpu().numpy() <0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strntest = lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_series = torch.tensor([[1.0, 2.0, 3.0, 0.0, 0.0],\n",
    "                               [4.0, 5.0, 0.0, 0.0, 0.0]])\n",
    "x_series = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "                         [1.0, 2.0, 3.0, 4.0, 5.0]])\n",
    "\n",
    "# Identify the start index of the padding\n",
    "start_of_padding = (padded_series == 0).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_of_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (strntest == 0).sum(dim=-1)\n",
    "np.argmax(strntest[100:].detach().cpu().numpy() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strntest[580:590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['W_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][580:584,0] # torch.Size([618, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][580:584,0]#.shape#[:,580:583] #torch.Size([618, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:584,0].detach().cpu().numpy()\n",
    "x = lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:584,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:583,0].detach().cpu().numpy()\n",
    "x2 = lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:583,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store running total and previous x, y values\n",
    "trapz_sum2 = 0.0\n",
    "prev_x = x2[0]\n",
    "prev_y = y2[0]\n",
    "\n",
    "trapz_values2 = []\n",
    "running_totals2 = []\n",
    "\n",
    "# Loop through the arrays\n",
    "for i in range(1, len(x2)):\n",
    "    # Current x, y values\n",
    "    current_x = x2[i]\n",
    "    current_y = y2[i]\n",
    "\n",
    "    # Calculate the trapezoidal rule for this segment\n",
    "    trapz_segment2 = (current_x - prev_x) * (prev_y + current_y) / 2\n",
    "    \n",
    "    # Update the running total\n",
    "    trapz_sum2 += trapz_segment2\n",
    "    \n",
    "    # Append the incremental calculation and running total to the lists\n",
    "    trapz_values2.append(trapz_segment2)\n",
    "    running_totals2.append(trapz_sum2)\n",
    "    \n",
    "    # Update previous x, y values for the next iteration\n",
    "    prev_x = current_x\n",
    "    prev_y = current_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapz_sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:583,0], lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:583,0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][580:585,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][580:585,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][580:585,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:585,0], lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:585,3], dim=0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:583,0], lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:583,3], dim=0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:,0].detach().cpu().numpy(), lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:,3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_totals2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store running total and previous x, y values\n",
    "trapz_sum = 0.0\n",
    "prev_x = x[0]\n",
    "prev_y = y[0]\n",
    "\n",
    "trapz_values = []\n",
    "running_totals = []\n",
    "\n",
    "# Loop through the arrays\n",
    "for i in range(1, len(x)):\n",
    "    # Current x, y values\n",
    "    current_x = x[i]\n",
    "    current_y = y[i]\n",
    "\n",
    "    # Calculate the trapezoidal rule for this segment\n",
    "    trapz_segment = (current_x - prev_x) * (prev_y + current_y) / 2\n",
    "    \n",
    "    # Update the running total\n",
    "    trapz_sum += trapz_segment\n",
    "    \n",
    "    # Append the incremental calculation and running total to the lists\n",
    "    trapz_values.append(trapz_segment)\n",
    "    running_totals.append(trapz_sum)\n",
    "    \n",
    "    # Update previous x, y values for the next iteration\n",
    "    prev_x = current_x\n",
    "    prev_y = current_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trapz_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:,:583], lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:,:583], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][:,0], lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'][:,3], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'], lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['strain'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'].detach().cpu().numpy() < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(lossval[-1]['stress_series_eqn_calculated_scaled_output'][-1]['stress_ser_eqn_calculated_scaled'][200:400, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossval[5][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in pinn2.named_parameters():\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(pinn2.named_parameters()))[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in pinn2.named_parameters():\n",
    "    if param is not None:\n",
    "        print(f\"Gradient {name}:\\t {param.grad.shape}\\t{param.grad.norm().item()} \\t {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled_output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled_output[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(stress_series_eqn_calculated_scaled_output[-1], dim=0)[0] - torch.min(stress_series_eqn_calculated_scaled_output[-1], dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(stress_series_eqn_calculated_scaled_output[-1], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_test = stress_series_eqn_calculated_scaled_output[-2][:,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_series_test = stress_series_eqn_calculated_scaled_output[-1][:,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x = strain_series_test, y = stress_series_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_test = stress_series_eqn_calculated_scaled_output[-2][:,1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_series_test = stress_series_eqn_calculated_scaled_output[-1][:,1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x = strain_series_test, y = stress_series_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(stress_series_eqn_calculated_scaled[:, 200:400], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)[-1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "\n",
    "histno=1\n",
    "histpath = os.path.join(nbpath,'model_jsons',f'{cp_name[:-4]}_training_history{histno}_{epochs_completed}ep.json')\n",
    "\n",
    "pinn.eval()\n",
    "\n",
    "with open(histpath, 'w') as f:\n",
    "    json.dump(hist_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
