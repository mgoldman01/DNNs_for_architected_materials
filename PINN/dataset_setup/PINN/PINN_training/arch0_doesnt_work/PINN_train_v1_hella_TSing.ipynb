{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import workflow_utils_v3\n",
    "import sys\n",
    "\n",
    "from workflow_utils_v3.FileDirectory import Directory\n",
    "\n",
    "dirs = Directory(rootpath = '/home/mgolub4/DLproj/MLTO_2024/')\n",
    "\n",
    "# Sets directory of entire package\n",
    "# rootpath = '/data/tigusa1/MLTO_UCAH/MLTO_2023/'\n",
    "\n",
    "nbpath = os.path.join(dirs._3_Dynamic_PINN_RNN, 'PINN_training')\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle\n",
    "from plotly.colors import sequential, qualitative\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "# import torchsummary\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split as TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '25APR24'\n",
    "fname_base = f'Dyn_PINN_v0_{date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready.csv'\n",
    "dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready_coeffs_scaled.csv'\n",
    "dyndb = pd.read_csv(dyndb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt', 'plateau_stress_g', 'energy_absorbed_g']\n",
    "dict_entries = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt', 'sig_pl', 'W']\n",
    "\n",
    "max_min_dict = {}\n",
    "\n",
    "for par, name in zip(max_min_params, dict_entries):\n",
    "    data = dyndb[par]\n",
    "    max = data.max()\n",
    "    min = data.min()\n",
    "    max_min_dict[name] = (max, min) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTr, idxRem = TTS(dyndb, stratify = dyndb['topology_family'], random_state=42, train_size = 0.8)\n",
    "idxVal, idxTe = TTS(idxRem, random_state = 42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['volFrac', \n",
    "        'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "        'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "        'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "        'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "        'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "        'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "        'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(array):\n",
    "    max = array.max()\n",
    "    min = array.min()\n",
    "\n",
    "    array = (array - min) / (max - min)\n",
    "\n",
    "    return array, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale(array, max, min):\n",
    "    array = array * (max-min) + min\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stress_Series:\n",
    "    def __init__(self, series):\n",
    "        self.series = series\n",
    "        self.max = series.max()\n",
    "        self.min = series.min()\n",
    "\n",
    "    def scale(self):\n",
    "        return (self.series - self.min) / (self.max - self.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def min_max_scale_series(batched_time_series):\n",
    "    # Calculate the maximum and minimum values for each series\n",
    "    max_values = torch.max(batched_time_series, dim=1)[0]\n",
    "    min_values = torch.min(batched_time_series, dim=1)[0]\n",
    "\n",
    "    # Calculate the range for each series\n",
    "    series_range = max_values - min_values\n",
    "\n",
    "    # Ensure non-zero range to avoid division by zero\n",
    "    # series_range = torch.where(series_range == 0, torch.tensor(1e-7), series_range)\n",
    "\n",
    "    # Min-max scale each series\n",
    "    scaled_time_series = (batched_time_series - min_values.unsqueeze(1)) / series_range.unsqueeze(1)\n",
    "\n",
    "    return scaled_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example usage:\n",
    "# # Assuming batched_time_series is a padded batched tensor of size (batch_size, longest_series)\n",
    "# batched_time_series = torch.tensor([[0.0, 2.0, 3.0, 4.0, 5.0],\n",
    "#                                     [0.0, 3.0, 4.0, 5.0, 6.0],\n",
    "#                                     [0.0, 4.0, 5.0, 6.0, 7.0]])\n",
    "\n",
    "# time = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# scaled_time_series = min_max_scale_series(batched_time_series)\n",
    "# print(\"Scaled time series:\")\n",
    "# print(scaled_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Proving that if I calculate W from scaled data and then unscale, the results are the same\n",
    "# data_time_series = torch.tensor([[0.0, 2.0, 3.0, 4.0, 5.0],\n",
    "#                                     [0.0, 3.0, 4.0, 5.0, 6.0],\n",
    "#                                     [0.0, 4.0, 5.0, 6.0, 7.0]])\n",
    "\n",
    "# pred_time_series = torch.tensor([[10.0, 3.0, 3.0, 8.0, 5.0],\n",
    "#                                     [4.0, 0.0, 1.0, 3.0, 8.0],\n",
    "#                                     [1.0, 3.0, 5.0, 3.0, 5.0]])\n",
    "\n",
    "# time = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# data_W = torch.trapz(data_time_series, time, dim=1)\n",
    "# pred_W = torch.trapz(pred_time_series, time, dim=1)\n",
    "\n",
    "# scaled_data =  min_max_scale_series(data_time_series)\n",
    "\n",
    "# scaled_pred = min_max_scale_series(pred_time_series)\n",
    "\n",
    "# data_W_scaled = torch.trapz(scaled_data, time, dim=1)\n",
    "# pred_W_scaled = torch.trapz(scaled_pred, time, dim=1)\n",
    "\n",
    "# print(data_W, '\\t', data_W_scaled)\n",
    "# print(pred_W, '\\t', pred_W_scaled)\n",
    "\n",
    "# max = torch.max(data_time_series, dim=1)\n",
    "# min = torch.min(data_time_series, dim=1)\n",
    "# range = max[0] - min[0]\n",
    "# data_W_scaled * range + min[0]\n",
    "# torch.max(pred_time_series)\n",
    "\n",
    "# scaled_time_series = min_max_scale_series(batched_time_series)\n",
    "# print(\"Scaled time series:\")\n",
    "# print(scaled_time_series)\n",
    "# torch.max(batched_time_series, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PINN_loss(nn.Module):\n",
    "\n",
    "#     def __init__(self, coeff_max_min_dict, offset = -0.01):\n",
    "#         super(PINN_loss, self).__init__()\n",
    "\n",
    "#     \"\"\"\n",
    "#        1 What is based on each instance of this list?\n",
    "#         stress_series_predicted, \n",
    "#         constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, \n",
    "#         W_data, sig_pl_data, strain, coeff_max_min_dict,  offset = -0.01, ):\n",
    "        \n",
    "#         A:\n",
    "#         stress_series_predicted,\n",
    "#         constitutive_equation_coefficients_predicted,\n",
    "#         constitutive_equation_coefficients_data,\n",
    "#         W_data, sig_pl_data, strain,\n",
    "#         stress_max_min_array,\n",
    "\n",
    "#        2 What is constant to the database as a whole?\n",
    "#         A:\n",
    "#         coeff_max_min_dict\n",
    "        \n",
    "#         and\n",
    "        \n",
    "#         offset, which is user defined... \n",
    "#         so only\n",
    "\n",
    "#         coeff_max_min_dict and\n",
    "#         offset are intrinsic to the PINN_loss call\n",
    "\n",
    "#         \"\"\"\n",
    "        \n",
    "\n",
    "#         # self.strs_pred = stress_series_predicted\n",
    "#         # self.coeffs_pred = constitutive_equation_coefficients_predicted\n",
    "#         # self.coeffs_data= constitutive_equation_coefficients_data\n",
    "        \n",
    "#         # self.W_data = W_data\n",
    "#         # self.W_data_unscaled = self.unscale\n",
    "        \n",
    "#         # self.sig_pl_data = sig_pl_data\n",
    "        \n",
    "#         # self.strain = strain\n",
    "#         # self.offset = offset\n",
    "        \n",
    "#         # self.coeff_max_min_dict = coeff_max_min_dict\n",
    "\n",
    "#         # self.stress_max_min_array = stress_max_min_array\n",
    "\n",
    "#         # self.A_pred = self.coeffs_pred[:, 0]\n",
    "#         # self.B_pred = self.coeffs_pred[:, 1]\n",
    "#         # self.C_pred = self.coeffs_pred[:, 2]\n",
    "#         # self.m_pred = self.coeffs_pred[:, 3]\n",
    "#         # self.n_pred = self.coeffs_pred[:, 4]\n",
    "        \n",
    "\n",
    "#         # if len(self.strs_pred.shape) == 3:\n",
    "#         #     self.strs_pred = torch.squeeze(self.strs_pred, axis=2)\n",
    "#         # else:\n",
    "#         #     pass\n",
    "\n",
    "#     def forward(self, stress_series_predicted, constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, \n",
    "#                  W_data, sig_pl_data, strain): # , coeff_max_min_dict, stress_max_min_array, <<-- don't think I need these because I'm calculating the loss as scaled\n",
    "        \n",
    "#         stress_series_eqn_calculated_scaled = self.constitutive_equation(constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, strain, self.offset)\n",
    "#         stress_series_eqn_calculated_scaled = stress_series_eqn_calculated_scaled.T\n",
    "\n",
    "#         predicted_sig_pl = self.eqn_predict_sig_pl(stress_series_eqn_calculated_scaled)\n",
    "\n",
    "#         W_pred = self.eqn_predicted_W(stress_series_predicted, strain)\n",
    "\n",
    "#         W_loss = nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "#         sig_pl_loss = nn.L1Loss()(predicted_sig_pl, sig_pl_data)\n",
    "\n",
    "#         constit_equation_coeff_loss = nn.L1Loss()(constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data)\n",
    "\n",
    "#         physics_loss = nn.L1Loss()(stress_series_predicted, stress_series_eqn_calculated_scaled)\n",
    "\n",
    "#         total_loss = W_loss + sig_pl_loss + constit_equation_coeff_loss + physics_loss\n",
    "\n",
    "#         return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss\n",
    "\n",
    "\n",
    "#     def unscale_coeffs(self, coeffs, coeff_max_min_dict, coeff_list =['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]):\n",
    "#         unscaled_coeffs = torch.zeros_like(coeffs)\n",
    "\n",
    "#         for i in range(coeffs.shape[0]):\n",
    "#             for j, name in enumerate(coeff_list):\n",
    "#                 max = coeff_max_min_dict[name][0]\n",
    "#                 min = coeff_max_min_dict[name][1]\n",
    "\n",
    "#                 unscaled_coeffs[i][j] = (coeffs[i][j] * (max - min)) + min\n",
    "\n",
    "#         return unscaled_coeffs\n",
    "    \n",
    "#     def unscale_dyn_params(array, coeff_max_min_dict, dyn_param = 'sig_pl'):\n",
    "#         max = coeff_max_min_dict[dyn_param][0]\n",
    "#         min = coeff_max_min_dict[dyn_param][1]\n",
    "\n",
    "#         array = array*(max - min) + min\n",
    "\n",
    "#         return array\n",
    "\n",
    "#     def constitutive_equation(self, coeffs_pred, strain, offset):\n",
    "#         A_pred = coeffs_pred[:, 0]\n",
    "#         B_pred = coeffs_pred[:, 1]\n",
    "#         C_pred = coeffs_pred[:, 2]\n",
    "#         m_pred = coeffs_pred[:, 3]\n",
    "#         n_pred = coeffs_pred[:, 4]\n",
    "\n",
    "#         stress_series_eqn_calculated = A_pred * (strain.T + offset)**m_pred + B_pred*((strain.T + offset)/(C_pred-(strain.T + offset)))**n_pred\n",
    "        \n",
    "#         max = torch.max(stress_series_eqn_calculated, dim=1)[0]\n",
    "#         min = torch.min(stress_series_eqn_calculated, dim=1)[0]\n",
    "\n",
    "#         stress_ser_range = max - min\n",
    "\n",
    "#         stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(1)) / stress_ser_range.unsqueeze(1)\n",
    "\n",
    "#         return stress_ser_eqn_calculated_scaled\n",
    "        \n",
    "#         # return stress_series_eqn_calculated\n",
    "    \n",
    "#     def eqn_predict_sig_pl(self, stress_series_eqn_calculated): # CALLED\n",
    "         \n",
    "#         sig_pl_pred = torch.mean(stress_series_eqn_calculated[:, 200:400], dim=1)\n",
    "\n",
    "#         return sig_pl_pred\n",
    "    \n",
    "#     def eqn_predicted_W(self, stress_pred, strain, offset): # CALLED\n",
    "#         stress_start_idx = int(-1*offset*1e3)\n",
    "#         W_pred = torch.trapz(stress_pred[:, stress_start_idx:], strain[:, stress_start_idx:], dim=1)\n",
    "\n",
    "#         return W_pred\n",
    "\n",
    "#     # def sig_pl_loss(sig_pl_pred, sig_pl_data):\n",
    "#     #     # sig_pl_pred = self.unscale_dyn_params(sig_pl_pred, self.coeff_max_min_dict, 'sig_pl') ?????????\n",
    "#     #     # sig_pl_data = self.unscale_dyn_params(sig_pl_data, self.coeff_max_min_dict, 'sig_pl')``\n",
    "#     #     return nn.L1Loss()(sig_pl_pred, sig_pl_data)\n",
    "    \n",
    "#     # def W_loss(W_pred, W_data):\n",
    "#     #     # W_pred = self.unscale_dyn_params(W_pred, self.coeff_max_min_dict, 'W')\n",
    "#     #     # W_data = self.unscale_dyn_params(W_data, self.coeff_max_min_dict, 'W')\n",
    "#     #     return nn.L1Loss()(W_pred, W_data)\n",
    "    \n",
    "#     # def constitutive_equation_loss(coeffs_pred, coeffs_data):\n",
    "#     #     # coeffs_pred = self.unscale_coeffs(coeffs_pred)\n",
    "#     #     # coeffs_data = self.unscale_coeffs(coeffs_data)\n",
    "#     #     return nn.L1Loss()(coeffs_pred, coeffs_data)\n",
    "    \n",
    "#     # def physics_loss(stress_series_pred, stress_series_eqn_calculated):\n",
    "#     #     return nn.L1Loss()(stress_series_pred, stress_series_eqn_calculated)\n",
    "    \n",
    "\n",
    "#     # def calc_total_loss(sig_pl_loss, W_loss, constitutive_equation_loss, physics_loss):\n",
    "#     #     total_loss = sig_pl_loss + W_loss + constitutive_equation_loss + physics_loss\n",
    "\n",
    "#     #     return total_loss, sig_pl_loss, W_loss, constitutive_equation_loss, physics_loss\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Dataset(Dataset):\n",
    "    def __init__(self, params, split_dataframe,\n",
    "                 feat_vec_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/voxel_embedding_feature_maps', \n",
    "                 stress_series_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/stress_series_data', \n",
    "                 stress_ser_suffix = '_proct_gaus_btrlp_fftlp',\n",
    "                #  scale_coeffs_by=[1e9, 1e11, 1, 1, 1],\n",
    "                 predicted_parameters=True,\n",
    "                  ):\n",
    "        self.df = split_dataframe\n",
    "        self.featvec_dir = feat_vec_directory # for pulling the feature vectors\n",
    "        self.stress_ser_dir = stress_series_directory # for pulling the time series files\n",
    "        self.params = params\n",
    "        self.predicted_parameters = predicted_parameters\n",
    "        # self.const_eqn_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]\n",
    "        self.const_eqn_params = ['A_opt scaled', 'B_opt scaled', 'C_opt scaled', 'm_opt scaled', 'n_opt scaled',]\n",
    "\n",
    "        self.stress_ser_suffix = stress_ser_suffix\n",
    "        # self.scale_coeffs_by = scale_coeffs_by\n",
    "\n",
    "        # self.scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dyn_series_fname = self.df['dyn_file_name_original'].iloc[idx]\n",
    "\n",
    "        sig_pl = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['plateau_stress_g scaled'].values[0].astype(np.float32)\n",
    "        W = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['energy_absorbed_g scaled'].values[0].astype(np.float32)\n",
    "\n",
    "        # feature vector from convolutional neural network convolutional layers output\n",
    "        featvec_fname = self.df['conv_feat_vec'].iloc[idx] + '.npy'\n",
    "        featvec_path = os.path.join(self.featvec_dir, featvec_fname)\n",
    "        featvec = np.load(featvec_path)\n",
    "        featvec = np.squeeze(featvec, axis=0).astype(np.float32)\n",
    "\n",
    "        # constitutive equation parameters\n",
    "        constit_eqn_coeffs = np.asarray(self.df[self.const_eqn_params].iloc[idx])\n",
    "\n",
    "        # for i, scaler in enumerate(self.scale_coeffs_by):\n",
    "        #     constit_eqn_coeffs[i] = constit_eqn_coeffs[i] / scaler\n",
    "            \n",
    "\n",
    "\n",
    "        # predicted parameters\n",
    "        if self.predicted_parameters:\n",
    "            paramvec = np.asarray([self.df[f'pred {par}'].iloc[idx] for par in self.params]).astype(np.float32)\n",
    "        else:\n",
    "            paramvec = np.asarray([self.df[f'{par}'].iloc[idx] for par in self.params]).astype(np.float32)\n",
    "\n",
    "        # stress_series -- for now (April 24), Imma use the truncated datasets, because I think padded batches for RNNs in pytorch will take care of differing lengths\n",
    "        stress_ser_fname = dyn_series_fname + self.stress_ser_suffix\n",
    "        stress_ser_path = os.path.join(self.stress_ser_dir, stress_ser_fname+'.csv')\n",
    "        stress_series = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_gsreg']).astype(np.float32)\n",
    "        \n",
    "        stress_series = Stress_Series(stress_series)\n",
    "        scaled_stress_series = stress_series.scale()\n",
    "        stress_series_dic = {'stress_series': scaled_stress_series, 'max': stress_series.max, 'min': stress_series.min}\n",
    "\n",
    "        strain = np.asarray(pd.read_csv(stress_ser_path)['Strain']).astype(np.float32)\n",
    "                   \n",
    "\n",
    "        return featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain, stress_series_dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain\n",
    "# return nonseries_list, padded_strain_ser, padded_stress_ser\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def padded_collate(batch):\n",
    "    # get list of stress series -- uneven lengths, but will be padded at end\n",
    "    stress_ser = [torch.tensor(item[-1]['stress_series'], requires_grad=True) for item in batch]\n",
    "    # get max and min values as a list of lists, where the first value is the maximum and second is the minimum value of the accompanying stress series\n",
    "    max_mins = [[item[-1]['max'],item[-1]['min']] for item in batch]\n",
    "    max_mins = np.asarray(max_mins)\n",
    "\n",
    "    # get strain series, convert to tensor\n",
    "    strain_ser = [torch.tensor(item[-2], requires_grad=True) for item in batch]\n",
    "    \n",
    "    # get other data (featvec, paramvec, constit_eqn_coeffs, W, sig_pl) from data\n",
    "    nonserbatch = [item[0:-2] for item in batch] # all until last element (stress series) of list given by PINN_Dataset\n",
    "\n",
    "    nonseries_list = []\n",
    "\n",
    "    # nonser_len = len(nonserbatch)\n",
    "    for i in range(5):\n",
    "        data_list = []\n",
    "        for data in nonserbatch:\n",
    "            # print(type(data), len(data))\n",
    "            data_list.append(data[i])\n",
    "        data_list = torch.tensor(np.asarray(data_list), requires_grad=True)\n",
    "        nonseries_list.append(data_list)\n",
    "\n",
    "    \n",
    "    padded_stress_ser = pad_sequence(stress_ser, batch_first=True, padding_value=0)\n",
    "    padded_strain_ser = pad_sequence(strain_ser, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    return nonseries_list, padded_strain_ser, padded_stress_ser, max_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def PINN_loss(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs_data, W_data, sig_pl_data, strain):\n",
    "#     global strspred_glob, strn_glob, strs_start_glob, strs_const_eqn_glob\n",
    "#     strn_glob = strain\n",
    "#     offset=-0.01\n",
    "    \n",
    "#     A_pred = constit_eqn_coeffs_pred[:, 0]\n",
    "#     B_pred = constit_eqn_coeffs_pred[:, 1]\n",
    "#     C_pred = constit_eqn_coeffs_pred[:, 2]\n",
    "#     m_pred = constit_eqn_coeffs_pred[:, 3]\n",
    "#     n_pred = constit_eqn_coeffs_pred[:, 4]\n",
    "\n",
    "#     if len(stress_series_pred.shape) == 3:\n",
    "#         strspred_glob = stress_pred = torch.squeeze(stress_series_pred, axis=2)\n",
    "#     else:\n",
    "#         strspred_glob = stress_pred = stress_series_pred\n",
    "\n",
    "\n",
    "#     stress_series_constit_eqn = A_pred * (strain.T + offset)**m_pred + B_pred*((strain.T + offset)/(C_pred-(strain.T + offset)))**n_pred\n",
    "\n",
    "#     strs_const_eqn_glob = stress_series_constit_eqn = stress_series_constit_eqn.T\n",
    "\n",
    "#     sig_pl_pred = torch.mean(stress_series_constit_eqn[:,200:400], dim=1)\n",
    "#     sig_pl_data = sig_pl_data\n",
    "#     # print('sig_pl data, pred shape:')\n",
    "#     # print(sig_pl_data.shape)\n",
    "#     # print(sig_pl_pred.shape)\n",
    "    \n",
    "\n",
    "#     strs_start_glob = stress_start = int(-1*offset*1e3) # ensures the calculation starts at the right point of the stress series\n",
    "#     W_pred = torch.trapz(stress_pred[:, stress_start:], strain[:, stress_start:], dim=1)\n",
    "#     W_data = W_data\n",
    "#     # print('W data, pred shape:')\n",
    "#     # print(W_data.shape)\n",
    "#     # print(W_pred.shape)\n",
    "\n",
    "\n",
    "#     loss_data_1 = nn.L1Loss()(sig_pl_pred, sig_pl_data) + nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "#     loss_data_2 = nn.L1Loss()(constit_eqn_coeffs_pred, const_eqn_coeffs_data)\n",
    "\n",
    "#     loss_physics = nn.L1Loss()(stress_pred, stress_series_constit_eqn)\n",
    "\n",
    "#     total_loss = loss_data_1 + loss_data_2 + loss_physics\n",
    "\n",
    "#     return total_loss, loss_data_1, loss_data_2, loss_physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "trdata = PINN_Dataset(params, idxTr)\n",
    "trloader = DataLoader(trdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "valdata = PINN_Dataset(params, idxVal)\n",
    "valloader = DataLoader(valdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "tedata = PINN_Dataset(params, idxTe)\n",
    "teloader = DataLoader(tedata, batch_size=batch_size, collate_fn = padded_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(trdata.__getitem__(0)[0][:]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trdata.__getitem__(0)[0][200:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset    returns:   featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain,               stress_series_dic\n",
    "\n",
    "# dataloader returns:   nonseries_list,                                   padded_strain_ser,    padded_stress_ser, max_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(trloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in next(iter(trloader)):\n",
    "#     print(len(i))\n",
    "#     print(type(i))\n",
    "#     for j in i:\n",
    "#         print(j.shape)\n",
    "#         if j.shape[0] == 2 and len(j.shape) == 1:\n",
    "#             print(j, type(j), j.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(5):\n",
    "# #     print(next(iter(trloader))[0][i].shape)\n",
    "# feat = next(iter(trloader))[0][0]\n",
    "# pars = next(iter(trloader))[0][1]\n",
    "# # torch.cat([feat,pars], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print([i.shape for i in next(iter(trloader))[1]])\n",
    "# for part in next(iter(trloader)):\n",
    "#     print(type(part))\n",
    "#     if isinstance(part, list):\n",
    "#         print(len(part))\n",
    "#     if isinstance(part, torch.Tensor):\n",
    "#         print(part.shape)\n",
    "\n",
    "# for part in next(iter(trloader))[0]:\n",
    "#     print(part.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params, hidden_size=128, num_lstm_layers=4, lstm_output_dim=1):\n",
    "#         numparams = len(params)\n",
    "#         input_vec_dim = 1024 + numparams\n",
    "#         linear_out_dims = 5\n",
    "#         # self.series_in_dim = series_input_dim\n",
    "#         # self.hidden_size = hidden_size\n",
    "#         # self.num_lstm_layers = num_lstm_layers\n",
    "#         # self.lstm_output_dim = lstm_output_dim\n",
    "#         super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "#         self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "#         self.lstm_linear = nn.Sequential(nn.Linear(hidden_size, lstm_output_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "#         self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "#             nn.Linear(input_vec_dim, 1024),\n",
    "#             nn.Linear(1024, 512),nn.ReLU(),\n",
    "#             nn.Linear(512, 256),nn.ReLU(),\n",
    "#             nn.Linear(256, 128),nn.ReLU(),\n",
    "#             nn.Linear(128, linear_out_dims)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, feature_vec, property_vec):\n",
    "#         input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "#         stress_ser, _ = self.stress_ser_predictor(input_vec) # MAKE SURE YOU KNOW IT KNOWS WHEN TO STOP... PROBABLY TAKEN CARE OF BY PADDED SET\n",
    "#         # stress_ser = self.lstm_linear(stress_ser)\n",
    "\n",
    "#         constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "\n",
    "#         return stress_ser, constit_eqn_coeffs\n",
    "\n",
    "\n",
    "# # return stress_series, featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_loss_nostaticmethods(nn.Module):\n",
    "\n",
    "    def __init__(self, coeff_max_min_dict = max_min_dict, offset = -0.01):\n",
    "        super(PINN_loss_nostaticmethods, self).__init__()\n",
    "\n",
    "        self.offset = offset\n",
    "        self.coeff_max_min_dict = coeff_max_min_dict\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, stress_series_predicted, constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, \n",
    "                 W_data, sig_pl_data, strain): # , , stress_max_min_array, <<-- don't think I need these because I'm calculating the loss as scaled\n",
    "        \n",
    "        stress_series_eqn_calculated_scaled_output = self.constitutive_equation(constitutive_equation_coefficients_predicted, strain)\n",
    "        # print(stress_series_eqn_calculated_scaled[0].shape)\n",
    "        stress_series_eqn_calculated_scaled = stress_series_eqn_calculated_scaled_output[0].T\n",
    "\n",
    "        predicted_sig_pl = self.eqn_predict_sig_pl(stress_series_eqn_calculated_scaled)\n",
    "\n",
    "        W_pred = self.eqn_predicted_W(stress_series_predicted, strain)\n",
    "        print(f\"W_pred: {W_pred.detach().cpu().numpy()}\\tSig_pl: {predicted_sig_pl.detach().cpu().numpy()}\")\n",
    "        \n",
    "        W_loss = nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "        sig_pl_loss = nn.L1Loss()(predicted_sig_pl, sig_pl_data)\n",
    "        \n",
    "\n",
    "        constit_equation_coeff_loss = nn.L1Loss()(constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data) #, self.offset, self.coeff_max_min_dict\n",
    "\n",
    "        stress_series_predicted = torch.squeeze(stress_series_predicted, dim=-1)\n",
    "        physics_loss = nn.L1Loss()(stress_series_predicted, stress_series_eqn_calculated_scaled)\n",
    "\n",
    "        total_loss = W_loss + sig_pl_loss + constit_equation_coeff_loss + physics_loss\n",
    "\n",
    "        return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "# return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, \n",
    "# stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "\n",
    "\n",
    "    def unscale_coeffs(self, coeffs, coeff_max_min_dict, coeff_list =['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]):\n",
    "        unscaled_coeffs = torch.zeros_like(coeffs)\n",
    "\n",
    "        for i in range(coeffs.shape[0]):\n",
    "            for j, name in enumerate(coeff_list):\n",
    "                max = coeff_max_min_dict[name][0]\n",
    "                min = coeff_max_min_dict[name][1]\n",
    "\n",
    "                unscaled_coeffs[i][j] = (coeffs[i][j] * (max - min)) + min\n",
    "\n",
    "        return unscaled_coeffs\n",
    "    \n",
    "    def unscale_dyn_params(array, coeff_max_min_dict, dyn_param = 'sig_pl'):\n",
    "        max = coeff_max_min_dict[dyn_param][0]\n",
    "        min = coeff_max_min_dict[dyn_param][1]\n",
    "\n",
    "        array = array*(max - min) + min\n",
    "\n",
    "        return array\n",
    "    \n",
    "    def constitutive_equation(self, coeffs_pred, strain):\n",
    "\n",
    "        coeffs_pred_unscaled = self.unscale_coeffs(coeffs_pred, self.coeff_max_min_dict)\n",
    "        A_pred = coeffs_pred_unscaled[:, 0]\n",
    "        B_pred = coeffs_pred_unscaled[:, 1]\n",
    "        C_pred = coeffs_pred_unscaled[:, 2]\n",
    "        m_pred = coeffs_pred_unscaled[:, 3]\n",
    "        n_pred = coeffs_pred_unscaled[:, 4]\n",
    "\n",
    "        mask = torch.ge(strain.T, self.offset*-1)\n",
    "        strain = strain.T + self.offset\n",
    "        strain = torch.where(mask, strain, torch.tensor(0))\n",
    "\n",
    "\n",
    "        # stress_series_eqn_calculated = A_pred * (strain.T + self.offset)**m_pred + B_pred*((strain.T + self.offset)/(C_pred-(strain.T + self.offset)))**n_pred\n",
    "        stress_series_eqn_calculated = A_pred * (strain + self.offset)**m_pred + B_pred*((strain + self.offset)/(C_pred-(strain + self.offset)))**n_pred\n",
    "\n",
    "        stress_series_eqn_calculated = torch.nan_to_num(stress_series_eqn_calculated, nan = 0.0)\n",
    "\n",
    "\n",
    "        max = torch.max(stress_series_eqn_calculated, dim=0)[0]\n",
    "\n",
    "        min = torch.min(stress_series_eqn_calculated, dim=0)[0]\n",
    "\n",
    "        stress_ser_range = max - min\n",
    "\n",
    "        # stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(1)) / stress_ser_range.unsqueeze(1)\n",
    "        stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(0)) / stress_ser_range.unsqueeze(0)\n",
    "\n",
    "\n",
    "        return stress_ser_eqn_calculated_scaled, coeffs_pred_unscaled, coeffs_pred, max, min, stress_ser_range, stress_series_eqn_calculated, strain\n",
    "        \n",
    "        # return stress_series_eqn_calculated\n",
    "    \n",
    "    def eqn_predict_sig_pl(self, stress_series_eqn_calculated): # CALLED\n",
    "         \n",
    "        sig_pl_pred = torch.mean(stress_series_eqn_calculated[:, 200:400], dim=1)\n",
    "\n",
    "        return sig_pl_pred\n",
    "    \n",
    "    def eqn_predicted_W(self, stress_pred, strain): # CALLED\n",
    "        stress_start_idx = int(-1*self.offset*1e3)\n",
    "        stress_pred = torch.squeeze(stress_pred, dim=-1)\n",
    "        # print(strain.shape, stress_pred.shape)\n",
    "        W_pred = torch.trapz(stress_pred[:, stress_start_idx:], strain[:, stress_start_idx:], dim=1)\n",
    "        # print(f\"W_pred shape: {W_pred.shape}\")\n",
    "        return W_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, stress_series_predicted, constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, \n",
    "                 W_data, sig_pl_data, strain): # , , stress_max_min_array, <<-- don't think I need these because I'm calculating the loss as scaled\n",
    "        \n",
    "        stress_series_eqn_calculated_scaled = PINN_loss.constitutive_equation(constitutive_equation_coefficients_predicted, strain)\n",
    "        # print(stress_series_eqn_calculated_scaled[0].shape)\n",
    "        stress_series_eqn_calculated_scaled = stress_series_eqn_calculated_scaled[0].T\n",
    "\n",
    "        predicted_sig_pl = PINN_loss.eqn_predict_sig_pl(stress_series_eqn_calculated_scaled)\n",
    "\n",
    "        W_pred = PINN_loss.eqn_predicted_W(stress_series_predicted, strain)\n",
    "        print(f\"W_pred: {W_pred.detach().cpu().numpy()}\\tSig_pl: {predicted_sig_pl.detach().cpu().numpy()}\")\n",
    "        W_loss = nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "        sig_pl_loss = nn.L1Loss()(predicted_sig_pl, sig_pl_data)\n",
    "\n",
    "        constit_equation_coeff_loss = nn.L1Loss()(constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data) #, PINN_loss.offset, PINN_loss.coeff_max_min_dict\n",
    "\n",
    "        stress_series_predicted = torch.squeeze(stress_series_predicted, dim=-1)\n",
    "        physics_loss = nn.L1Loss()(stress_series_predicted, stress_series_eqn_calculated_scaled)\n",
    "\n",
    "        total_loss = W_loss + sig_pl_loss + constit_equation_coeff_loss + physics_loss\n",
    "\n",
    "        return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PINN_loss_staticmethods(nn.Module):\n",
    "\n",
    "#     def __init__(self, coeff_max_min_dict = max_min_dict, offset = -0.01):\n",
    "#         super(PINN_loss_staticmethods, self).__init__()\n",
    "\n",
    "#         self.offset = offset\n",
    "#         self.coeff_max_min_dict = coeff_max_min_dict\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, stress_series_predicted, constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data, \n",
    "#                  W_data, sig_pl_data, strain): # , , stress_max_min_array, <<-- don't think I need these because I'm calculating the loss as scaled\n",
    "        \n",
    "#         stress_series_eqn_calculated_scaled = PINN_loss_staticmethods.constitutive_equation(constitutive_equation_coefficients_predicted, strain)\n",
    "#         # print(stress_series_eqn_calculated_scaled[0].shape)\n",
    "#         stress_series_eqn_calculated_scaled = stress_series_eqn_calculated_scaled[0].T\n",
    "\n",
    "#         predicted_sig_pl = PINN_loss_staticmethods.eqn_predict_sig_pl(stress_series_eqn_calculated_scaled)\n",
    "\n",
    "#         W_pred = PINN_loss_staticmethods.eqn_predicted_W(stress_series_predicted, strain)\n",
    "#         print(f\"W_pred: {W_pred.detach().cpu().numpy()}\\tSig_pl: {predicted_sig_pl.detach().cpu().numpy()}\")\n",
    "#         W_loss = nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "#         sig_pl_loss = nn.L1Loss()(predicted_sig_pl, sig_pl_data)\n",
    "\n",
    "#         constit_equation_coeff_loss = nn.L1Loss()(constitutive_equation_coefficients_predicted, constitutive_equation_coefficients_data) #, PINN_loss.offset, PINN_loss.coeff_max_min_dict\n",
    "\n",
    "#         stress_series_predicted = torch.squeeze(stress_series_predicted, dim=-1)\n",
    "#         physics_loss = nn.L1Loss()(stress_series_predicted, stress_series_eqn_calculated_scaled)\n",
    "\n",
    "#         total_loss = W_loss + sig_pl_loss + constit_equation_coeff_loss + physics_loss\n",
    "\n",
    "#         return total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss\n",
    "\n",
    "\n",
    "#     @staticmethod\n",
    "#     def unscale_coeffs(coeffs, coeff_max_min_dict, coeff_list =['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]):\n",
    "#         unscaled_coeffs = torch.zeros_like(coeffs)\n",
    "\n",
    "#         for i in range(coeffs.shape[0]):\n",
    "#             for j, name in enumerate(coeff_list):\n",
    "#                 max = coeff_max_min_dict[name][0]\n",
    "#                 min = coeff_max_min_dict[name][1]\n",
    "\n",
    "#                 unscaled_coeffs[i][j] = (coeffs[i][j] * (max - min)) + min\n",
    "\n",
    "#         return unscaled_coeffs\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def unscale_dyn_params(array, coeff_max_min_dict, dyn_param = 'sig_pl'):\n",
    "#         max = coeff_max_min_dict[dyn_param][0]\n",
    "#         min = coeff_max_min_dict[dyn_param][1]\n",
    "\n",
    "#         array = array*(max - min) + min\n",
    "\n",
    "#         return array\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def constitutive_equation(self, coeffs_pred, strain):\n",
    "\n",
    "#         coeffs_pred_unscaled = PINN_loss_staticmethods.unscale_coeffs(coeffs_pred, self.coeff_max_min_dict)\n",
    "#         A_pred = coeffs_pred_unscaled[:, 0]\n",
    "#         B_pred = coeffs_pred_unscaled[:, 1]\n",
    "#         C_pred = coeffs_pred_unscaled[:, 2]\n",
    "#         m_pred = coeffs_pred_unscaled[:, 3]\n",
    "#         n_pred = coeffs_pred_unscaled[:, 4]\n",
    "\n",
    "#         mask = torch.ge(strain.T, PINN_loss_staticmethods.offset*-1)\n",
    "#         strain = strain.T + PINN_loss_staticmethods.offset\n",
    "#         strain = torch.where(mask, strain, torch.tensor(0))\n",
    "\n",
    "\n",
    "#         # stress_series_eqn_calculated = A_pred * (strain.T + self.offset)**m_pred + B_pred*((strain.T + self.offset)/(C_pred-(strain.T + self.offset)))**n_pred\n",
    "#         stress_series_eqn_calculated = A_pred * (strain + PINN_loss_staticmethods.offset)**m_pred + B_pred*((strain + PINN_loss_staticmethods.offset)/(C_pred-(strain + PINN_loss_staticmethods.offset)))**n_pred\n",
    "\n",
    "        \n",
    "#         max = torch.max(stress_series_eqn_calculated, dim=0)[0]\n",
    "#         min = torch.min(stress_series_eqn_calculated, dim=0)[0]\n",
    "\n",
    "#         stress_ser_range = max - min\n",
    "\n",
    "#         # stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(1)) / stress_ser_range.unsqueeze(1)\n",
    "#         stress_ser_eqn_calculated_scaled = (stress_series_eqn_calculated - min.unsqueeze(0)) / stress_ser_range.unsqueeze(0)\n",
    "\n",
    "\n",
    "#         return stress_ser_eqn_calculated_scaled, coeffs_pred_unscaled, coeffs_pred, max, min, stress_ser_range, stress_series_eqn_calculated, strain\n",
    "        \n",
    "#         # return stress_series_eqn_calculated\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def eqn_predict_sig_pl(self, stress_series_eqn_calculated): # CALLED\n",
    "         \n",
    "#         sig_pl_pred = torch.mean(stress_series_eqn_calculated[:, 200:400], dim=1)\n",
    "\n",
    "#         return sig_pl_pred\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def eqn_predicted_W(self, stress_pred, strain): # CALLED\n",
    "#         stress_start_idx = int(-1*self.offset*1e3)\n",
    "#         stress_pred = torch.squeeze(stress_pred, dim=-1)\n",
    "#         # print(strain.shape, stress_pred.shape)\n",
    "#         W_pred = torch.trapz(stress_pred[:, stress_start_idx:], strain[:, stress_start_idx:], dim=1)\n",
    "#         # print(f\"W_pred shape: {W_pred.shape}\")\n",
    "#         return W_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, hidden_size=256, num_lstm_layers=4, lstm_output_dim=1):\n",
    "        numparams = len(params)\n",
    "        input_vec_dim = 1024 + numparams\n",
    "        linear_out_dims = 5\n",
    "        # self.series_in_dim = series_input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_output_dim = lstm_output_dim\n",
    "        super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "        self.lstm_linear = nn.Sequential(nn.Linear(hidden_size, lstm_output_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "        self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "            nn.Linear(input_vec_dim, 1024),\n",
    "            nn.Linear(1024, 512),nn.ReLU(),\n",
    "            nn.Linear(512, 256),nn.ReLU(),\n",
    "            nn.Linear(256, 128),nn.ReLU(),\n",
    "            nn.Linear(128, linear_out_dims)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, feature_vec, property_vec, strain_series):\n",
    "        input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "        batch_size = strain_series.size(0)\n",
    "        h = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        c = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        \n",
    "        stress_ser = []\n",
    "\n",
    "        for i in range(strain_series.size(1)):\n",
    "            stress, (h, c) = self.stress_ser_predictor(input_vec, (h, c))\n",
    "            stress = self.lstm_linear(stress)\n",
    "            stress_ser.append(stress)\n",
    "       \n",
    "        stress_ser = torch.stack(stress_ser, dim=1)\n",
    "\n",
    "        constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "        print(f'constit_eqn_coeffs predicted\\n{constit_eqn_coeffs.detach().cpu().numpy()}')\n",
    "\n",
    "        return stress_ser, constit_eqn_coeffs\n",
    "\n",
    "\n",
    "# return stress_series, featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor()\n",
    "# torch.tensor(np.array([[0,0,0], [0,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = Dynamic_Stress_PINN(params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(pinn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns-->         featvec, paramvec, constit_eqn_coeffs, W, sig_pl, || strain,            || stress_series,\n",
    "# dataloader returns:->     ^--------------nonseries_list------------------^  || padded_strain_ser, || padded_stress_ser\n",
    "\n",
    "# dataset    returns:   featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain,               stress_series_dic\n",
    "# dataloader returns:   nonseries_list,                                   padded_strain_ser,    padded_stress_ser, max_mins\n",
    "\n",
    "optimizer = optim.SGD(pinn.parameters(), lr=0.000001)#optim.Adam(pinn.parameters(), lr=0.00001)\n",
    "def pinn_train(pinn, dataloader, loss_func=PINN_loss_nostaticmethods, optimizer=optimizer): #PINN_loss\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    # running_loss = 0.0\n",
    "    total_run_loss = 0.0\n",
    "    dyn_param_run_loss = 0.0\n",
    "    coeff_run_loss = 0.0\n",
    "    phys_run_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    for non_series_data, strain_series, stress_series, max_mins in pbar:\n",
    "        global feature_vec, param_vec, strs_ser_glob, coeffs_glob, strn_ser_glob\n",
    "        feature_vec = non_series_data[0].cuda()\n",
    "        param_vec   = non_series_data[1].cuda()\n",
    "        strs_ser_glob = stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "        coeffs_glob = const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "        W = non_series_data[3].cuda()\n",
    "        # print(f\"W shape: {W.shape}\")\n",
    "        sig_pl = non_series_data[4].cuda()\n",
    "        strain_series = strn_ser_glob = strain_series.cuda()\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        global stress_series_pred\n",
    "        global constit_eqn_coeffs_pred\n",
    "        stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "        # print(stress_series_pred.shape, constit_eqn_coeffs_pred.shape)\n",
    "\n",
    "        # loss_val, W_loss, sig_pl_loss, const_eqn_coeff_loss, phys_loss, *args = loss_func()(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "                                                                    # PINN_loss(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs_data, W_data, sig_pl_data, strain, offset=-0.01):\n",
    "        #total_loss, W_loss, sig_pl_loss, constit_equation_coeff_loss, physics_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "        \n",
    "        global loss_val, W_loss, sig_pl_loss, const_eqn_coeff_loss, phys_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output\n",
    "        loss_val, W_loss, sig_pl_loss, const_eqn_coeff_loss, phys_loss, stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output = loss_func()(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "\n",
    "        dyn_param_loss = W_loss + sig_pl_loss\n",
    "        loss_val.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        # running_loss += loss_val.item()\n",
    "        total_run_loss += loss_val.item()\n",
    "        dyn_param_run_loss += dyn_param_loss.item()\n",
    "        coeff_run_loss += const_eqn_coeff_loss.item()\n",
    "        phys_run_loss += phys_loss.item()\n",
    "        # pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "        pbar.set_description(f'Losses:\\tTotal: {total_run_loss / (pbar.n + 1):.4f}\\tDyn Params: {dyn_param_run_loss / (pbar.n + 1):.4f}\\tCoeff: {coeff_run_loss / (pbar.n + 1):.4f}\\tPhysics: {phys_run_loss / (pbar.n + 1):.4f}\\t')\n",
    "        # print(\"Data index is: f{idx}\")\n",
    "    return total_run_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_validate(pinn, dataloader, loss_func=PINN_loss_nostaticmethods):\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    with torch.no_grad():\n",
    "        for non_series_data, strain_series, stress_series, max_mins in pbar:\n",
    "            feature_vec = non_series_data[0].cuda()\n",
    "            param_vec   = non_series_data[1].cuda()\n",
    "            stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "            const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "            W = non_series_data[3].cuda()\n",
    "            sig_pl = non_series_data[4].cuda()\n",
    "            strain_series = strain_series.cuda()\n",
    "\n",
    "            stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "\n",
    "            loss_val, dyn_param_loss, const_eqn_coeff_loss, phys_loss = loss_func()(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "            # return total_loss, loss_data_1, loss_data_2, loss_physics\n",
    "            \n",
    "            running_loss += loss_val.item()\n",
    "            pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}\\t ')\n",
    "        return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training hyper(?)parameters\n",
    "\n",
    "EPOCHS = 125\n",
    "\n",
    "lossfunc = torch.nn.L1Loss() # this is MAE loss\n",
    "lossfunc_name = 'MAE'\n",
    "optimizer = optim.Adam(pinn.parameters(), lr=0.001)\n",
    "\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "cp_name = f'CP_{fname_base}.pth'\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)\n",
    "print(best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 75\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "# earlystop_min_delta = 0.000075\n",
    "earlystop_min_delta = 0.00075 # For L1Loss (MAE)\n",
    "\n",
    "# os.makedirs(best_weights_path, exist_ok=True)\n",
    "best_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs_completed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean(strs_ser_glob[:,200:400], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Losses:\tTotal: nan\tDyn Params: nan\tCoeff: nan\tPhysics: nan\t:  20%|        | 8/40 [00:13<00:54,  1.71s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/PINN_training/model_CPs/CP_Dyn_PINN_v0_25APR24.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpinn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     train_loss_val \u001b[38;5;241m=\u001b[39m train_loss[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[39], line 43\u001b[0m, in \u001b[0;36mpinn_train\u001b[0;34m(pinn, dataloader, loss_func, optimizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m dyn_param_loss \u001b[38;5;241m=\u001b[39m W_loss \u001b[38;5;241m+\u001b[39m sig_pl_loss\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     hist_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlossfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: train_losses, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlossfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: val_losses}\n\u001b[0;32m---> 65\u001b[0m     pinn\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_weights_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/PINN_training/model_CPs/CP_Dyn_PINN_v0_25APR24.pth'"
     ]
    }
   ],
   "source": [
    "\n",
    "# return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n",
    "lossfunc = PINN_loss_nostaticmethods\n",
    "try:\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train the model\n",
    "        train_loss = pinn_train(pinn, trloader, lossfunc, optimizer)\n",
    "        train_loss_val = train_loss[0]\n",
    "        train_dyn_param_loss = train_loss[1]\n",
    "        train_const_eqn_coeff_loss = train_loss[2]\n",
    "        train_phys_loss = train_loss[3]\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = pinn_validate(pinn, valloader, lossfunc)\n",
    "        val_loss_val = val_loss[0]\n",
    "        val_dyn_param_loss = val_loss[1]\n",
    "        val_const_eqn_coeff_loss = val_loss[2]\n",
    "        val_phys_loss = val_loss[3]\n",
    "\n",
    "        print(f'training:\\ttotal loss: {train_loss_val:.5f}, dynamic param loss: {train_dyn_param_loss:.5f}\\nconstitutive equation coefficient loss: {train_const_eqn_coeff_loss:.5f}, physics_loss: {train_phys_loss:.5f}')\n",
    "        print(f'validation:\\ttotal_loss:{val_loss_val:.5f}, dynamic param loss: {val_dyn_param_loss:.5f}\\nconstitutive equation coefficient loss: {val_const_eqn_coeff_loss:.5f}, physics_loss: {val_phys_loss:.5f}')\n",
    "\n",
    "\n",
    "        # Save the model's weights if validation loss is improved\n",
    "        improvement_delta = best_val_loss - val_loss_val\n",
    "\n",
    "        if val_loss_val < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(pinn.state_dict(), best_weights_path)  # Save model weights to file\n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "\n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "\n",
    "\n",
    "        # Collect model training history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "            pinn.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss_val:.5f}\\tValidation Loss: {val_loss_val:.5f}')\n",
    "\n",
    "        epochs_completed +=1\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    pinn.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "    pinn.load_state_dict(torch.load(best_weights_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global stress_series_eqn_calculated_scaled, predicted_sig_pl, W_pred, stress_series_predicted, stress_series_eqn_calculated_scaled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled_output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_scaled_output[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(stress_series_eqn_calculated_scaled_output[-1], dim=0)[0] - torch.min(stress_series_eqn_calculated_scaled_output[-1], dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(stress_series_eqn_calculated_scaled_output[-1], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_test = stress_series_eqn_calculated_scaled_output[-2][:,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_series_test = stress_series_eqn_calculated_scaled_output[-1][:,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x = strain_series_test, y = stress_series_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_test = stress_series_eqn_calculated_scaled_output[-2][:,1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_series_test = stress_series_eqn_calculated_scaled_output[-1][:,1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x = strain_series_test, y = stress_series_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sig_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(stress_series_eqn_calculated_scaled[:, 200:400], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)[-1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_eqn_coeffs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strn_ser_glob[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp = lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)[0]\n",
    "\n",
    "# for i in range(0, samp.shape[0], 8):\n",
    "#     print(samp[i,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(batch[0][i].shape)\n",
    "print(batch[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_sel = batch[0][2].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __TROUBLESHOOTING Stress series prediction and whatnot__\n",
    "## Starting with unscaling the coeffs, then\n",
    "## masking the strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = []\n",
    "\n",
    "maxmin_keys = list(max_min_dict.keys())\n",
    "for i in range(2):\n",
    "    co = coeffs_sel[i, :]\n",
    "    coeffunscl = []\n",
    "    for j in range(5):\n",
    "        max = max_min_dict[maxmin_keys[j]][0]\n",
    "        min = max_min_dict[maxmin_keys[j]][1]\n",
    "\n",
    "        unscaled = co[j] * (max - min) + max\n",
    "\n",
    "        coeffunscl.append(unscaled)\n",
    "    coeffs.append(coeffunscl)\n",
    "coeffs = np.asarray(coeffs)\n",
    "\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = -0.01\n",
    "mask = torch.ge(strain, offset*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strainoff = strain + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strainoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strainoff_mask = torch.where(mask, strainoff, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strainoff_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invec = torch.cat([batch[0][0], batch[0][1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constcoeff_pred1 = pinn.constit_eqn_coeff_predictor(invec.cuda())\n",
    "print(constcoeff_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_loss_staticmethods.unscale_coeffs(coeffs = constcoeff_pred1, coeff_max_min_dict=max_min_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_static.coeff_max_min_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0][1,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss2 = PINN_loss_nostaticmethods()(out[0].cpu(), out[1].cpu(), batch[0][2], batch[0][3], batch[0][4], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss = PINN_loss_nostaticmethods()(out[0].cpu(), out[1].cpu(), batch[0][2], batch[0][3], batch[0][4], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss[-1]#[-2][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(testloss[-1][6][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss[-1][6][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testloss[-1][5],'\\n')\n",
    "print(torch.min(testloss[-1][6], dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss_static = PINN_loss_staticmethods(coeff_max_min_dict = max_min_dict)\n",
    "# loss_static(out[0].cpu(), out[1].cpu(), batch[0][2], batch[0][3], batch[0][4], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constcoeff_pred1_unscl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constiteqn_ser = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in batch[0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constcoeff_data1 = batch[0][2]\n",
    "print(constcoeff_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyndb['A_opt scaled'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.L1Loss()(constcoeff_pred1, constcoeff_data1.cuda()).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pinn(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pinn(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser[0,:].detach().cpu().numpy().T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc().constitutive_equation(coeffs_glob, strn_ser_glob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_ser_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_pred = coeffs_glob\n",
    "strain = strn_ser_glob\n",
    "offset = -0.01\n",
    "\n",
    "print(coeffs_pred.shape, strain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain.T[0]+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(strain.T[0]+offset)**m_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(strain.T + offset)**m_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ge(strain.T, offset*-1)\n",
    "result = strain.T + offset\n",
    "result = torch.where(mask, result, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale_coeffs(coeffs, coeff_max_min_dict, coeff_list =['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]):\n",
    "    unscaled_coeffs = torch.zeros_like(coeffs)\n",
    "\n",
    "    for i in range(coeffs.shape[0]):\n",
    "        for j, name in enumerate(coeff_list):\n",
    "            max = coeff_max_min_dict[name][0]\n",
    "            min = coeff_max_min_dict[name][1]\n",
    "\n",
    "            unscaled_coeffs[i][j] = (coeffs[i][j] * (max - min)) + min\n",
    "\n",
    "    return unscaled_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_coeffs = unscale_coeffs(coeffs_glob, max_min_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A_pred = unscaled_coeffs[:, 0]\n",
    "B_pred = unscaled_coeffs[:, 1]\n",
    "C_pred = unscaled_coeffs[:, 2]\n",
    "m_pred = unscaled_coeffs[:, 3]\n",
    "n_pred = unscaled_coeffs[:, 4]\n",
    "print(A_pred, A_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stress_series_eqn_calculated_offset = A_pred * (strain.T + offset)**m_pred + B_pred*((strain.T + offset)/(C_pred-(strain.T + offset)))**n_pred\n",
    "\n",
    "stress_series_eqn_calculated = A_pred * (strain.T )**m_pred + B_pred*((strain.T )/(C_pred-(strain.T )))**n_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain.T**m_pred * A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "A_pred * (num + offset)**m_pred + B_pred*((num + offset)/(C_pred-(num + offset)))**n_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_eqn_calculated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(stress_series_eqn_calculated, dim=0)[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.constit_eqn_coeff_predictor[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_const_eqn_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strspred_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(strs_start_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strspred_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strn_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(strspred_glob[:, 10:], strn_glob[:, 10:], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_glob = coeffs_glob[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A_glob*strs_ser_glob.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_ser_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=PINN_loss\n",
    "optimizer=optim.Adam(pinn.parameters())\n",
    "pinn.train()  # Set the model to training mode\n",
    "running_loss = 0.0\n",
    "batch = next(iter(trloader)) # Use tqdm for progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_series_data = batch[0]\n",
    "strain_series = batch[1]\n",
    "stress_series = batch[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# global feature_vec, param_vec, strs_ser_glob, coeffs_glob\n",
    "feature_vec = non_series_data[0].cuda()\n",
    "param_vec   = non_series_data[1].cuda()\n",
    "strs_ser_glob = stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "coeffs_glob = const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "W = non_series_data[3].cuda()\n",
    "sig_pl = non_series_data[4].cuda()\n",
    "strain = strain_series.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_val, dyn_param_loss, const_eqn_coeff_loss, phys_loss = loss_func(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs,)# W, sig_pl, strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_ser_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_eqn_coeffs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(200)\n",
    "t2 = torch.randn(100)\n",
    "\n",
    "t3 = torch.cat([t1, t2], dim=0)\n",
    "print(t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "\n",
    "histno=1\n",
    "histpath = os.path.join(nbpath,'model_jsons',f'{cp_name[:-4]}_training_history{histno}_{epochs_completed}ep.json')\n",
    "\n",
    "pinn.eval()\n",
    "\n",
    "with open(histpath, 'w') as f:\n",
    "    json.dump(hist_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
