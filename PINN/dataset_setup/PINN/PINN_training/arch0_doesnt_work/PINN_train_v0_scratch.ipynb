{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import workflow_utils_v3\n",
    "import sys\n",
    "\n",
    "from workflow_utils_v3.FileDirectory import Directory\n",
    "\n",
    "dirs = Directory(rootpath = '/home/mgolub4/DLproj/MLTO_2024/')\n",
    "\n",
    "# Sets directory of entire package\n",
    "# rootpath = '/data/tigusa1/MLTO_UCAH/MLTO_2023/'\n",
    "\n",
    "nbpath = os.path.join(dirs._3_Dynamic_PINN_RNN, 'PINN_training')\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle\n",
    "from plotly.colors import sequential, qualitative\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "# import torchsummary\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split as TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '25APR24'\n",
    "fname_base = f'Dyn_PINN_v0_{date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready.csv'\n",
    "dyndb_path = '/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/dyn_stat_database_PINN_ready_coeffs_scaled.csv'\n",
    "dyndb = pd.read_csv(dyndb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTr, idxRem = TTS(dyndb, stratify = dyndb['topology_family'], random_state=42, train_size = 0.8)\n",
    "idxVal, idxTe = TTS(idxRem, random_state = 42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['volFrac', \n",
    "        'CH_11 scaled', 'CH_22 scaled', 'CH_33 scaled', 'CH_44 scaled', 'CH_55 scaled', 'CH_66 scaled',\n",
    "        'CH_12 scaled', 'CH_13 scaled','CH_23 scaled',\n",
    "        'EH_11 scaled', 'EH_22 scaled', 'EH_33 scaled',\n",
    "        'GH_23 scaled', 'GH_13 scaled', 'GH_12 scaled', \n",
    "        'vH_12 scaled', 'vH_13 scaled', 'vH_23 scaled', 'vH_21 scaled', 'vH_31 scaled','vH_32 scaled',\n",
    "        'KH_11 scaled', 'KH_22 scaled', 'KH_33 scaled', \n",
    "        'kappaH_11 scaled', 'kappaH_22 scaled', 'kappaH_33 scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(array):\n",
    "    max = array.max()\n",
    "    min = array.min()\n",
    "\n",
    "    array = (array - min) / (max - min)\n",
    "\n",
    "    return array, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stress_Series:\n",
    "    def __init__(self, series):\n",
    "        self.series = series\n",
    "        self.max = series.max()\n",
    "        self.min = series.min()\n",
    "\n",
    "    def scale(self):\n",
    "        return (self.series - self.min) / (self.max - self.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Dataset(Dataset):\n",
    "    def __init__(self, params, split_dataframe,\n",
    "                 feat_vec_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/voxel_embedding_feature_maps', \n",
    "                 stress_series_directory='/home/mgolub4/DLproj/MLTO_2024/3_Dynamic_PINN_RNN/dyn_data/stress_series_data', \n",
    "                 stress_ser_suffix = '_proct_gaus_btrlp_fftlp',\n",
    "                 scale_coeffs_by=[1e9, 1e11, 1, 1, 1],\n",
    "                 predicted_parameters=True,\n",
    "                  ):\n",
    "        self.df = split_dataframe\n",
    "        self.featvec_dir = feat_vec_directory # for pulling the feature vectors\n",
    "        self.stress_ser_dir = stress_series_directory # for pulling the time series files\n",
    "        self.params = params\n",
    "        self.predicted_parameters = predicted_parameters\n",
    "        # self.const_eqn_params = ['A_opt', 'B_opt', 'C_opt', 'm_opt', 'n_opt',]\n",
    "        self.const_eqn_params = ['A_opt scaled', 'B_opt scaled', 'C_opt scaled', 'm_opt scaled', 'n_opt scaled',]\n",
    "\n",
    "        self.stress_ser_suffix = stress_ser_suffix\n",
    "        self.scale_coeffs_by = scale_coeffs_by\n",
    "\n",
    "        # self.scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dyn_series_fname = self.df['dyn_file_name_original'].iloc[idx]\n",
    "\n",
    "        sig_pl = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['plateau_stress_g scaled'].values[0].astype(np.float32)\n",
    "        W = self.df[self.df['dyn_file_name_original'] == dyn_series_fname]['energy_absorbed_g scaled'].values[0].astype(np.float32)\n",
    "\n",
    "        # feature vector from convolutional neural network convolutional layers output\n",
    "        featvec_fname = self.df['conv_feat_vec'].iloc[idx] + '.npy'\n",
    "        featvec_path = os.path.join(self.featvec_dir, featvec_fname)\n",
    "        featvec = np.load(featvec_path)\n",
    "        featvec = np.squeeze(featvec, axis=0).astype(np.float32)\n",
    "\n",
    "        # constitutive equation parameters\n",
    "        constit_eqn_coeffs = np.asarray(self.df[self.const_eqn_params].iloc[idx])\n",
    "\n",
    "        for i, scaler in enumerate(self.scale_coeffs_by):\n",
    "            constit_eqn_coeffs[i] = constit_eqn_coeffs[i] / scaler\n",
    "            \n",
    "\n",
    "\n",
    "        # predicted parameters\n",
    "        if self.predicted_parameters:\n",
    "            paramvec = np.asarray([self.df[f'pred {par}'].iloc[idx] for par in self.params]).astype(np.float32)\n",
    "        else:\n",
    "            paramvec = np.asarray([self.df[f'{par}'].iloc[idx] for par in self.params]).astype(np.float32)\n",
    "\n",
    "        # stress_series -- for now (April 24), Imma use the truncated datasets, because I think padded batches for RNNs in pytorch will take care of differing lengths\n",
    "        stress_ser_fname = dyn_series_fname + self.stress_ser_suffix\n",
    "        stress_ser_path = os.path.join(self.stress_ser_dir, stress_ser_fname+'.csv')\n",
    "        stress_series = np.asarray(pd.read_csv(stress_ser_path)['stress_bottom_gsreg']).astype(np.float32)\n",
    "        \n",
    "        # stress_series = Stress_Series(stress_series)\n",
    "        # scaled_stress_series = stress_series.scale()\n",
    "        # stress_series_dic = {'stress_series': scaled_stress_series, 'max': stress_series.max, 'min': stress_series.min}\n",
    "\n",
    "        strain = np.asarray(pd.read_csv(stress_ser_path)['Strain']).astype(np.float32)\n",
    "                   \n",
    "\n",
    "        return featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain, stress_series #stress_series_dic\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - featvec        - paramvec\n",
    "        targets:\n",
    "        - stress_series        - constit_eqn_coeffs        - W        - sig_pl        other(?):        - strain --> needed for later\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PINN_loss(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs_data, W_data, sig_pl_data, strain):\n",
    "    global strspred_glob, strn_glob, strs_start_glob, strs_const_eqn_glob\n",
    "    strn_glob = strain\n",
    "    offset=-0.01\n",
    "    \n",
    "    A_pred = constit_eqn_coeffs_pred[:, 0]\n",
    "    B_pred = constit_eqn_coeffs_pred[:, 1]\n",
    "    C_pred = constit_eqn_coeffs_pred[:, 2]\n",
    "    m_pred = constit_eqn_coeffs_pred[:, 3]\n",
    "    n_pred = constit_eqn_coeffs_pred[:, 4]\n",
    "\n",
    "    if len(stress_series_pred.shape) == 3:\n",
    "        strspred_glob = stress_pred = torch.squeeze(stress_series_pred, axis=2)\n",
    "    else:\n",
    "        strspred_glob = stress_pred = stress_series_pred\n",
    "\n",
    "    \n",
    "\n",
    "    stress_series_constit_eqn = A_pred * (strain.T + offset)**m_pred + B_pred*((strain.T + offset)/(C_pred-(strain.T + offset)))**n_pred\n",
    "\n",
    "    strs_const_eqn_glob = stress_series_constit_eqn = stress_series_constit_eqn.T\n",
    "\n",
    "    sig_pl_pred = torch.mean(stress_series_constit_eqn[:,200:400], dim=1)\n",
    "    sig_pl_data = sig_pl_data\n",
    "    # print('sig_pl data, pred shape:')\n",
    "    # print(sig_pl_data.shape)\n",
    "    # print(sig_pl_pred.shape)\n",
    "    \n",
    "\n",
    "    strs_start_glob = stress_start = int(-1*offset*1e3) # ensures the calculation starts at the right point of the stress series\n",
    "    W_pred = torch.trapz(stress_pred[:, stress_start:], strain[:, stress_start:], dim=1)\n",
    "    W_data = W_data\n",
    "    # print('W data, pred shape:')\n",
    "    # print(W_data.shape)\n",
    "    # print(W_pred.shape)\n",
    "\n",
    "\n",
    "    loss_data_1 = nn.L1Loss()(sig_pl_pred, sig_pl_data) + nn.L1Loss()(W_pred, W_data)\n",
    "\n",
    "    loss_data_2 = nn.L1Loss()(constit_eqn_coeffs_pred, const_eqn_coeffs_data)\n",
    "\n",
    "    loss_physics = nn.L1Loss()(stress_pred, stress_series_constit_eqn)\n",
    "\n",
    "    total_loss = loss_data_1 + loss_data_2 + loss_physics\n",
    "\n",
    "    return total_loss, loss_data_1, loss_data_2, loss_physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return featvec, paramvec, stress_series, constit_eqn_coeffs, W, sig_pl, strain\n",
    "# return nonseries_list, padded_strain_ser, padded_stress_ser\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def padded_collate(batch):\n",
    "    stress_ser = [torch.tensor(item[-1], requires_grad=True) for item in batch]\n",
    "    strain_ser = [torch.tensor(item[-2], requires_grad=True) for item in batch]\n",
    "    nonserbatch = [item[0:-2] for item in batch] # all until last element (stress series) of list given by PINN_Dataset\n",
    "\n",
    "    nonseries_list = []\n",
    "\n",
    "    # nonser_len = len(nonserbatch)\n",
    "    for i in range(5):\n",
    "        data_list = []\n",
    "        for data in nonserbatch:\n",
    "            # print(type(data), len(data))\n",
    "            data_list.append(data[i])\n",
    "        data_list = torch.tensor(np.asarray(data_list), requires_grad=True)\n",
    "        nonseries_list.append(data_list)\n",
    "\n",
    "    \n",
    "    padded_stress_ser = pad_sequence(stress_ser, batch_first=True, padding_value=0)\n",
    "    padded_strain_ser = pad_sequence(strain_ser, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    return nonseries_list, padded_strain_ser, padded_stress_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "trdata = PINN_Dataset(params, idxTr)\n",
    "trloader = DataLoader(trdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "valdata = PINN_Dataset(params, idxVal)\n",
    "valloader = DataLoader(valdata, batch_size=batch_size, collate_fn = padded_collate, shuffle=True)\n",
    "\n",
    "tedata = PINN_Dataset(params, idxTe)\n",
    "teloader = DataLoader(tedata, batch_size=batch_size, collate_fn = padded_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(5):\n",
    "# #     print(next(iter(trloader))[0][i].shape)\n",
    "# feat = next(iter(trloader))[0][0]\n",
    "# pars = next(iter(trloader))[0][1]\n",
    "# # torch.cat([feat,pars], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print([i.shape for i in next(iter(trloader))[1]])\n",
    "# for part in next(iter(trloader)):\n",
    "#     print(type(part))\n",
    "#     if isinstance(part, list):\n",
    "#         print(len(part))\n",
    "#     if isinstance(part, torch.Tensor):\n",
    "#         print(part.shape)\n",
    "\n",
    "# for part in next(iter(trloader))[0]:\n",
    "#     print(part.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params, hidden_size=128, num_lstm_layers=4, lstm_output_dim=1):\n",
    "#         numparams = len(params)\n",
    "#         input_vec_dim = 1024 + numparams\n",
    "#         linear_out_dims = 5\n",
    "#         # self.series_in_dim = series_input_dim\n",
    "#         # self.hidden_size = hidden_size\n",
    "#         # self.num_lstm_layers = num_lstm_layers\n",
    "#         # self.lstm_output_dim = lstm_output_dim\n",
    "#         super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "#         self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "#         self.lstm_linear = nn.Sequential(nn.Linear(hidden_size, lstm_output_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "#         self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "#             nn.Linear(input_vec_dim, 1024),\n",
    "#             nn.Linear(1024, 512),nn.ReLU(),\n",
    "#             nn.Linear(512, 256),nn.ReLU(),\n",
    "#             nn.Linear(256, 128),nn.ReLU(),\n",
    "#             nn.Linear(128, linear_out_dims)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, feature_vec, property_vec):\n",
    "#         input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "#         stress_ser, _ = self.stress_ser_predictor(input_vec) # MAKE SURE YOU KNOW IT KNOWS WHEN TO STOP... PROBABLY TAKEN CARE OF BY PADDED SET\n",
    "#         # stress_ser = self.lstm_linear(stress_ser)\n",
    "\n",
    "#         constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "\n",
    "#         return stress_ser, constit_eqn_coeffs\n",
    "\n",
    "\n",
    "# # return stress_series, featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_Stress_PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, hidden_size=256, num_lstm_layers=4, lstm_output_dim=1):\n",
    "        numparams = len(params)\n",
    "        input_vec_dim = 1024 + numparams\n",
    "        linear_out_dims = 5\n",
    "        # self.series_in_dim = series_input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_output_dim = lstm_output_dim\n",
    "        super(Dynamic_Stress_PINN, self).__init__()\n",
    "\n",
    "        self.stress_ser_predictor = nn.LSTM(input_vec_dim, hidden_size, num_lstm_layers, batch_first=True)\n",
    "        self.lstm_linear = nn.Sequential(nn.Linear(hidden_size, lstm_output_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "        self.constit_eqn_coeff_predictor = nn.Sequential(\n",
    "            nn.Linear(input_vec_dim, 1024),\n",
    "            nn.Linear(1024, 512),nn.ReLU(),\n",
    "            nn.Linear(512, 256),nn.ReLU(),\n",
    "            nn.Linear(256, 128),nn.ReLU(),\n",
    "            nn.Linear(128, linear_out_dims)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, feature_vec, property_vec, strain_series):\n",
    "        input_vec = torch.cat([feature_vec, property_vec], dim=1)\n",
    "\n",
    "        batch_size = strain_series.size(0)\n",
    "        h = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        c = torch.randn(self.num_lstm_layers, self.hidden_size).to(strain_series.device)\n",
    "        \n",
    "        stress_ser = []\n",
    "\n",
    "        for i in range(strain_series.size(1)):\n",
    "            stress, (h, c) = self.stress_ser_predictor(input_vec, (h, c))\n",
    "            stress = self.lstm_linear(stress)\n",
    "            stress_ser.append(stress)\n",
    "       \n",
    "        stress_ser = torch.stack(stress_ser, dim=1)\n",
    "\n",
    "        constit_eqn_coeffs = self.constit_eqn_coeff_predictor(input_vec)\n",
    "\n",
    "        return stress_ser, constit_eqn_coeffs\n",
    "\n",
    "\n",
    "# return stress_series, featvec, paramvec, constit_eqn_coeffs, W, sig_pl, strain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor()\n",
    "# torch.tensor(np.array([[0,0,0], [0,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = Dynamic_Stress_PINN(params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns-->         featvec, paramvec, constit_eqn_coeffs, W, sig_pl, || strain,            || stress_series,\n",
    "# dataloader returns -->     ^--------------nonseries_list------------------^  || padded_strain_ser, || padded_stress_ser\n",
    "\n",
    "\n",
    "def pinn_train(pinn, dataloader, loss_func=PINN_loss, optimizer=optim.Adam(pinn.parameters(), lr=0.0001)):\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    # running_loss = 0.0\n",
    "    total_run_loss = 0.0\n",
    "    dyn_param_run_loss = 0.0\n",
    "    coeff_run_loss = 0.0\n",
    "    phys_run_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    for non_series_data, strain_series, stress_series in pbar:\n",
    "        global feature_vec, param_vec, strs_ser_glob, coeffs_glob, strn_ser_glob\n",
    "        feature_vec = non_series_data[0].cuda()\n",
    "        param_vec   = non_series_data[1].cuda()\n",
    "        strs_ser_glob = stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "        coeffs_glob = const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "        W = non_series_data[3].cuda()\n",
    "        sig_pl = non_series_data[4].cuda()\n",
    "        strain_series = strn_ser_glob = strain_series.cuda()\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        global stress_series_pred\n",
    "        global constit_eqn_coeffs_pred\n",
    "        stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "        # print(stress_series_pred.shape, constit_eqn_coeffs_pred.shape)\n",
    "\n",
    "        loss_val, dyn_param_loss, const_eqn_coeff_loss, phys_loss = loss_func(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "                                                                    # PINN_loss(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs_data, W_data, sig_pl_data, strain, offset=-0.01):\n",
    "        \n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        # running_loss += loss_val.item()\n",
    "        total_run_loss += loss_val.item()\n",
    "        dyn_param_run_loss += dyn_param_loss.item()\n",
    "        coeff_run_loss += const_eqn_coeff_loss.item()\n",
    "        phys_run_loss += phys_loss.item()\n",
    "        # pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}')\n",
    "        pbar.set_description(f'Losses:\\tTotal - {total_run_loss / (pbar.n + 1):.4f}\\tDyn Params - {dyn_param_run_loss / (pbar.n + 1):.4f}\\tCoeff - {coeff_run_loss / (pbar.n + 1):.4f}\\tPhysics - {phys_run_loss / (pbar.n + 1):.4f}\\t')\n",
    "        # print(\"Data index is: f{idx}\")\n",
    "    return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_validate(pinn, dataloader, loss_func=PINN_loss):\n",
    "    pinn.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(dataloader)  # Use tqdm for progress bars\n",
    "    with torch.no_grad():\n",
    "        for non_series_data, strain_series, stress_series in pbar:\n",
    "            feature_vec = non_series_data[0].cuda()\n",
    "            param_vec   = non_series_data[1].cuda()\n",
    "            stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "            const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "            W = non_series_data[3].cuda()\n",
    "            sig_pl = non_series_data[4].cuda()\n",
    "            strain_series = strain_series.cuda()\n",
    "\n",
    "            stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec, strain_series)\n",
    "\n",
    "            loss_val, dyn_param_loss, const_eqn_coeff_loss, phys_loss = loss_func(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs, W, sig_pl, strain_series)\n",
    "            # return total_loss, loss_data_1, loss_data_2, loss_physics\n",
    "            \n",
    "            running_loss += loss_val.item()\n",
    "            pbar.set_description(f'Train Loss: {running_loss / (pbar.n + 1):.4f}\\t ')\n",
    "        return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training hyper(?)parameters\n",
    "\n",
    "EPOCHS = 125\n",
    "\n",
    "lossfunc = torch.nn.L1Loss() # this is MAE loss\n",
    "lossfunc_name = 'MAE'\n",
    "optimizer = optim.Adam(pinn.parameters(), lr=0.001)\n",
    "\n",
    "cp_dir = os.path.join(nbpath, 'model_CPs')\n",
    "\n",
    "cp_name = f'CP_{fname_base}.pth'\n",
    "best_weights_path = os.path.join(cp_dir, cp_name)\n",
    "print(best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 75\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "# earlystop_min_delta = 0.000075\n",
    "earlystop_min_delta = 0.00075 # For L1Loss (MAE)\n",
    "\n",
    "# os.makedirs(best_weights_path, exist_ok=True)\n",
    "best_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs_completed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return running_loss / len(dataloader),  dyn_param_loss, const_eqn_coeff_loss, phys_loss\n",
    "lossfunc = PINN_loss\n",
    "try:\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train the model\n",
    "        train_loss = pinn_train(pinn, trloader, lossfunc, optimizer)\n",
    "        train_loss_val = train_loss[0]\n",
    "        train_dyn_param_loss = train_loss[1]\n",
    "        train_const_eqn_coeff_loss = train_loss[2]\n",
    "        train_phys_loss = train_loss[3]\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = pinn_validate(pinn, valloader, lossfunc)\n",
    "        val_loss_val = val_loss[0]\n",
    "        val_dyn_param_loss = val_loss[1]\n",
    "        val_const_eqn_coeff_loss = val_loss[2]\n",
    "        val_phys_loss = val_loss[3]\n",
    "\n",
    "        print(f'training:\\ttotal loss: {train_loss_val:.5f}, dynamic param loss: {train_dyn_param_loss:.5f}\\nconstitutive equation coefficient loss: {train_const_eqn_coeff_loss:.5f}, physics_loss: {train_phys_loss:.5f}')\n",
    "        print(f'validation:\\ttotal_loss:{val_loss_val:.5f}, dynamic param loss: {val_dyn_param_loss:.5f}\\nconstitutive equation coefficient loss: {val_const_eqn_coeff_loss:.5f}, physics_loss: {val_phys_loss:.5f}')\n",
    "\n",
    "\n",
    "        # Save the model's weights if validation loss is improved\n",
    "        improvement_delta = best_val_loss - val_loss_val\n",
    "\n",
    "        if val_loss_val < best_val_loss:\n",
    "            pct_improved = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            print(f\"Val loss improved from {best_val_loss:.5f} to {val_loss:.5f} ({pct_improved:.2f}% improvement) saving model state...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(pinn.state_dict(), best_weights_path)  # Save model weights to file\n",
    "        else:\n",
    "            print(f'Val loss did not improve from {best_val_loss:.5f}.')\n",
    "            # early_stop_counter += 1  # Increment early stopping counter\n",
    "\n",
    "        if improvement_delta > earlystop_min_delta:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "\n",
    "\n",
    "        # Collect model training history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Validation loss did not improve for {early_stop_counter} epochs. Early stopping...')\n",
    "            pinn.load_state_dict(torch.load(best_weights_path))\n",
    "            print(f\"Model best weights restored - training epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}]\\tTrain Loss: {train_loss_val:.5f}\\tValidation Loss: {val_loss_val:.5f}')\n",
    "\n",
    "        epochs_completed +=1\n",
    "\n",
    "\n",
    "    # Load the best weights at end of training epochs\n",
    "    pinn.load_state_dict(torch.load(best_weights_path))  # Load best model weights\n",
    "    print(f'Training epochs completed, best model weights restored - epoch {best_epoch}')\n",
    "    min_val_loss = best_val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "    pinn.load_state_dict(torch.load(best_weights_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(batch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = batch[-1][0,:].numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn(batch[0][0].cuda(),batch[0][1].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.constit_eqn_coeff_predictor[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_const_eqn_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strspred_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(strs_start_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strspred_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strn_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trapz(strspred_glob[:, 10:], strn_glob[:, 10:], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_glob = coeffs_glob[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A_glob*strs_ser_glob.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_ser_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=PINN_loss\n",
    "optimizer=optim.Adam(pinn.parameters())\n",
    "pinn.train()  # Set the model to training mode\n",
    "running_loss = 0.0\n",
    "batch = next(iter(trloader)) # Use tqdm for progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_series_data = batch[0]\n",
    "strain_series = batch[1]\n",
    "stress_series = batch[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# global feature_vec, param_vec, strs_ser_glob, coeffs_glob\n",
    "feature_vec = non_series_data[0].cuda()\n",
    "param_vec   = non_series_data[1].cuda()\n",
    "strs_ser_glob = stress_series = stress_series.cuda()  # Move inputs to GPU\n",
    "coeffs_glob = const_eqn_coeffs = non_series_data[2].cuda()\n",
    "\n",
    "W = non_series_data[3].cuda()\n",
    "sig_pl = non_series_data[4].cuda()\n",
    "strain = strain_series.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "stress_series_pred, constit_eqn_coeffs_pred = pinn(feature_vec, param_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_series_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_val, dyn_param_loss, const_eqn_coeff_loss, phys_loss = loss_func(stress_series_pred, constit_eqn_coeffs_pred, const_eqn_coeffs,)# W, sig_pl, strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_ser_glob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_eqn_coeffs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(200)\n",
    "t2 = torch.randn(100)\n",
    "\n",
    "t3 = torch.cat([t1, t2], dim=0)\n",
    "print(t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_dict = {f'train_loss {lossfunc_name}': train_losses, f'val_loss {lossfunc_name}': val_losses}\n",
    "\n",
    "histno=1\n",
    "histpath = os.path.join(nbpath,'model_jsons',f'{cp_name[:-4]}_training_history{histno}_{epochs_completed}ep.json')\n",
    "\n",
    "pinn.eval()\n",
    "\n",
    "with open(histpath, 'w') as f:\n",
    "    json.dump(hist_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
